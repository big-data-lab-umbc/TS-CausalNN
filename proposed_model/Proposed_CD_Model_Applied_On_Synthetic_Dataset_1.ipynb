{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0u1p2Rz6EjmL"
      },
      "source": [
        "#TS-CausalNN applied to Synthetic Dataset 1\n",
        "\n",
        "\n",
        "This notebook contains the proposed model. Here we have developed the proposed Causal Conv2D layer and the optimization function.\n",
        "\n",
        "The functions to visualize the predicted causal graph are available after the model training codes. The predicted graph is compared with ground truth using an adjacency matrix (array).   \n",
        "\n",
        "In this notebook, we applied the proposed model to the synthetic dataset-1 to generate a full causal graph and summary graph."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# high SNR with inst\n",
        "# Dataset V3\n",
        "#Size of time-series: t\n",
        "t = 100000\n",
        "\n",
        "#Create noise\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "np.random.seed(1001)\n",
        "np.set_printoptions(suppress=True)\n",
        "noise = np.random.normal(0,1,t)\n",
        "print(noise.size)\n",
        "\n",
        "#Variable 1\n",
        "source1 = np.zeros((t))\n",
        "source1_1st_term = np.zeros((t))\n",
        "source1_noise_term = noise*0.1\n",
        "#source1[1] = noise[1] + 10\n",
        "#source1[2] = noise[2] + 10\n",
        "#for x in range(3,t):\n",
        "#  source1[x] = 0.125*math.sqrt(2)*math.exp(-source1[x-1]*source1[x-1]/2) + noise[x]\n",
        "\n",
        "for i in range(0,t):\n",
        "  if(i<5):\n",
        "    source1_1st_term[i] = 2 * np.cos(i/10)\n",
        "    source1[i] = 2 * np.cos(i/10) + noise[i]*0.1\n",
        "  else:\n",
        "    source1_1st_term[i] = 2 * np.cos(i/10) + 2 * np.log(np.abs(source1[i-2] - source1[i-5]) + 1)\n",
        "    source1[i] = 2 * np.cos(i/10) + 2 * np.log(np.abs(source1[i-2] - source1[i-5]) + 1) + noise[i]*0.1\n",
        "\n",
        "\n",
        "#Variable 2\n",
        "source2 = np.zeros((t))\n",
        "source2_1st_term = np.zeros((t))\n",
        "np.set_printoptions(suppress=True)\n",
        "noise2 = np.random.normal(0,1,t)\n",
        "noise2\n",
        "import math\n",
        "source2[1] = noise2[1]\n",
        "source2[2] = noise2[2]\n",
        "for x in range(3,t):\n",
        "  if source1[x-1] > 0:\n",
        "    source2_1st_term[x] = 12*math.exp(-source1[x-1]*source1[x-1]/2)  - 4*math.exp(-source1[x]*source1[x]/2)\n",
        "    source2[x] = 12*math.exp(-source1[x-1]*source1[x-1]/2) + noise2[x]  - 4*math.exp(-source1[x]*source1[x]/2)\n",
        "  else:\n",
        "    source2_1st_term[x] = -12*math.exp(-source1[x-1]*source1[x-1])  - 4*math.exp(-source1[x]*source1[x]/2)\n",
        "    source2[x] = -12*math.exp(-source1[x-1]*source1[x-1]) + noise2[x]  - 4*math.exp(-source1[x]*source1[x]/2)\n",
        "\n",
        "#Variable 3\n",
        "source3 = np.zeros((t))\n",
        "source3_1st_term = np.zeros((t))\n",
        "np.set_printoptions(suppress=True)\n",
        "noise3 = np.random.normal(0,1,t) #np.random.normal(-0.1,0.1,t)\n",
        "noise3 = noise3/5\n",
        "import math\n",
        "source3[1] = noise3[1]\n",
        "source3[2] = noise3[2]\n",
        "for x in range(3,t):\n",
        "  source3_1st_term[x] = -10.5*math.exp(-source1[x-1]*source1[x-1]/2) # - 8.5 *math.exp(-source1[x]*source1[x]/2)\n",
        "  source3[x] = -10.5*math.exp(-source1[x-1]*source1[x-1]/2) + noise3[x] # - 8.5*math.exp(-source1[x]*source1[x]/2)\n",
        "\n",
        "#Variable 4\n",
        "source4 = np.zeros((t))\n",
        "source4_1st_term = np.zeros((t))\n",
        "source4_cf0 = np.zeros((t))\n",
        "source4_cf1 = np.zeros((t))\n",
        "treat = np.zeros((t))\n",
        "np.set_printoptions(suppress=True)\n",
        "noise4 = np.random.normal(0,1,t)\n",
        "import math\n",
        "source4[1] = noise4[1] + 10\n",
        "source4[2] = noise4[2] + 10\n",
        "source4_1st_term[1] = 10\n",
        "source4_1st_term[2] = 10\n",
        "source4_cf0[1] = noise4[1] + 10\n",
        "source4_cf0[2] = noise4[2] + 10\n",
        "source4_cf1[1] = noise4[1] + 10\n",
        "source4_cf1[2] = noise4[2] + 10\n",
        "for x in range(3,t):\n",
        "  source4[x] = -11.5*math.exp(-source1[x-1]*source1[x-1]/2) + 1.2*math.sqrt(2)*math.exp(-source4[x-1]*source4[x-1]/2) + 7.5*math.exp(-source3[x-1]*source3[x-1]/2)- 5*math.exp(-source3[x]*source3[x]/2) + noise4[x]\n",
        "  source4_1st_term[x] = -11.5*math.exp(-source1[x-1]*source1[x-1]/2) + 1.2*math.sqrt(2)*math.exp(-source4[x-1]*source4[x-1]/2) + 7.5*math.exp(-source3[x-1]*source3[x-1]/2)- 5*math.exp(-source3[x]*source3[x]/2)\n",
        "\n",
        "\n",
        "dict={'S1':source1,'S2':source2,'S3':source3,'S4':source4}\n",
        "data=pd.DataFrame(dict)\n",
        "data.to_csv('synthetic_data_v3.csv',header=True,index=False)\n",
        "from google.colab import files\n",
        "files.download( \"synthetic_data_v3.csv\" )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3me_B5zJFM7",
        "outputId": "4f2f3cd6-3ea0-4b74-cc86-a5b2807a9e8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "z96f-sD2rAOk",
        "outputId": "44a2e789-7d04-42fd-d898-cc284954ea15"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         S1        S2        S3         S4     S4_cf1     S4_cf0   S1_core  \\\n",
              "0  1.891355  0.000000  0.000000   0.000000   0.000000   0.000000  2.000000   \n",
              "1  1.900402  0.864968  0.186294   9.985518   9.985518   9.985518  1.990008   \n",
              "2  1.929503  0.220185 -0.037330  10.338397  10.338397  10.338397  1.960133   \n",
              "3  1.776680  0.979300 -1.533456   5.063771   2.069614   2.069990  1.910673   \n",
              "4  1.721463  3.898139 -2.538196   0.524210   0.870249   1.065672  1.842122   \n",
              "\n",
              "    S2-core   S3-core    S4-core  \n",
              "0  0.000000  0.000000   0.000000  \n",
              "1  0.000000  0.000000  10.000000  \n",
              "2  0.000000  0.000000  10.000000  \n",
              "3  1.039982 -1.632127   4.164263  \n",
              "4  1.566931 -2.166429  -0.257863  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0f78944d-b0ef-4211-8270-2519190a6cd0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>S1</th>\n",
              "      <th>S2</th>\n",
              "      <th>S3</th>\n",
              "      <th>S4</th>\n",
              "      <th>S4_cf1</th>\n",
              "      <th>S4_cf0</th>\n",
              "      <th>S1_core</th>\n",
              "      <th>S2-core</th>\n",
              "      <th>S3-core</th>\n",
              "      <th>S4-core</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.891355</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.900402</td>\n",
              "      <td>0.864968</td>\n",
              "      <td>0.186294</td>\n",
              "      <td>9.985518</td>\n",
              "      <td>9.985518</td>\n",
              "      <td>9.985518</td>\n",
              "      <td>1.990008</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.929503</td>\n",
              "      <td>0.220185</td>\n",
              "      <td>-0.037330</td>\n",
              "      <td>10.338397</td>\n",
              "      <td>10.338397</td>\n",
              "      <td>10.338397</td>\n",
              "      <td>1.960133</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.776680</td>\n",
              "      <td>0.979300</td>\n",
              "      <td>-1.533456</td>\n",
              "      <td>5.063771</td>\n",
              "      <td>2.069614</td>\n",
              "      <td>2.069990</td>\n",
              "      <td>1.910673</td>\n",
              "      <td>1.039982</td>\n",
              "      <td>-1.632127</td>\n",
              "      <td>4.164263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.721463</td>\n",
              "      <td>3.898139</td>\n",
              "      <td>-2.538196</td>\n",
              "      <td>0.524210</td>\n",
              "      <td>0.870249</td>\n",
              "      <td>1.065672</td>\n",
              "      <td>1.842122</td>\n",
              "      <td>1.566931</td>\n",
              "      <td>-2.166429</td>\n",
              "      <td>-0.257863</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0f78944d-b0ef-4211-8270-2519190a6cd0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0f78944d-b0ef-4211-8270-2519190a6cd0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0f78944d-b0ef-4211-8270-2519190a6cd0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1eaab58c-3786-4d4d-a2dc-d87bbddf1993\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1eaab58c-3786-4d4d-a2dc-d87bbddf1993')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1eaab58c-3786-4d4d-a2dc-d87bbddf1993 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"S1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08983550288735039,\n        \"min\": 1.7214634295558018,\n        \"max\": 1.9295032183422942,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1.9004018179242066,\n          1.7214634295558018,\n          1.9295032183422942\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"S2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.5684918350984816,\n        \"min\": 0.0,\n        \"max\": 3.8981391352505788,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.8649679129389449,\n          3.8981391352505788,\n          0.22018545348758117\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"S3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.1992217181010678,\n        \"min\": -2.5381964702933058,\n        \"max\": 0.18629384151508113,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.18629384151508113,\n          -2.5381964702933058,\n          -0.03733009675031331\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"S4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.955409034322732,\n        \"min\": 0.0,\n        \"max\": 10.338396608331866,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          9.985517614190087,\n          0.5242101906508299,\n          10.338396608331866\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"S4_cf1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.0841169044317915,\n        \"min\": 0.0,\n        \"max\": 10.338396608331866,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          9.985517614190087,\n          0.870248872265921,\n          10.338396608331866\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"S4_cf0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.048345765716013,\n        \"min\": 0.0,\n        \"max\": 10.338396608331866,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          9.985517614190087,\n          1.0656721147434272,\n          10.338396608331866\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"S1_core\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06508577411516903,\n        \"min\": 1.8421219880057702,\n        \"max\": 2.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1.9900083305560516,\n          1.8421219880057702,\n          1.9601331556824833\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"S2-core\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7378406821990349,\n        \"min\": 0.0,\n        \"max\": 1.5669308572437823,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.0,\n          1.0399815628393272,\n          1.5669308572437823\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"S3-core\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0572897247021285,\n        \"min\": -2.166428664192278,\n        \"max\": 0.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.0,\n          -1.632126755548504,\n          -2.166428664192278\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"S4-core\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.0770166228807945,\n        \"min\": -0.2578633356865313,\n        \"max\": 10.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          10.0,\n          -0.2578633356865313,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "data.iloc[0:5,:]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.io\n",
        "\n",
        "def signaltonoise(a, axis=0, ddof=0):\n",
        "    a = np.asanyarray(a)\n",
        "    m = a.mean(axis)\n",
        "    sd = a.std(axis=axis, ddof=ddof)\n",
        "    return np.where(sd == 0, 0, (m*m)/(sd*sd))\n",
        "\n"
      ],
      "metadata": {
        "id": "5wbvAZ0YJMz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_np = data.to_numpy()"
      ],
      "metadata": {
        "id": "zfJw775iJYWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snr1 = signaltonoise(data_np[:,0])\n",
        "snr2 = signaltonoise(data_np[:,1])\n",
        "snr3 = signaltonoise(data_np[:,2])\n",
        "snr4 = signaltonoise(data_np[:,3])"
      ],
      "metadata": {
        "id": "U3duiwVkJPiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snr1, snr2, snr3, snr4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynBRfnUoJmVF",
        "outputId": "60c40d5b-4bf9-421b-da86-c19963302620"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array(0.50489652), array(0.02071972), array(1.36470455), array(0.56348485))"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "true_graph = np.zeros((4,4))\n",
        "true_graph[0,0]=1\n",
        "true_graph[0,1]=1\n",
        "true_graph[0,2]=1\n",
        "true_graph[0,3]=1\n",
        "true_graph[2,3]=1\n",
        "true_graph[3,3]=1\n",
        "true_graph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b80PfUYGIwQZ",
        "outputId": "85a344c7-a7c6-4272-b38a-4aa39ea1c289"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 1., 1., 1.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1.],\n",
              "       [0., 0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "true_full_graph = np.zeros((4,24))\n",
        "true_full_graph[0,0]=1\n",
        "true_full_graph[0,12]=1\n",
        "\n",
        "true_full_graph[1,16]=1\n",
        "true_full_graph[1,20]=1\n",
        "\n",
        "true_full_graph[2,16]=1\n",
        "\n",
        "true_full_graph[3,16]=1\n",
        "true_full_graph[3,18]=1\n",
        "true_full_graph[3,19]=1\n",
        "true_full_graph[3,22]=1\n",
        "true_full_graph\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jkng21cFeX4a",
        "outputId": "1b48f5a2-8568-4527-eb1c-89f5a0dd6a57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        1., 0., 0., 0., 1., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        1., 0., 1., 1., 0., 0., 1., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "Ha-7TrLfuwHX",
        "outputId": "a334d6a1-eee0-4d2c-9541-55cfe07a341e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 4 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtUAAAGhCAYAAACu6EghAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAADVEUlEQVR4nOzdd3gUVRfA4d9syaYnlJACCaH33jvSBbEhouKHWLBiQywoHREEUexgAysgCogKKKD03gOhk5BASKEkm5Bk28z3x5KFNYVAEhL0vM+zD+zMnTtn7k5mz9y9M6NomqYhhBBCCCGEuG660g5ACCGEEEKIm50k1UIIIYQQQhSRJNVCCCGEEEIUkSTVQgghhBBCFJEk1UIIIYQQQhSRJNVCCCGEEEIUkSTVQgghhBBCFJEk1UIIIYQQQhSRJNVCCCGEEEIUkSTVQgghhBBCFFGpJ9Xjx49HURS3V926dUs7LCGEEEIIIQrNUNoBADRo0IBVq1a53hsMZSIsIYQQQgghCqVMZK8Gg4GQkJDrWlZVVRISEvDz80NRlGKOTAghhBAlQdM00tPTCQsLQ6cr9R/OhSiyMpFUHz16lLCwMDw9PWnXrh1TpkwhIiIiz7IWiwWLxeJ6f/r0aerXr3+jQhVCCCFEMYqPj6dKlSqlHYYQRaZomqaVZgDLly8nIyODOnXqcObMGSZMmMDp06fZv38/fn5+ucqPHz+eCRMm5JoeHx+Pv7//jQhZCCGEEEVkNpsJDw8nNTWVgICA0g5HiCIr9aT6n1JTU6latSrvvvsujz76aK75/+ypzvmjTEtLk6Ra5JKebWPK8kPc3iSMttUrlHY4Bcq2OTBn26jk51naoQghRIkzm80EBATI97f41yhzg5gCAwOpXbs2x44dy3O+yWTC39/f7SX+XbacOEfs2YvFUteMP4/ww9Y47vtsS7HUV5K6z1hL68mrOXmueLZdiKLSNI0nvt3Bm79Fl3YoQghR5pW5pDojI4Pjx48TGhpa2qEUijnbRvz5TACOJKUzf1scqlpynf8XLXbiz2cy8ddous9Yw0WLvcTWVdxUVSM6wVxg+xxKNHPfZ1vo+s6aYllncSeo6dk2ruXHnWv5fE6nZgHw96Hka44LnO379Pc7mbbikGtaeraN4T/s4s8DiddVZ0mzOVSOJKVfU5uWFlXVUFWNCxetRarn78PJvLhgD+nZtmKKrOTsiU/ljwNJfLEh5pqXVVWNrSfOkWGxcyjRzKJdp677c94ee54V+8vmPlwU6dk27p21mW82xxapnpx2VVWNsxmXf8k9ee4ipy5kXnX5DIudFfvPkG1zFCkOIf7rSj2pHjlyJGvXriU2NpZNmzZx1113odfruf/++0s7NACSzdkFfhE0Hv8nnab9zenULHq9t47XFkWxaPfpa17PN5tj6fv+ercDYl7aTVlNp2l/89XGGI6nXLyudRXVqQuZuQ6+B8+YGbVoH4lp2a5pZ9KyWBWd5Gq/ycsO0veD9bz5+0EA7A6V/afT3JLsA6fNxRrrlXeE0TStwC+NH7bG8eKCPdgdap7zNx07S6PxfzL2lwMALIs6w/xtcfnWt3RvAg3G/cGna45fNc4r2y2nNSx2BztPXsBRyJO09cfOsiwqkU/WHCfJ7Kzvg9VH+W3fGR7/dmeh6shR1MQxPzaHyoLtccSdc37RP/XdTnq9t4752+OvuZ6tJ87dsCQg02qnw9t/Uf31ZTSbtJIdsecLLK+qGhn5nFA9PGc7i3ef5v1VR68phmybg7eWHWTLiXO55m2PPc/MVUfy3Xevl70Q+965DEuen8M3m2MZ9NkWGo77gz4z1zPix72sPnh9J4wDZ23mye92uvab/OSc+JSWdUdS2HD0bKHLf74+hm2x513HlKtJy7Qx8ddo9p9Oc00bs2Q/Hd/+m7QsG498vZ2Wb65iR+x5LlrsdJm+ho5v/33VNnn6+108+d0uxizZX+jYhRC5lfrdP06dOsX999/PuXPnCAoKomPHjmzZsoWgoKDSDo3lUWd46vtdDGhehRn3NnFNz0kSd8enuqZN/v3yz6NRp1K5p0XBVzI/P383VrvKJ4OboyiK66A6c9UR3ryzkatcts3Bc/N20zQikKe61MCc7f5FndfBMsvqwMtDn2v61hPn2B57nqe61kSvu77bD+4/ncZtH26gagVv1r58CwC/7DnN8/P3AHA85SI/PtEOgHZT/gLgg/ubkWmx8+Wl3q6vNsYw5rZ6vLF4Pwt2xPN01xq80if/B/78eSCRJHM2/2sXmWteeraN3u+tIyEtm1kPtqBPQ+etGTccPcsHq4+SlH45WX3i2538GZ3E+lduIby8d666Xl8cBUCVcl481D4SnaJQ3sfDNX/6n4cB+HbLSR5sW5Wnv98FQIeaFfOsb+SPewF4e8Uhnupawy1mP0+jW9mnvr+c9KZl2bDYHby4YA/LohJ5tltNXupVJ9/2yZFlvZzY3P3JJt68qyFJ5oJP0nIkpGbRfqrz8xraPpK5m2J5886GDG4TgaIoaJrGrLUnqBPiS7e6wQCMWrSPedviaRoeyDePtsb/H9v0T5+vO8HcTbGuHvnYqf1YdSnJ+mpDDL3qB+Np1ONjcj8smbNtbDp2lq51KuFpdO7XU5cf4ssNMdzWOJSPHmie5/p2xV3AaleLZSz9nweSOHPFic+XG2JoGVk+V7mdJ53rvP9z53CjnH1t0a5ThJf3ptUVy1xZX35sDhW9oqDTKXyx/gSfrXO+jr/Vl1MXMjmTlk3b6hUYOGszAKsPJvPJ4OZ57o//FHP2IqEBnq42zcuVRwlN01AUhQ1HzxLkZ6JOiB9J5mzavLWa8j4e7BrT023ZuZtic9W3PyGNHvWD3aZpmkam1ZHrc79ynTmS0rOJqJD3tqmqRp/312Ey6Fk6vEOet1jN79iYM8/TqLvuW7OmZ9sY8tU2AA5N6oOnUU+yOZuKviYAPl17nGYRgbSvUdG1TOY1/tI44bcDLNp12tmp8lZfdIrzeASwYHscaw6nAPDN5pO82LO2azmHpqEj7+2yOVTWHXEut3DnKaYPbJJnuX8zh8OBzVb2fzkSpUOv12MwGAp1bCj1pHr+/PmlHUK+Rl86a/951ymmDmiEUa8j/nwmd32ykWB/Tw4kXO5VXRZ1+afJb7ec5GBiOoNahrPj5AUm3N4AD8PlHwUSUrP4ZU8C4PzJ/9vNJ13zDp5Jx+ZQsdpVDHqFz9ad4M/oJP6MTuLUhayrxrw3PpU7Pt7I0PaRjL+9gdu8QZfGFYcEeOVK+nfHXWD4D7sZ3a8etzYKRdM0UtItTF52kAHNq9C5tvMk5/eoMwCcPJfJt1tOcmfTMFdCDbAt5jwz/jxMt7qVXNM2HTubqyfyWHIGC3Y4p32y5jj+Xkae7FKDvOT0srapXoHawe53hJm99gQJl5KTJ7/bSezUfqRn23jwy6256vkzOgmAedvieKprjVyJbY4P/zrGh385x/Qvero9jSsHYNDr2B2X6irTe+Y61//TsmyE51lTbm8tO8hn607w5UMt6V7PmVxctNjd6p656igzr+jF/GJ9DA0rB2DUK66E9krfbI7N1dN1OjWLh+dsp3+TMNe091YeoUo552efc3CIPXuRp7/fRfSZy/tyTjI0esl+Ri/ZT/WKPoztX5+3Lw0r+WpoS7rVDWbeNufntyc+lcbj/+TEW33R5XGylnMB5uRlB92mH0lKd/0/NctGizedD4CKmdIXTYOzGRYq+JoY+tU2dsWlMqRdVSbe0RDAdYL2274zjLktm2B/T1csd368kV71g12f996xvQjwdv+sE1KzOJdhpVEV9zsOaJrG2QwrQX4mt+n/PJYu35/I3Z9s5LMhLV1JU2JaNgM+3eRWrtO0v93er3ih0+V1kfuEeP/pNCoHelHOx4NTFzLp+LZz+WXPdXI73tR4fdnlWJ6/XGfU6TQ6Tfub2Kn9cKgaNofqljR/suYY64+c5amuNRjy1TbCAjzZNKp7rjjyUm3UMl7oUcu1b8ZO7efqNT9/0Uq7Kat5sUdtbmsSireHgdg8epVnrjrK8FtqYtBfPh6+sGAPv+xJYNStdYk5e5HbGodx4mwGv+5NYHvsBZ7oXN1V9sqPYXvsed798wgT7mhA7WA/EtKyOJKUAUCWzYHJoHfrPNhw9CwPfrmVZ26pwcu966KqGuczrXz01zG3E4Ato7oTdz6TsEBPqpRzT+BVVeOZH3ZRraJPro6Ai5bLJ7WLdp0m+kwa322Jo0vtIMp5G1ly6ZgfO7Wf62RB+0fdOX8/45ceYFvMeX5+qj3rj6bQqVYQXh56Dp65/DfT4921ZFovJ+UFjazJb15CalaufRTAoWr8ti+B5hHlCnWCdjPLyMjg1KnrH5ok/hu8vb0JDQ3Fw8OjwHJl7u4f16qkrh4+npJB9xlrXe+bVAngl+EdeWH+btfB8VqFBXjSJDyQ5cU4NnB8//o81D6SrzbGUjnQkye/2+WaFzu1n1vZyNd+d/3/+e61aF+jAl9vjuXl3nW55YoxzNPvacyU5Yc4f8UQgJyEadSiKOZdMeShX+NQft935prj/u3Zjtz24Qa3aX0bhVAzyJcPLiW0sVP7ceGilWaTVgLww2NtaF+zotsyY5bsd/XUABx+sw91Rq8oVAxrX+5K1Qo+rvdXts+VFAWmDWjMyz/ty3P+OwObuE5SjqdkYDLoqFLOm9pvLMd66ef4HaN7UNHX5FpH9Yo+/DWyKxctdhqM+6NQ8eZ4uXcdHu4QibeHocC4AWpW8uVYcobbtPDyXqwa0YVV0ck888OufJa8dp8PaUmjygEs2B7Pfa3D8fbQ88jc7WyPvUCIvyeJ5qv3zAKU8zZyITPvXqONr3Vj18kLPDtvt9v0Y5NvRa9TqDZqWa5l1ozsSmRFH7dpOW32z3kv/biXn3edonVkeeY/3pZ1R1MYOmd7vrHe3zqCKXc7f1nqPmMNx1MKP4a/V/1gUjIsqKrG7P+15ExaFnd94kzK72gaRlqWzdXz6Gcy0K5GBdeJwpWm35N731zyTAfu/HgjANETexdqX4mZ0heLXeV4SgbeHgYiK3jn2Z45Yqf2o9O0v4g/n/tk/807G7o6Jf7piS7VOX/phOaB1hHUfGN5vuv4p5+fakf0mXT+PpTMX1dce9CldhATbm/guhajeUQghxPT+fTBFqRn2+laJ4jeM9e5OiZeu7UuO09eYGUe7Xml6Im9GfbNDlLSLfz0VHsOJphdnRM5x9f1R1MIDfDEx2Rw/TpXkKXDOzB0znZe61OXQ4npfLXReYIYFuDJnyO6YNQreR7DZj3YgmkrDnGikBdxrxnZ1dUeR968FaNeydXTlnOSf6W5D7ci7nym60T9oweacVvjMEpCad/9w+FwcPToUby9vQkKCpIHyIlcNE3DarWSkpKCw+GgVq1aBT6oSJLqfHz011He+fOI27SVL3am53vr8lmidIzvX5+alfzy7Jn96cl23DNrM60iy7E99kK+dVQP8uHEVZKBQ5P68Mbi/fy861SRYwbngXr4D7sLLLP46fauJAPgkQ7VsNgd/LIngbuaVWbiHQ0Yt/QA31zR038t7mwaxuS7Grl+di4o4biaCbc3oFvdSq5eny2jutN52t+upBqcScuVScq2N7oTdSqNR7/ecc3r61izIjPva4q/p5HaowuflOQIC/B09fD/G0y+qyFeRj0jLg25udLs/7WgV/1g1xemzaFS61Ii91y3muw4eYFudStRyd+T565I1jvUrMDGY7nHL//TxDsa0LN+cKESqpIw4Xbn30F+vnmkNbWD/Yg5e9E1LKUwvhjSkse+yX/ffKVPHaatOHxNsf7Tox2ruX51KIwALyNpWXmfcHl76Mm03rgL7WKn9uPgGTO3vr8egM2jul3zPnB7kzCW7r3cSdOldhBrLw3FKE5PdK7O7HUnqF7Rh3fubcLdn2wiorw3fRqG5Eqq8/JSz9o8271WscdV2kl1dnY2MTExREZG4uXldcPXL24emZmZnDx5kmrVquHpmf9tbyWpzseHq48yY+WRqxcsAxpW9md/MV/gdzN4tGM1zl+0sriIF2uufqkLNYJ8i5RUAzzYNoLvtuR/4WLOSU6OSn4m1EvDDUTJ0inwXPdatK5Wngc+z30CKsS1quRnomoFb1eHxZZR3Wk7ZXUpR3VtFKXgYSNX+ucvn8WhrCTVV0uUhCjsviJJdT5upqRaFJ2nUUe2rXjvnCCEEP8WklSL/7LC7iulfku9suqmPtMQ10wSaiGEEEIUhSTV+bi5+++FEEIIIYpH165deeGFF0o7jDJPkup85HW7KyGEEEKI0pSSksJTTz1FREQEJpOJkJAQevfuzcaNG11lPvvsM7p27Yq/vz+KopCamnrVeocOHYqiKLlex44dY9GiRUyaNKlIcSuKwpIlS/KdP3fu3DzXf+UrNja2SDGUtFK/T3VZJT3VQgghhChrBgwYgNVq5euvv6Z69eokJSWxevVqzp27fLeizMxM+vTpQ58+fRg1alSh6+7Tpw9z5sxxmxYUFIRen/9DogCsVutV7+F8NYMGDaJPnz6u93fffTcNGzZk4sSJbrEUVnHEdK2kpzofx1Myrl5ICCGEEP8Kzqd72kvlVdh7RqSmprJ+/XrefvttbrnlFqpWrUrr1q0ZNWoUt99+u6vcCy+8wGuvvUbbtm2vqQ1yer6vfOn1+lzDPyIjI5k0aRJDhgzB39+fxx9/HKvVyvDhwwkNDcXT05OqVasyZcoUV3mAu+66C0VRXO+v5OXl5bZeDw8PvL29Xe89PT154oknCAoKwt/fn27durF37+XbqI4fP56mTZvyxRdfuF1QqCgKs2fP5rbbbsPb25t69eqxefNmjh07RteuXfHx8aF9+/YcP378mtoqL9JTnY+/r3iwgBBCCPFfZrE7n1L5b5Zlc1B/7LU9jKu4XPmQpoL4+vri6+vLkiVLaNu2LSaT6arLlJR33nmHsWPHMm7cOAA++OADli5dyo8//khERATx8fHExzufvLt9+3YqVarEnDlz6NOnz1V7vvMycOBAvLy8WL58OQEBAcyePZvu3btz5MgRypcvD8CxY8f4+eefWbRokds6Jk2axLvvvsu7777Lq6++ygMPPED16tUZNWoUERERPPLIIwwfPpzly6/9uQ9XkqRaCCGEEAXKtqn/+qT6ZmAwGJg7dy7Dhg1j1qxZNG/enC5dunDffffRuHHjItf/22+/4evr63p/6623snDhwjzLduvWjZdeesn1Pi4ujlq1atGxY0cURaFq1aqueTnDNgIDAwkJCbnmuDZs2MC2bdtITk52nUi88847LFmyhJ9++onHH38ccA75+Oabb3INE3n44Ye59957AXj11Vdp164dY8aMoXfv3gA8//zzPPzww9cc1z9JUi2EEEKIAllsDvAylnYYJcrLqCd6Yu9SW3dhDRgwgH79+rF+/Xq2bNnC8uXLmTZtGl988QVDhw4tUhy33HILn376qeu9j49PvmVbtmzp9n7o0KH07NmTOnXq0KdPH2677TZ69epVpHhy7N27l4yMDCpUqOA2PSsry23YRtWqVfMcd33lCUdwcDAAjRo1cpuWnZ2N2Wwu0j3TJanOR84jjYUQQoj/utnrTjDmtvqlHUaJUhSlUEMwygJPT0969uxJz549GTNmDI899hjjxo0rclLt4+NDzZo1C132Ss2bNycmJobly5ezatUq7r33Xnr06MFPP/1UpJgAMjIyCA0NZc2aNbnmBQYG5htTDqPx8glhTn6X1zRVLdozK26OvacUSEothBBCOJ1JyyrtEEQB6tevX+Dt6m4Uf39/Bg0axKBBg7jnnnvo06cP58+fp3z58hiNRhwOx3XV27x5cxITEzEYDHle5FhWSFKdH8mqhRBCCEB+vS0rzp07x8CBA3nkkUdo3Lgxfn5+7Nixg2nTpnHHHXe4yiUmJpKYmMixY8cAiIqKws/Pj4iICNdFfcXt3XffJTQ0lGbNmqHT6Vi4cCEhISGunuTIyEhWr15Nhw4dMJlMlCtXrtB19+jRg3bt2nHnnXcybdo0ateuTUJCAr///jt33XVXrqEopUWS6nzI4UMIIYRwku/EssHX15c2bdrw3nvvcfz4cWw2G+Hh4QwbNozXX3/dVW7WrFlMmDDB9b5z584AzJkzp8hDRPLj5+fHtGnTOHr0KHq9nlatWrFs2TJ0Oufdm2fMmMGIESP4/PPPqVy58jU9yEVRFJYtW8Ybb7zBww8/TEpKCiEhIXTu3Nk1RrosULTC3hyxjDKbzQQEBJCWllakweX/1GTCn6Rl2YqtPiGEEOJmdXuTMD64v1mx1llS39+FlZ2dTUxMjNs9jYXIS2H3FXn4Sz7kly4hhBDCSSffiUJclSTV+ZDjhxBCCOEkY6qFuDpJqvMhBxAhhBDCyWov2q3GhPgvkKQ6H5JSCyGEEE6/R50p7RCEKPMkqc6HdFQLIYQQQojCkqQ6HzL8QwghhBBCFJYk1flISbeUdghCCCGEEOImIUm1EEIIIYQQRSRJtRBCCCGEEEUkSbUQQgghxH9QZGQkM2fOLO0w/jUkqRZCCCGEuIkMHToURVGYOnWq2/QlS5Zc040Wtm/fzuOPP17c4bmcOXOGBx54gNq1a6PT6XjhhRdKbF1lgSTVQgghhBA3GU9PT95++20uXLhw3XUEBQXh7e1djFG5s1gsBAUFMXr0aJo0aVJi6ykrJKkWQgghhNA0sF4snZemXXO4PXr0ICQkhClTpuRb5ueff6ZBgwaYTCYiIyOZMWOG2/wrh39omsb48eOJiIjAZDIRFhbGc8895yprsVgYOXIklStXxsfHhzZt2rBmzZoCY4yMjOT9999nyJAhBAQEXPM23mwMpR2AEEIIIUSps2XCW2Gls+7XE8DD55oW0ev1vPXWWzzwwAM899xzVKlSxW3+zp07uffeexk/fjyDBg1i06ZNPP3001SoUIGhQ4fmqu/nn3/mvffeY/78+TRo0IDExET27t3rmj98+HCio6OZP38+YWFhLF68mD59+hAVFUWtWrWua7P/bSSpFkIIIYS4Cd111100bdqUcePG8eWXX7rNe/fdd+nevTtjxowBoHbt2kRHRzN9+vQ8k+q4uDhCQkLo0aMHRqORiIgIWrdu7Zo3Z84c4uLiCAtznniMHDmSFStWMGfOHN56662S3dCbhCTVQgghhBBGb2ePcWmt+zq9/fbbdOvWjZEjR7pNP3jwIHfccYfbtA4dOjBz5kwcDgd6vd5t3sCBA5k5cybVq1enT58+9O3bl/79+2MwGIiKisLhcFC7dm23ZSwWCxUqVADA19fXNf3BBx9k1qxZ171NNytJqoUQQgghFOWah2CUBZ07d6Z3796MGjUqzx7owgoPD+fw4cOsWrWKlStX8vTTTzN9+nTWrl1LRkYGer2enTt35krGc5LpPXv2uKb5+/tfdxw3M0mqhRBCCCFuYlOnTqVp06bUqVPHNa1evXps3LjRrdzGjRupXbt2rsQ4h5eXF/3796d///4888wz1K1bl6ioKJo1a4bD4SA5OZlOnTrluWzNmjWLb4NuUpJUCyGEEELcxBo1asTgwYP54IMPXNNeeuklWrVqxaRJkxg0aBCbN2/mo48+4pNPPsmzjrlz5+JwOGjTpg3e3t589913eHl5UbVqVSpUqMDgwYMZMmQIM2bMoFmzZqSkpLB69WoaN25Mv3798o0tpwc7IyODlJQU9uzZg4eHB/Xr1y/WNigL5JZ6QgghhBA3uYkTJ6Kqqut98+bN+fHHH5k/fz4NGzZk7NixTJw4Md8hIoGBgXz++ed06NCBxo0bs2rVKn799VfXmOk5c+YwZMgQXnrpJerUqcOdd97J9u3biYiIKDCuZs2a0axZM3bu3MkPP/xAs2bN6Nu3b7Ftd1miaNp13ByxDDGbzQQEBJCWllasY3giX/u92OoSQgghbnaxU/PvjbweJfX9XVjZ2dnExMRQrVo1PD09b/j6xc2jsPuK9FQLIYQQQghRRJJUCyGEEEIIUUSSVAshhBBCCFFEklSXMh+y8MBW2mFcFz2OQpWrrcRTRUm57vWEcA7QrmmdN7NQzuFLptu0iqThT0aBy9VSThGpnCnJ0HIxYKetLhoTVjywMVi/qkif9T+Vw4wRe7HVBxoKly/kce5PJXFZSfHXacSea78oqsqkUEM5DYCCSm0lnoH6NeiuaKPctKvMv7FaKod4WL8cZ5vnvIquAmmYsOYxp3QvQ3Luvzf1pVBC/GvJLfUKEMZZGuli2K7WIQMvrBjprNuLL1ksU9sCzgPck/rfqKU7xVpHY35V26NeOlepoZymHOmEKefxVbLI0jyI0qrRXneAcmQQrVXlc493ydRMfGi/i4NaOGvUptRQEqinxFFFSSFZC0RDIZHy2DQ9O7S6dNXtZq7HdParkUyy/Y9WukPEaKE40PGC4WcOa+E8b3sGgN667TTVHedd+0C66XbRQneUxw2/c0StTC/rNMqRTi/9TpK1QNaqTZhunMUA/QYuaL6UUzIYYxvKITWCzvp9PGtYAkDD7C/Y7/kYAMOsI/DAzlGtMmbNGz0q9xv+Yotanwf0qwHoq98GwGpHM16xPY6Pkk1z5SieipWTWjAtlCO01UXTUX8Au6ZjjP1hQpTzvGe/h8Ue42imO8Z8e1eWqW34xuNtAJ6xPkdr3UFm2W/nDBXwxEI7XTRpmg9+ShYAX18q+4h1JDWV0ww3LOEh62ukEEAX3T4e0S/nEdvLnNSCAQU9DkI4T3v9AayagV/Ujm77gwc2fvIYT2NdDHPtvTishROnVcKs+XCvfg2r1Bac0cqTqJXDjA8VMJOONwFcZIB+HfMdt6ChUFk5S7QWyYP6lZzT/FmutkGHSl/dVprojjPMsAyAyOwfAKiipLDB9LzbNB0qfXTb6KA7QAaerFGbMs9jMgAPWkdh0Ywc08JIxZeH9X/QVbeHprrjzHH0xp9MzmjlGaRfQ4wWgg8W/lKbskOtw3EtlIf1fxCjhbJCbUU50glXkqmmS+QXRwd8yeJ2/SaWODqQih/HPIcAsMzRGj0qvfU7ABhgGUc9XRwH1Eh2azVppRzmpBaMBSNp5Dx1S8Ofi5jxARS663bSWBdDT91OhtueZbhhCXfrNwBQLfs7Jhu+5AHD3yx1tOOUFsRn9n6k4uf6O+yu281WtR7peFNXiaOl7jC/O9owzLCMx/S/09XyHj+aJlJFOev6TLM0D85qAcywD8RXySJJK8dZLYA9Wg3ClRQilGR8yGa12oz6ykneN37Eu/aB/Kq2AxRA4xXDAnzJ4h37QHrqdjHDw/kUsb8cTZnvuIUsTLxqmM+fjpaMMP6ERTOwRm3KbrUmnoqVLWp9tqj1MODgbeNnpGjlqKPEUUNJoLt1Bk/rf6GycpZ2umjCdSncbRnPLq0WOjTXscYTC1aMtNEdJEqtRlUlCQ2FaC0SD2yMMCwkSEnlqFqFM1p5/lab8Zjhd5679Ddt1rzxVy4n7KGcJ1DJoJtuNz86upCGLye0UDar9Yn1HAzAcOuz7FJr0UO/kyi1Orfpt+BDFq/Zh11qG2iuHKGLfh+BpPO2/X4c6Lhdv4lQzrNbq4k3Fu7Vr2GzWp+muuN87+iOVTNwXAvDhA0vxcJJLRgNHSasNFWO01O/gwNqJO95fMpPjs7co18HwKOG5XhhIU4LZoB1PCo6uul2cbt+E+/Z7yGQDPZr1WioxPClxztUVMx8Yb+VD+x3u/6m0vGmuXKUJrrjjDZ+D8Btljd5ybAQHyWb42oY9xv+5pzmR0vLp0QoyQzUryVKrUZ//Rb66LYRp1Xic0c/phi/ZJBlDJmYCFYu8K7xE0bYniZeC2Ka8TP2q9X40nErf5lGYtEM1LXMJZAMMnFeBGXFgIaOKkoyt+q2cUCLpAJm3jR+RYCSSdvsD7Fh4BwBeGDjC+M7bFYb8KnjdpZ4jKGp7jgvWZ8kQpfEA/rVfGV33m0hQSvPGa0CQUoaChor1Ra00R1kh1rn0ro1WiqHSSGQ940f872jOwsdXfk3u8nv1yBugMLuI2Xi7h8ff/wx06dPJzExkSZNmvDhhx+6njd/NSV19XD1137lhOeDxVafEEIIcTPao9bgTuukf93dP2w2G8eOHSMsLIyAgIAbvn5x8zh37hzJyckFPjgHykBP9YIFCxgxYgSzZs2iTZs2zJw5k969e3P48GEqVapUanH9cKnXTwghhPgva6o7ThvlIFC8SXVpMxgMeHt7k5KSgtFoRKeTEbHCnaZpZGZmkpycTGBgYIEJNZSBnuo2bdrQqlUrPvroIwBUVSU8PJxnn32W11577arLl9iZ7ng5axVCCCFcxqcVa3Wl3VMNYLVaiYmJcXtoihD/FBgYSEhICIqiFFiuVHuqrVYrO3fuZNSoUa5pOp2OHj16sHnz5jyXsVgsWCwW13uz2VzicQohhBDi38fDw4NatWphteZ1UaoQYDQar9pDnaNUk+qzZ8/icDgIDg52mx4cHMyhQ4fyXGbKlClMmDDhRoQnhBBCiH85nU4nT1QUxeKmG0A0atQo0tLSXK/4+PjSDkkIIYQQQvzHlWpSXbFiRfR6PUlJSW7Tk5KSCAkJyXMZk8mEv7+/26skPG99ukTqFUIIIW42P9i7lXYIQpR5pZpUe3h40KJFC1avXu2apqoqq1evpl27dqUYGfyiduRbe49Cld2u1uYj+x25pls0A1/bexKrBuex1GU/OzqxyVGfBK282/QszSPP8pNszlv9zbX3Il4NKlSMZs2bfWq1q5abZ7+FxY4OhaqzpAyyjMlzulnzLtTyO9Ta7FWrFzmOK9vBql19PNVRtXKh637HNrBQ6y0JE23/I0kL5JRWMc/5x9VQnrY+d9312wrRVnl51TbM7X20WtXt/STbg/ziaM+dlon0sEzLt54DVyyXoXmyS62ZZ7k0zZt+lsl0t0zPFXO8GsRg6yii1Mh81/O1vSeJWjkA1jsask2tk2/Zm906RyN+d7TGoSk8aB3FCTXvTo/SNNV2n9v7OfbexVb3lX8rfzhassTRHotW+NGTGZon6ZqX27SfHZ2KLb4rvX/pvtsAr9seZbTtYX5ydC708g9ZX2WYdYTrvUNTSNO8edt+XwFLCSGgDNz9Y8GCBTz00EPMnj2b1q1bM3PmTH788UcOHTqUa6x1Xkrq6uHI136/ruUUVLR8z1U0VylwPmzA+fwv9/IG7HhgJxNPGigxDDP8znE1jB8c3TnHtd2VxBML2ZhyTfclEzt6BunX4IGNlWoLYrVQtzKvGebxpOFX7rGMZYdWFz0OgrnAJs/neMt2P3+orUjTfEjFF0+stNcdYIdaG09smPEmGw/XthqwY0ePFxay8KQcZhSgpnKabVq9ArehgRJDFSWFP1T3e5f7kIUdPeFKMrfpt/CFvS8ZXD3x9iabukocBhzEaiGYFCvtdNEscnTCjoGayik8sBOtRQLOh75YMfKcfhEjjD+xW63JXdaJALTVRVNLOcW3jp6ubVVQ0aHRXneAPWpN0vOJyZdMGuliOKhGoKKjh24nK9WWpOPNKMP31FPieNn2BG8Yv2eq7X7OUJ6GSizpeJGgVcSK0VVXuJLEQP1aVjpakoEXIcp5aigJrHY05wwVrtomfmQy0rCAXxwd2KXVdm2HARUbBqooyYRynu1aXcD5lMxB+jVMtg92PYDkSh7Y8MRKBl5u8w3YqakkcEgLBxQilTP01u3gW0dPMvHEhywu4ulqy+LiiYVPjTNZqbbkV0e7S23dItf+EkQqKQQUaf3Ofd2ZcL1smA/ADPu9qCh4YnX9XVRRUohQkkjTfKmiJLNWbcIw/e9kYuJLRz9eNcwjSEljjr0PFZQ09qnVLz3sRiPnwTOX43T+X0HFAzsWPPDARn3lJCe0UIKUVI5rYVS89PCQEOUcd1rf5EvjdLrrd1Mt+zs0FAw48CWLAfp17FZrcVCLIAtTPu2huU3X48CBHtCIVBLprdtBfd1JnrcNz1XWm2yqKYkc0Kq6pnuRzUHPRwCIzP4eEzYsGN220fmwIF9eNczjKcOvl8r+kO9n0UQ5xkD9Wt6x30s2HvTU7WSt2gQzPpTHzBrTCPaq1VmptuCgWpWFpolMt93Lx447AecDvHrrtjPH0Ycsco+59cBGW100OjSsGNikNsSElUrKBbI1Dyx4XHq4kVNP3Q5eNizgedtwDmpVMWBHQ8GB/tJ3h0Iz5RhPG37hbft9HNMq00w5hoLGLq02dZQ4ButXM9fRm466KDapDUjQKjJI/zfxWiVWqS3ybYv6SiyVlFSi1GpcwA8dKj5kk40Hf5tGcEiN4AnbCGy5LrXS0KPiQP+vu0+1EMWt1JNqgI8++sj18JemTZvywQcf0KZNm0ItW9aS6n8bE1Ys5N1j/l+jx0E7XTS71ZpcxOvqCwghrokJK3b0l5Lz/HmRzSuGBSx3tL7qSfm1MGLPI6n899OhoqJwtZNJSaqFKFiZSKqLQpJqIYQQouRJUi1EwW66u38IIYQQ4saqHexb2iEIUeZJUi2EEEKIAgV6yTBAIa5GkmohhBBCFEi9uUeKCnFDSFIthBBCiALtOHmhtEMQosyTpFoIIYQQQogikqRaCCGEEEKIIpKkWgghhBBCiCKSpFoIIYQQQogikqRaCCGEEEKIIpKkWgghhBBCiCKSpFoIIYQQQogikqQ6HyH+nqUdghBCCFEmBPmZSjsEIco8SarzUaWcV2mHIIQQQpQJE29vUNohCFHmSVKdjz4NQ65ruRB/TzyN196sU+9uVOiy+SX8dzYNu6Z1Ptg2otBlu9WtdE11l5SPHmhGx5oVi6UuL6OeTwY3L5a6rmbuw614uXcdVr7YOde8JlUCrrveSXc25I5r/Nzz06FmhWvaJ4rLP/9eGlb2552BTXKVm/Vgi1zTdozuUWDdo/vVK1pw/3K7x/QkZkpfvn6kNQadct3t9Vz3WsUc2dXVDvalT4PrO04X1tD2kQXOf3tA4Y/bxeHX4R2Lra5OtQo+jlYO9CLAy8h7g5oQNb4XtzYKLbZ1C/FvpWiappV2EEVhNpsJCAggLS0Nf3//YqvXoWqsPZJMw8oBHEvOoEFYAAcS0rhocRBe3os9can0ahDCqEX7+ONAEgCrX+pCjSBfAPafTmPc0gOM7FWH5lUD2Xz8HJtPnOOxjtWp6OuBoij8suc0z8/fwwf3N+P2JmEcSUrn8W92EHsuEwAPgw6rXQXgsY7VeO3Wuhj0zgRk0/GzPPD5VgCWPdeJWsG+GPU6jqdkMOm3aOLOZXLi7EXubx3OU11qsvZIMmN+OQBAj3qVeKRjNdrXcB5UU9ItxJ2/yKkLWew8eYFRt9Zje+x5ADrXDuLUhUxCA7zYE3+BXSdTebRjNXrPXMfR5Ay3Njvy5q3EnL3IhmNnGdKuKifPZdLj3bW52vbHJ9oxekkUR5Kcy7/UszaLd5/mq6Gt2BOfyor9iRxMNPNklxrM2xZH7WA/ftp5CoATb/VFp1PYcuIce+NTybDYCQ3wIup0KuYsOysOJOJQNba93h0UaD15tWu9C59sx2frTjCgeWVaRZangq/z58zYsxcZ/MVW7mgaxtD2kWw+cY7485mkW+wcT85g1cFkVx07RvegnLcHv0ed4bl5uwF4umsNPllznHtaVCE0wBOLXaVn/WDmbYtjR+wFHu9cnQfbVnXVccfHG9kbn+p6/9dLXeg2w9lOdUP8+F+7qvRpEEK/DzaQaM4G4P7W4bzWpx4nzmZgzraz+mASI3vXwd/TiKpqHE3O4I8Diby78ggACx5vS7OIcngYdNgdKg/N2cbGY+dc67y/dTjj+jcg2+bg2Xm7WX/0LIcm9eF0ahbdZ6wlvLwX7apXoGoFH37fd4YGYf4s3HmK6kE+XLTYSTJbAGdSse9UGi0jy9GtbjBTlh1k8e7TvNKnLnoFOtUOYtaa4+yOT2XC7Q146rudTLqzId3rBaNXFFdCfepCFnM2xtIkPIAe9YLxMuqp/voyAJ7oXJ1eDUJoUiWAIV9tY9Nx53Zseq0bYYFeLN59ihcX7GXp8A5k21T2xF/grWWHnJ/t1H6cy7Bw6kIWG46dZfofh3mpZ22C/EyEBXqhUxRe/Xkf/2tXlQ41KhIW6ImnUY+XUY/VoWJzqBxOTOeeWZsBiJ7YG28PA5uOn+VEykVGL9nPHU3DqBzoxSdrjjPmtvocSUxnwY54Xu5dh9ubhNFp2t8YdAoLnmhHgzB/5m2LY1vMeR5qH8l9n20BYOvr3en57lqqlPMm+ozZ9Tktero9d3+yCYAfHmvDq4v2EX8+C4BHO1bjsU7VaDflL7e/rzrBfpzPtPJIh2os3ZvA4DYRvL/6KNPvacxFi4MXf9zDkLZVGdW3Hnqd4lrO7lAx6HWczbDwzh+HeaRjNfQ6hTkbY+hYM4h3Vx7mSFIGviYDW1/vToNxfwDOxHPsbfVZvPs0P+08xewhLci2Ofh93xkm/BrNc91q8sFfx9xiHNiiCr6eBuZsjL20P0Zw8IyZPZf+Ln56sh1v/n6QxLRs/nihM54eOkYu3MevexMAeP++ptzRtDIA2TYHT3+/i78PJzPrwRa0rV6BJhP+BKBvoxDCy3lz4uxFVkYnUdHXxPv3NcXLQ+9q19UvdeGRuds5eS6TZhGB7I5zxnBLnSDmPNyaw4nprD6UxKJdp5kxsAmTlx2kZ71gBrUOx9/TCDiP938dSubvw8mu5Qc0r8L0exqz9kgKP+86xeh+9fEy6tkVd4FOtSqyaPdpvtoQQ5I5m251g/l51ykUBb59pA0Pfuk8tv8wrA1bT5zn/dVHXduclmXDZNDx+boTnM2w8PXmkwCMurUuP+08xdHkDAw6hU61KtKmegW2nDjHOwOb0PLNVa72X/liZ2oF+/Hh6qPMuHTMuKVOELP/1xKrQyX27EXqh/qju2L/KAkl9f0tRGmRpLoYnLqQSWqmjYaVr7/H8UqapqEoCpqmcdHqwNuov+aDm6ZpHEpMp3awn+uLM9vmwOZQ8bv0RVBUpy5kUs7bgx93xDOwZTi+JkOuMuZsG4fOpFMjyAeLXaWSnwmDXufaxuJmsTvQNPA06gE4lpzBqQuZ+HkaaFG1/HXXm5JucZ0M5TidmsWx5Ay61A7C5lAx6gv3C4WqathVjXMXLWRk26kV7Ic52/lFaTLoXeU0TcOcZSfAu3g+L03T2HsqjZqVfPP8rHJkWOz4eOgL/HzizmVS3tejwHqKKtvmwK5q17WO9GwbPh6GEk8KrpRldeDl4fz8VFVzrdvuUNHrlDzb81hyOqBQs5IvDlVzS3JzHE5MJ8Nip0XVcljtKuZsGxV8Lu+LaVk2FAVXggfk+vsqib+306lZOBwaERW88y1zNsNCxUsnr6qqoSi47dP5bXN+chL/vFy5jaqqEXc+k8iKPoWuO4fF7iDT4qCcj8c1LwvObT6QYKZTzYrXtP8lpmXj62nIc3+/aLHjk8/fgd2h4tA0TAY9dodKps3hti/kSM20EnP2Is0iyrlNz7Y5OHfRSuXAGz/ksSx8fwtRnCSpFkIIIcQNJ9/f4t+m5LqZbpCccwKz2XyVkkIIIYQoK3K+t2/yvj0hXG76pDo9PR2A8PDwUo5ECCGEENcqPT2dgIDiGT4pRGm66Yd/qKpKQkICfn5+xT5m0Gw2Ex4eTnx8vPw0VYKknW8MaecbQ9r5xpB2vjFKsp01TSM9PZ2wsDB0OrkZmbj53fQ91TqdjipVqpToOvz9/eWgfQNIO98Y0s43hrTzjSHtfGOUVDtLD7X4N5FTQyGEEEIIIYpIkmohhBBCCCGKSJLqAphMJsaNG4fJZCrtUP7VpJ1vDGnnG0Pa+caQdr4xpJ2FKLyb/kJFIYQQQgghSpv0VAshhBBCCFFEklQLIYQQQghRRJJUCyGEEEIIUUSSVAshhBBCCFFEklQLIYQQQghRRJJU5+Pjjz8mMjIST09P2rRpw7Zt20o7pDJjypQptGrVCj8/PypVqsSdd97J4cOH3cpkZ2fzzDPPUKFCBXx9fRkwYABJSUluZeLi4ujXrx/e3t5UqlSJl19+Gbvd7lZmzZo1NG/eHJPJRM2aNZk7d26ueP4rn9XUqVNRFIUXXnjBNU3auXicPn2aBx98kAoVKuDl5UWjRo3YsWOHa76maYwdO5bQ0FC8vLzo0aMHR48edavj/PnzDB48GH9/fwIDA3n00UfJyMhwK7Nv3z46deqEp6cn4eHhTJs2LVcsCxcupG7dunh6etKoUSOWLVtWMht9gzkcDsaMGUO1atXw8vKiRo0aTJo0iStvQCXtfH3WrVtH//79CQsLQ1EUlixZ4ja/LLVrYWIR4qaliVzmz5+veXh4aF999ZV24MABbdiwYVpgYKCWlJRU2qGVCb1799bmzJmj7d+/X9uzZ4/Wt29fLSIiQsvIyHCVefLJJ7Xw8HBt9erV2o4dO7S2bdtq7du3d8232+1aw4YNtR49emi7d+/Wli1bplWsWFEbNWqUq8yJEyc0b29vbcSIEVp0dLT24Ycfanq9XluxYoWrzH/ls9q2bZsWGRmpNW7cWHv++edd06Wdi+78+fNa1apVtaFDh2pbt27VTpw4of3xxx/asWPHXGWmTp2qBQQEaEuWLNH27t2r3X777Vq1atW0rKwsV5k+ffpoTZo00bZs2aKtX79eq1mzpnb//fe75qelpWnBwcHa4MGDtf3792vz5s3TvLy8tNmzZ7vKbNy4UdPr9dq0adO06OhobfTo0ZrRaNSioqJuTGOUoMmTJ2sVKlTQfvvtNy0mJkZbuHCh5uvrq73//vuuMtLO12fZsmXaG2+8oS1atEgDtMWLF7vNL0vtWphYhLhZSVKdh9atW2vPPPOM673D4dDCwsK0KVOmlGJUZVdycrIGaGvXrtU0TdNSU1M1o9GoLVy40FXm4MGDGqBt3rxZ0zTnl4BOp9MSExNdZT799FPN399fs1gsmqZp2iuvvKI1aNDAbV2DBg3Sevfu7Xr/X/is0tPTtVq1amkrV67UunTp4kqqpZ2Lx6uvvqp17Ngx3/mqqmohISHa9OnTXdNSU1M1k8mkzZs3T9M0TYuOjtYAbfv27a4yy5cv1xRF0U6fPq1pmqZ98sknWrly5VztnrPuOnXquN7fe++9Wr9+/dzW36ZNG+2JJ54o2kaWAf369dMeeeQRt2l33323NnjwYE3TpJ2Lyz+T6rLUroWJRYibmaH0+siLh6qqJCQk4Ofnh6IoRa7ParWyY8cOnn/+ecxms2t6586dWbduHU8//XSR1/Fvc+rUKQA8PDwwm82sW7cOm81G69atXW0YFhZGlSpV+Pvvv6lfvz5r1qyhfv36eHl5ucq0b98es9nM1q1badKkCevXr6dTp065PofXXnsNs9n8n/msnnjiCXr06EHr1q2x2+1YLBZp52K0ePFiunfvzp133smGDRsICwvjscceY+jQoQDExMSQmJhI27ZtXduvKAotWrRgzZo19O3bl7/++ouAgABq167tKtO6dWsUReHvv/+mf//+rF27lnbt2pGdnU12djYAHTt25O233+bkyZOUK1eOjRs3Mnz4cLd27tq1K7/99pvbtJtR8+bNmTt3Lrt27aJmzZpERUWxbt063nrrLcxms7RzMcrMzHRtR1lq13/Gomka6enptGrVis2bN3Pffffd4JYSonjd9E9UPHXqFOHh4aUdhhBCCCGuQ79+/fDx8WHBggWlHYoQRXLT91T7+fkBEB8fj7+/fylHI4QQQojCMJvNhIeHYzDc9KmIEMC/IKnOGfLh7+9f5pJqTdOKZUhKcdM0jcNJ6VQt74OXh760wylVDlVDryvcZ5Rldfxr2yvL6kBDw9uj9A4JFruDixYH5X08Si2GsiQty4aXUY+HQW7SJP7dUlJSaNmyZWmHIUSRydG6AKqq4VCvPjpmVXQSC7bHuU07k5ZFq8mrmPHn4XyWcrfvVCrjlx4gLdMGwLkMC1cbmZOYls3fh5KvWg5g/+k0PvrrKFlWB40n/Emfmeu57cP1aJpG/PlM+n+4gbG/7MecbeNESka+dUSdSuPkuYt8sf4EyenZbvPtDhVV1dA0jbhzmSzadcqt/TKtztu4HTxjZtSifSSmuS+/8dhZvlh/olDbk7O+bJuDY8mX4405e5G98amFWn7+tjjqjlnOX4eSXO2enzs+3ki9sStYsD2OuHOZBcaYlmnj282xnL9ozXN+ltVRqPjAuY1XLpeW5YzzzwOJTP/jEGoe++eRpHR+2BqXa999Y3EUka/9ztkMC7/uTaDumOVctNhxqBr1xq6g/tg/sDlU9sansmL/GcB5ApaebcNqV1my+zQHEtJc2x51Ko2pyw9x0XL59nx2h8qmY2d5f9VRnp232y0Gh6rx7sojbDx2FptDZVW0e7t3mbaG5pNW5tovrqRpGmN/2c+Qr7bx3ZaTfLM5lrkbY2g/ZTWxZy8W2JY7Ys/zweqjrjbdG5/KA59vYf/pNByqRmJaNjaHSss3V1Hj9WVkWR38cSCRZHO2q/0/XH2U6AQzqw8mceHS56tpGmlZNlLSLQWuH5zHFNul9Wuaxvxtcby78gjjftmPzaGiac5jTtOJf9Jkwp/c8s6aPOs5k5aV6/P9cUc8Pd9dS9y5TFf9K/af4b7PNpNty3ufsztUFu6IZ962OP4+nHzV+AtitatY7A6+3XKS6ITc45Jz9tWLFjvPztvNoNmb2R13gdcXR3E2w9l2p1Oz+H3fGcb9sh+7Q+XCRStTlh/kWHI6NofK/tNpee7zOfadSnUdv2wO1dUWV64fIDXTytTlh/hxRzzZNgf3ztrM7LXH0TTNFUte2/fpmuNsOXGO+POZpKRbiE4w8+jc7Xy27rir3LkMC1+sP5FvPf+UabWTbXOQZM4mJd3C0r0JWOwOV8w7Ys+zYv8ZlkWdYcrygzw/fzePfb2D3/YluNVzNsPCB6uPciYty2367/vOcPcnGzmd6j49Pw5VY298KlGn0lgedcbt77uk7Nixg3bt2pX4eoQoaTf9mGqz2UxAQABpaWnF2lOtaRrVRjnvr7lmZFfG/LKf5hHlGNa5Oj9sPYmPyYBRr2PSr9GkXzrorHyxM+kWO/VD/Rm/9ADzt8cD8NOT7fhuy0mW7EngtsahVK/ow9HkDEIDvNDQmLMxNt84fn6qHS2qlne9t9pVnv5+JyfOXuREijOJmHxXQ+LOZdK/SRgNKwe4ymbbHHjodeh0CpGv/Q5A3RA/DiWmX3X7f3isjWtb7vpkE093rcHE36JzlVv7cleqVvDBaldpP/WvXF8kr/apS+faFTmalMELC/bwfPdavL/68j1Jx/evT5c6lahW0ccVI8DdzSszpF0kCtAkPBBwfibHkjOo5O9J1Kk0HpqzzZVYfP1Ia9YeTuGrjTEALH66PdtiznNfqwiW7T9DjSBfKvmZsKsaC7bH0btBCPfM2uwW687RPQjwMjLxt2jCAr1YFnWG1pHlSUjLYllUolvZ1/vW5cG2VUlIzWJH7AVeWxTFRw80I7ycN++tOsKawymU8zaydHhHOk37G4DNo7rx5u8H+X3fGe5pUQWjXuHgmXRaVi3HG/3q5fpVY//pNO7+ZBPDOlejvI+JSZfaf9/4XjQe/ycAnw5ujkGvo0aQD+W8PfAxGag9ermrjie6VMfh0OjfJIw7Pt5Y8IcOPNG5OrPXnQDgt2c7ctuHGwD3/cbTqOOV3nXd9ofX+9ZlUKsIHvpqG3uuOKn5amhLQgO8eG7ebo4m5z5Zq1bRh061KlK1go9r+94Z2IR7WlRhzeFk4s9nEuTnydojyTQLL8euuAuuv6t/CvY3sfX1Hq73FruD9UfOYtArtKtRgTqjVwAw6c6G9KofTJu3Vueq475W4XnWP+rWukxZfshtWpVyXmx4tRuv/LSXH3ecck3//rE2dKhZkd1xF1h1MIlnu9XC06hH0zTqjF6B9YoTpX+6tWEIPiYDP+28XF/s1H6AM9nZGnOOd/88wo6TFwA4NKkPnkY9I37cw6Jdp13LjO5Xj1UHk9hy4rxrmk6Btwc0pko5bzKtduyqxsd/H2PfqTRXmU2vdSMs0Au4nCT7eRqd846f5dM1x6kf5k+z8ECe/G4X97aswrR7mjBiwR4W7b68foCPHmiGUa+jbbUKNJn4p6ttHvt6B1n/SPJ71g/m7maVeer7XW7Ta1XyzbXflPfxoEaQD3Mebo2maSzfn8ibv0Vjzr6c/B1+sw+DZm9hT3wqz3WvxdrDyew9lcbmUd0IDfByO9YMaVeVbzafBCDE35PESydRf73UhepBvqzYf4YftsVTL9SP2WtP5PPJOT8nVdW4//MtbI1xtvsDbSKYfGdDzmZYCfAykmVzEHP2IlXLe5OQlsWe+FTeWLw/V13NIgJ5qksNYs5ezLXfXalX/WCGdoikfY2K3Dt7M9tizlMjyIfne9Rm9OIoPnygOQ99tc1V57PdatKldiVWHUziiW93uj6/p7/fybKoRHaN6cnwH3ax6fg51zp6Nwhm9v9Kphc55/u7atWqHDp0CE9PzxJZjxA3Sokm1evWrWP69Ons3LmTM2fOsHjxYu68807XfE3TGDduHJ9//jmpqal06NCBTz/9lFq1ahV6HSWVVF+4aKXZpJXXtWxhE9fCWvZcJ0YtjipUD6yfycC6V24hw2Kn07S/aVG1HK2rlefTNcevuuz1OjSpDx//fYwP/zp23XU0iwhkd1xqnvMOTuyDp1HHqz/vc0teruTjoefiNfQA52VA8ypUreDNuyuPFKme6xFe3ov481k8170WveoHE38+M1eCcTOadEcDxvxy4JqX++ulLnSbsfa61hni78lvz3Wk5Zur8pw/tH0kczfFXlfd/xQ7tZ9bgpZj+xs9aDU57/Vfq4jy3rSvUSHfk4mSNuvBFjz53c585wd6G0m9yi89Zcnjnavz2br8k+MrXXlieTVeRn2ukwWAtwc04tWfo64pxmv16/CO9P+ocHFOvKMBY6/4m/xf26p8u+Vkgcscf6tvoYfJXYuc7++dO3fSvHnzYq9fiButRJPq5cuXs3HjRlq0aMHdd9+dK6l+++23mTJlCl9//TXVqlVjzJgxREVFER0dXegz1pJKqh/4fIvb2boQQgjxX3Rfq3CmDmhc7PWW1Pf3tVJVFas17+F6QhiNRvT6wl1PdcOGfyiK4pZUa5pGWFgYL730EiNHjgQgLS2N4OBg5s6dW+j7VZbUH2VevU9CCCHEf1HOMKTiVBaSaqvVSkxMDKqa/7AsIQIDAwkJCbnqzSdK7VL/nJvA9+hxeQxkQEAAbdq0KfAm8BaLBYvl8rjdf8ON+oUQQoiy7EhSOrWD/Uo7jGKlaRpnzpxBr9cTHh6OTif3bhDuNE0jMzOT5GTnhdyhoaEFli+1pDox0XnhV3BwsNv04OBg17y8TJkyhQkTJpRobEIIIYS4bPXB5H9dUm2328nMzCQsLAxvb+/SDkeUUV5ezou3k5OTqVSpUoFDQW6607JRo0aRlpbmesXHl87FO0IIIYS4eTkczgtLPTzk3viiYDknXTZbwRdll1pSHRISAkBSUpLb9KSkJNe8vJhMJteDXsriA1+EEEIIcfMoiw9pE2VLYfeRUkuqq1WrRkhICKtXX75XrNlsZuvWrXITeCGEEKIMyXkgjRAifyWaVGdkZLBnzx727NkDOC9O3LNnD3FxcSiKwgsvvMCbb77J0qVLiYqKYsiQIYSFhbnddk8IIYQQpWvxPx7uI/5bIiMjmTlzZmmHUeaVaFK9Y8cOmjVrRrNmzQAYMWIEzZo1Y+zYsQC88sorPPvsszz++OO0atWKjIwMVqxYIU9VEkIIIcqQUxcK95hzUfJSUlJ46qmniIiIwGQyERISQu/evdm40fnU3PPnz/Pss89Sp04dvLy8iIiI4LnnniMtLa3Aert27YqiKLledrud7du38/jjj193zLGxsSiK4upkzcv48ePzXP+Vr7KuRO/+0bVrVwq6DbaiKEycOJGJEyeWZBhCCCGEKIKyn878dwwYMACr1crXX39N9erVSUpKYvXq1Zw753xgXUJCAgkJCbzzzjvUr1+fkydP8uSTT5KQkMBPP/1UYN3Dhg3LlZMZDAaCgoIKXM5ms2E0Gou0XSNHjuTJJ590vW/VqhWPP/44w4YNu676rFbrDb8I9aa7+4cQQgghbqyboJPwPyE1NZX169fz9ttvc8stt1C1alVat27NqFGjuP322wFo2LAhP//8M/3796dGjRp069aNyZMn8+uvv2K32wus39vbm5CQELcX5B7+oSgKn376Kbfffjs+Pj5MnjyZCxcuMHjwYIKCgvDy8qJWrVrMmTMHcF5HB9CsWTMURaFr16651u3r6+u2Xr1ej5+fn+u9zWbj3nvvJTAwkPLly3PHHXcQGxvrWn7o0KHceeedTJ48mbCwMOrUqePqIf/xxx/p1KkTXl5etGrViiNHjrB9+3ZatmyJr68vt956KykpKUX4ZJxK7T7VQgghhLg5KP+BvmpN08iylc4FmV5GfaGGN/j6+uLr68uSJUto27YtJpOpUPXnPLXSYCi+tG/8+PFMnTqVmTNnYjAYGDNmDNHR0SxfvpyKFSty7NgxsrKcw4a2bdtG69atWbVqFQ0aNLjmHmSbzUbv3r1p164d69evx2Aw8Oabb9KnTx/27dvnqm/16tX4+/uzcuVKt+XHjRvHzJkziYiI4JFHHuGBBx7Az8+P999/H29vb+69917Gjh3Lp59+WqQ2kaRaCCGEEAX6L/RUZ9kc1B/7R6msO3pib7w9rp6SGQwG5s6dy7Bhw5g1axbNmzenS5cu3HfffTRu3DjPZc6ePcukSZMKNSb6k08+4YsvvnC9f+KJJ5gxY0aeZR944AEefvhh1/u4uDiaNWtGy5YtAWfvdo6c4SMVKlQo8LbJ+VmwYAGqqvLFF1+4Tj7mzJlDYGAga9asoVevXgD4+PjwxRdfuJLsnJ7skSNH0rt3bwCef/557r//flavXk2HDh0AePTRR5k7d+41x/VPMvxDCCGEEAWy2NXSDkFcMmDAABISEli6dCl9+vRhzZo1NG/ePM+k0Gw2069fP+rXr8/48eOvWvfgwYNdd23bs2cPo0aNyrdsTvKc46mnnmL+/Pk0bdqUV155hU2bNl3rpuVr7969HDt2DD8/P1dvffny5cnOzub48eOuco0aNcqzF/zKE46cJ3k3atTIbVrOo8iLQnqqhRBCCPGf52XUEz2xd6mt+1p4enrSs2dPevbsyZgxY3jssccYN24cQ4cOdZVJT0+nT58++Pn5sXjx4kJdSBgQEEDNmjULFYOPj4/b+1tvvZWTJ0+ybNkyVq5cSffu3XnmmWd45513rmnb8pKRkUGLFi34/vvvc8278iLKf8aU48ptz+np/uc0VS36iaMk1UIIIYT4z1MUpVBDMMqi+vXrs2TJEtd7s9lM7969MZlMLF269IbdqjgoKIiHHnqIhx56iE6dOvHyyy/zzjvvuHqPcx4Nf62aN2/OggULqFSpUpl+krYM/xBCiH+xKkoKXmSXdhhCiGJw7tw5unXrxnfffce+ffuIiYlh4cKFTJs2jTvuuANwJtS9evXi4sWLfPnll5jNZhITE0lMTLzupLYwxo4dyy+//MKxY8c4cOAAv/32G/Xq1QOgUqVKeHl5sWLFCpKSkq56z+x/Gjx4MBUrVuSOO+5g/fr1xMTEsGbNGp577jlOnTpVEptzXSSpFmWWCWtph/CvE0g6OlSqKWf4wPghdZS4K+ZqdNXtIYRzpRZfSSib+5FGK+UQ5TCX6FpqKKfZYHqeTabnSnQ9heWBjS66vXhiuUpJjYH6NTRSTuQ5V8H9Z9ph+t/oo9tWPEHeBIwUfFs08e/l6+tLmzZteO+99+jcuTMNGzZkzJgxDBs2jI8++giAXbt2sXXrVqKioqhZsyahoaGuV3x8fInF5uHhwahRo2jcuDGdO3dGr9czf/58wHmB5QcffMDs2bMJCwtznQAUlre3N+vWrSMiIoK7776bevXq8eijj5KdnV2meq4VraCns9wEzGYzAQEBrtvFFJfI134vtrrKijDOssr0Mt84ejHVfv8NXXdX3R7GGr5hpO1Jdmm1r1r+Of0iRhh/4iHrq6xVm1yaqlFFSeGUFkRxPopgoH4N/9Ov5DHrSJIpV2BZfzJY7DGO/Vo1nrcNL7YYqijJPKD/izn23qRcJYbrVVuJ50/Tq2xV69JGdwiADM2ThpavAOip28HnHu8CEJn9Q67lXzL8yK26bdxlnUg63tcdhzfZ9NFtY5XaHDO+ruk1lNOkaIGYyT0mTodKD91Odqm1OUtAodf1kuFHnjUsYZLtQb5z9MCOnh66XexQa3OOAAzYsaMnr/2pne4AJ9RQkihfqHW1UQ5yn+EvJtn+x3kKPhb10O3kC48ZmDVvGlu+KLDsP+lxUF85yQEtEl+yGGf8miWOjqxXc1/5/5j+d0YbnWMQ8/pMi5uCyt26Ddxn+IsJtiHs16q7zX/L8DkPGP7md0drnrG9AEAI53jd+ANz7H3YrdUCoLNuL994vJ1n3Pfp/+J1w/c8Z3uWpwxLOaGGcr/h7zzLFpcqSgq1lXj+UptR8LEn5+u0cMcnD2xYubYHZnTURfGdxxQm2x7gc8dteZbRofKyYQHb1Lr8rTa7pvqDuEAKgeS1DbFT+11TXVdTUt/fhZWdnU1MTAzVqlWTJzmLAhV2X5Ge6nwYsDPF8Dl36da7pjVXjjDV8NlVe5eqKwn00O0kiAs8pP+DCqQxw/gpbxi+4w7dhlzlGyonGG34Fn8uXppy+TzHi2xaKodoq4umpnIKXzLxwFbg+nWojDJ8T6znA1Tg8k8sv5hG461YeNLw6xXrurpgzhPr+QAzjJ/kmtdEOUY15Uyey/XSbWe8YS56HMz1mEZ1XSLfe7yVz1qc21xFSeZj40xGGJ1PfZpk+MpVYrh+CRtML/CSYSHg/AJvpRzCl0zA2e7ddTvzrL2Hbid/e7xIE+VYrnnTjZ/RWBfDG8bvMWFFh0of3TbCOJtrW/d5Pk4N3Rnu0Odc1eyMu53uADtNT3CLbje/erxOR11UPtvp5E+GW2/dfI83edqwlFkeMwGoTAp/eLzCffq/8qlBo7duG1WUZHzJZIj+D4JIzbOkJxaaKMe4T+9MPHISagBf5fKwgPa6A3mux4csvMjmWcMSauoS+J/+8v0/b9Nt5k+Pl6mhnC5we6801fg573rMYo7HdEIv9Yp30EWx2vQy+zyHMdv4bq5l/qdfyWce7/Gn6WUe0S+ntXIQgIqkFdgT/axhCQBjjN+xxTScR/TLme3xHjs9n6KtLpqdpif50SP3E13b6/Yzz2MyWz2HM8f4NjWU09RUTvGa4QfKYaa5coTvjZPpotsLOHvDF5gmcZd+IxOMcwvc/s66vXzh4bxFlb+SWWDZBkoM4wxfE0i6a9pEw1x+NY1mlOEHXjT8xAD9Br71mJrn8to/3rW81Ds+RP8HvXXbXXN0FM+dHSYbvmSGxyxa6Y7wg8dkaiqneFT/u+uY9cCl5Leffht+ZDLL+B5bPJ/ldv1mFpvGueqpdcX+1EI5zNfGqVRXEgCYavwCfyWLuR7TaKM75Eqor5UeB+MNc+l1qR2qKwks9BhP50uf6ZU2mJ7nK4936KrbU0CNGt8apzDPOJl/tnyOQNLxIptKXGCf6TGOeD7EbOO7bscaPY58h+sEcYGZxo8BeMOY/wnEXx4v8ZThV+Z4THf1ajdXjhDKOfTk//P/o/rf2e75DC8Yfi5gO4UQ+ZGe6nyMfuMF3jTOcb3vZHmP9aYXXe+HWl9hq1qXLJxnLCasdNPtZoPaiCjPxwqse4j1VU5rFblLv4G59j7s8HwKgB/tXfjO0YOlpjFMsg3mS0c/fvSYQGvdYbflz2l+tLZ8QoSSzEjDAkzYeNb2LBoKNZXT/OgxCW/lcsL2uu1RTmihzPd40zVtgGUcMVoo5/FHh8q7xk9I1CqQqZmY5eiPFSNtddFuywDYNR3L1DY8ZxtOKOfZ7PksABNs/yNFC8SGgbeMX3CH9U02mJ7Pc/tfsj5JAhXYrNbnLcMXVFbO0UW/L9/2mma7l066/bTTR7umjbcNIQsTbxs/B2CG7R5eupSID7KMYatWz62OWM8HXP9f6WhOT/0uzml+HFAj6azPPwGukz0XGwZUdG51ACx3tOJW/XbaZH/EVs/cvdY5vWafG2fQU7+THpZp9NLtYKGjC9s9nwHgkBrO67ZHWWQa71quQ/b7LDONIuBSwjXb3o9WusO8aXuQLExkYmKBxyRClAsALHW043b9Zo6rodxmnUxf3Tb+Upty4VJv6UKP8bTSHSFL88BLyZ2A5sQ51vANjxhWANDVMgM9KjOMn9JU5/4TfLRalb7WKXxufIee+l2u6UOtr7BGbep6/7R+Ce11B0jQKvKafRjqpXP4f7ZjXupkz6W3bjvr1UZcwJ8tpmdc23tlO230fJ4ULYBOlpm00R3CigF/LnJMq0wFzPxomuS2jFXT46HkTioetr7M32ozXjbM5xnDUuLUICJ0hXu61nPWZ5hunI1JufyTfJ3suTygX81utRbvGGfxm9qW7+3d+cbjberp4tyWz2n/nHZZ5OjIWkdjflXbc8LzQVe5H+1deN3+KMc8h+QZx3Drs2xSG3Aefx7V/05f/TbWORrzotGZIH1l7+P6fC+v+3vu0m1gmvEz5jj6sFWtSx0lns8ct2HHQE/dDqooKfTU7eRN+4OY8bn0S1FuIZxjy6XjwT/NsN1Dd/0ut33pgFqVBrqTebbFt8a36KTf7zbvqFqZPtapHPf8X57ryFm+PGb66LezxNGBJrrjGHCwR61JH/021jia8LvpDVY5mrFPq8FUo/NXgobZX7D/iuP25R5vjUf0Kxhr/NY1b5TtUZY52tBIF8NGtQHapf26HGZ2ezofsTzH3pu37IP51PgeW9V6fO64DT8yifJ8DFVTSCYw1/68Q63NBNsQphtnU1cXT5Psz1DRYUdHFp7UVeJYYXrNbZm+lreorJzFh2yWqB1w9i5rxHoOdiv3lPV5PvV43/X+uBrKs7ZnOayF40cmtZTTbNfquC0Xmf098z3epDJnucU6AzsG6akW/1mF3Vckqc7HV6MH5foCykuMGkwavhzXQhmgz90LXRQ1s7/J9wu0uDxhfZHZHu+5TVvpaMFGtQHjjd/ku9w6R6MCk9HS9pT1ebIwcbd+PbfrNxe5vkzN5HaiUhjrHI04olXhMcPyIq//erxse5yFjq6FSmITtPLEqiG0v+LEpTSZNW9XL+4P9ltcPZwlqTj36SvjL4wULYAg5dou3MlPZPYPhfrMr2aSbTBjjLlvX3Ve82WxoxMP6leSoFVAh0ZVXdHv7wqQrAVSSUktlrqKaomjPTFqqOukJC8H1XDq6fIeo7rK0Ywe+t0A9LBMY5XplRKJs7jMsffmYUP+Dz6JzP5BkmrxnyVJdVGNL/y4TSHKqguaL+WUjNIOQwhxk6ue/R0npvYv1jolqRY3CxlTLYSQhFoIUSzyGmsuhHAnSbUQQgghCjTXY3pphyBEmVcmkuqPP/6YyMhIPD09adOmDdu2/XfuNyqEEEIIIW5+pZ5UL1iwgBEjRjBu3Dh27dpFkyZN6N27N8nJxXPhixBCCCGEECWt1JPqd999l2HDhvHwww9Tv359Zs2ahbe3N1999dXVFxZCCCGEEKIMKNWk2mq1snPnTnr06OGaptPp6NGjB5s3530bNIvFgtlsdnsJIYQQQohrExkZycyZM0s7jH+NUk2qz549i8PhIDg42G16cHAwiYmJeS4zZcoUAgICXK/w8PAbEaoQQgghRJkwdOhQFEVh6lT3p6kuWbIERcn9iPn8bN++nccff7y4w3NZtGgRPXv2JCgoCH9/f9q1a8cff+R/P/SbXakP/7hWo0aNIi0tzfWKj8/7xvtCCCGEEP9Wnp6evP3221y4cOHqhfMRFBSEt7d3MUblbt26dfTs2ZNly5axc+dObrnlFvr378/u3btLbJ2lqVST6ooVK6LX60lKSnKbnpSUREhISJ7LmEwm/P393V5CCCGEEP8lPXr0ICQkhClTpuRb5ueff6ZBgwaYTCYiIyOZMWOG2/wrh39omsb48eOJiIjAZDIRFhbGc8895yprsVgYOXIklStXxsfHhzZt2rBmzZoCY5w5cyavvPIKrVq1olatWrz11lvUqlWLX3/99bq3uywr1aTaw8ODFi1asHr1atc0VVVZvXo17dq1K8XIhBBCCPGfomlgvVg6r+t4uLVer+ett97iww8/5NSpU7nm79y5k3vvvZf77ruPqKgoxo8fz5gxY5g7d26e9f3888+89957zJ49m6NHj7JkyRIaNWrkmj98+HA2b97M/Pnz2bdvHwMHDqRPnz4cPXq00DGrqkp6ejrly5e/5u29GRhKO4ARI0bw0EMP0bJlS1q3bs3MmTO5ePEiDz/8cGmHJoQQQgjAohkwlXYQJc2WCW+Flc66X08AD59rXuyuu+6iadOmjBs3ji+//NJt3rvvvkv37t0ZM2YMALVr1yY6Oprp06czdOjQXHXFxcUREhJCjx49MBqNRERE0Lp1a9e8OXPmEBcXR1iYs41GjhzJihUrmDNnDm+99Vah4n3nnXfIyMjg3nvvveZtvRmU+pjqQYMG8c477zB27FiaNm3Knj17WLFiRa6LF2+0V2zDSnX9QgghRFlxXKtc2iGIfLz99tt8/fXXHDx40G36wYMH6dChg9u0Dh06cPToURwOR656Bg4cSFZWFtWrV2fYsGEsXrwYu90OQFRUFA6Hg9q1a+Pr6+t6rV27luPHjwO4TX/yySdz1f/DDz8wYcIEfvzxRypVqlRcm1+mlHpPNTh/Uhg+fHhph+Fmp1q7tEMQQohi8529Ow8aVl+9oBB5GGl7gmWlHURJM3o7e4xLa93XqXPnzvTu3ZtRo0bl2QNdWOHh4Rw+fJhVq1axcuVKnn76aaZPn87atWvJyMhAr9ezc+dO9Hq923K+vr4A7NmzxzXtn9e7zZ8/n8cee4yFCxe63Ub536bUe6rLquNaZZ6wvsDdlvG0zP4033JmzYufHZ1KPJ69avVClfvW7r6zZmievGMb6DZthu2eYovrehxQq7r+/24pxtLJ8h7JWuB1L79Hrc4E2/+KLyDgeevThS77s6Njsa67NB1XQ7nf+sZ1LRuj5v5Va5rt+n5atGgGLmr5/8h9Qs37AurCOq1VKFS5g2qE6/8dst/ne3v3a17XVrWu6/+tsj9hvP2hAst/aL+Tuy3jedH6FACH1Sqc0/xc8w+rVa45BoA4Nei6lkvXvK5aplb2N/SyvH1d9d/MHrW+dF3L/eFoSfvsD65r2Wgt8rqWu6koinMIRmm8ruE2eHmZOnUqv/76q9szPurVq8fGjRvdym3cuJHatWvnSoxzeHl50b9/fz744APWrFnD5s2biYqKolmzZjgcDpKTk6lZs6bbK+fGEldOu7Inet68eTz88MPMmzePfv36FWk7y7oy0VNdVv2htnb9v2b2NxzzHALAY9aX2KXWopXuEKvUFihoLHZ0pKESw2vG+QA0zv4cM97UU+LQoeKJlZ76ncxzdOOkFkwVJYWaymma647yof1ujl6qO0c/y1sEKanM9ZgGwBDra9RR4qmni6Ovfitb1bp8br+Ni3jyu8co6umctxZcrzbif6xy1bPc0ZqPHHfxkeMuYj0fAOAcAdTL/ordpifwVGysdLTgV0c7PvD4KFcb/OTozD36dW7TOlne40XDz9yt3+CadkStzF3WiRzwfBSAJ6wv8Ifa2rXOHAvsXXnV/jgNlFgy8OSkFsL3ju4owEKP8VTTud8JJlqtylq1MSscrUjRAsnExC26Pbzn4TzR+Z/1NYKVC1QilZVqC8YYvqWzPsq1/A61NlFqNSba/0cAF9nj+YRrXrwWTGvLJ+w0PUEFJR2AdY5GbstfqZflbf40vep6f6f1TQC+cfTCAxurTSMJU87TPvsDDIqDdaYXAbjPOpr3jJ/wi6MDmZqJCkoavzra8ZNpIg9bX2ak4Uca6E4C8IvakV+yncnyIP3fvG383LW+BK08Ycp5APpa3iJai+Ql21P4k0ll5Sw9dTvRKw6eNywGIFXzIVC5CEB3y3SOa5XxJRM9KjWUBKK1qvTU7eRDj494xzaQHx1d2eb5DABv2gYzUL+WOrpTrnZ5wvYivmTTRneQuro4DqpV+djD+QXd0fI+P3pMcMW31NGOGfaBnNKCKE8633m8xS+O9rxi/NG1PamaD7vVmixw3MIKtTWtlcs/XXaxvEu8Vonh+iWMMP7ENrUOg6xjGKxfzQ61Dh7YWGpyjhNc6OhCfe0kt+m3AjDVdh+zHP3d1nWHZSK1daeYbvwMcJ5smvF2xQsQmf094Pxi66XbTqSSyBEt3PU3GJn9PQoaTZXjBCgXSdLKcVCrmmsfB5hlv4059j6UUzIIUlJZrzbCGws2DHTQRTHXY7qr7DTbvdyl30gt3WnXtFutUwENHRoqOt6wP8rgS73MPzk6k6wFUl05Q10ljhVqa5Y4OjDa8C11dPEEKWa2qPW4zzoGE1a8yeYCzl6jGtnf0lO3k0Alg1TNl51qLaoriWzV6rq2fZdWm8XZOR0FGs2Vo+zTqmPHQCvlEK8bf6CZ7liubW6aPZsqSgq/mUa7pkWrVelrnUKU6VH8lCy2qnVpozuUa9ka2d9iwEEVJYXmuqM8b1jEY9aRZODJPfp1fGPvRYSSjL9ykTitEg50xGvOk6kjWjjv2QbwovHnAv9+t6p1GWQdSwAZ7PW8fF/eZtmz8FWyWG96kVTNBw/seCsWABK1coQoztuVNcz+Ai8sbL/0NwLwhu0RNBSaKMcZZFjjmj7QMpZDWgRRno+5xXCfdTRb1PrUUeK4X/8XH9vvxKRY6aSL4mdHZxzo8MKCHpVnDL+w3NGa3VotNpmGu/bVOfberFab84ptGCFcYITxJwDOaX6u41heRtqe4CdHF+DS+GjFTowazC3W91z78FDrK6xRm/KkfikjDT9iUFQA5tp75VuvKBsaNWrE4MGD+eCDyydNL730Eq1atWLSpEkMGjSIzZs389FHH/HJJ5/kWcfcuXNxOBy0adMGb29vvvvuO7y8vKhatSoVKlRg8ODBDBkyhBkzZtCsWTNSUlJYvXo1jRs3zjdZ/uGHH3jooYd4//33adOmjesZJF5eXgQEBBR/Q5QyRdOu45LTMsRsNhMQEEBaWlqx3l4v8rXfc01TUAkijWTK5blMG+UgC0yTnMtn/3BN69tvegRfJZvNjvqMsQ/lmObsFQrlHF6KhRNa/hdPKKjEeD4IwBPWF9mm1qGp7jjtdNF8ZL8TM86LHx7T/05n3T6G2V7CggcmrNgwoF76weKfyUHT7Nmk4scA3TpmeMzilFaR7pZ3sODh3EblDN11u1nqaEfKpTapoZymoRLDL2oHQHHVOdN+N0fUKvylNiM7n8tdIpUzrDG598Dk146vGuZRVUniGdtzaP/4wcWfizxtWMpSR7tcvSuHTQ9hUmxudTdQYphu/IyFjs7McfTBiIPVHi8RoUvJFcsG03NUUc7mGZsBO75kkYqzd2+Q/m/8yOQLR8Fn5hVIo7d+B0sd7cjg8k+ACiptdQd5RL+cnvpd9LNMxl/JpIqSwkJH13zr66Tbx7OGxbxmG8YFzRcVHWn4FhhDjna6A9yi28M79nsZql/B68Z57Fcjuc2a90UoNZTTBHCRXVptdKg8ol9OTeU0b9ofdNuWHJ8a3+NW/Xaetz7NKrUFF7ncGxmhJLlORK5sWw9sWDHmquth/XL66LfzsPUVMvEENHzJumK9GpFKIrFaqGuZfaZH8VeyeMQ6kr/U5tRW4nnD8D3v2u9hr1Yz1zo8sLHMYxSHtHCG257Pt82G6v/gLfsDTDZ8yRq16VU/86663Uw2fsWz1mfZpTmHmh03DUavaLm2P0cYZ+mj386Pji55ti0498F2umh2qrUvtUnJ6KzbS3/dZjar9dmt1cKBjrhLSa4eB8c9nb/gzLLfxlT7AwSSTnXlDLu0Wjyh/42qSiKzHf3po9vOd44ebvtBUQVznnHGb/jC3pddWm0G6tfwjP4XHrWNdI0LfkL/K331W3nQ+jrp/2jL8pjpotvLn2pLsvFgomEuW9R6/Kq2B6C5coRPPWbysu0J1qlN3JYdpv+NMOUcE+xDAIXB+lX8T7+S5Y7WWDHyqeP269wqjUqk5vvdAxBEKh96fMgutRZPG5bmmn/lPhVEKg8aVrLAfgsJVGSy4Utq6U5xv3U0Di73YFYkjS66vfymtsWCB7FTi7eXsaS+vwsrOzubmJgYqlWrhqdnyf29lIShQ4eSmprKkiVLXNNiY2OpU6cOVquVnNTu559/ZuzYsRw9epTQ0FCeffZZRo4c6VomMjKSF154gRdeeIElS5YwdepUDh48iMPhoFGjRrz55pt07+78pcxms/Hmm2/yzTffcPr0aSpWrEjbtm2ZMGGC211CrtS1a1fWrl2ba/pDDz2U711IyqLC7iuSVOcjr6T6ahooMfxucv6Efa1JtQ9ZlFMyOKVd30+l0w2zaKw7QX/r5DwTkMK4XbeJ8ca5PGEdwS6tltvB9cpes2uRk1Q/aX2BFVf0/OdHjwM/MnnVMJ8taj1+UYt3iENBSfGVwpUkXjAsYoB+vWva1ZLqkqPhjaVEk6S86FBppzvAPrVGrsTj+mn4k+k60funvrotnMefLWr9Ylqfu3KYqakksF2rQ07P7NVp11D2+nXQRTHLOJM3bI+wVO1w9QXKsJy/+1n2/ky131/K0fz3eJHNdONnbFPr8IzhF352dGaa/b4i1ytJtfivKuy+IsM/itEBLZJZ9v6c0ipe87IX8eJiIcYQ5udl+5MU9ct/qdqepZZ2+dShoF5H3UOsr9JUOc4fastClXegJxU/RtlL5u4rj1lHMtn4Je/YCx5zG68F85LtKU5pQTxvWMRX9j4AfGy/gynGL/nV0bZE4subcsMTagAVHRvVvHsfrp+Sb0INsEwt2Xa9gD/btWv98i75hBpgo9qIxpbPc/3ycnO7qftsblpZeDLc5nxoxzeOXtyofViI/zpJqouVUsq9MsVx4Czeg+86tQnraHL1gjfIIS2CAdYJhS7/nn0ASx3tOHFpCME8R3c2q/VdP3ULUZz+XQk17FFzD6kRN5ok1ELcKJJUC1EgJdf9Wa8coyuEyO0WywwaKjGsUFuVdihCCHHDSFIthBCiWMVoocTIyacQ4j/m3/VboxBCCCHENbjJ79cgboDC7iOSVAshhBDiPyfnAShWq7WUIxFlXWZmJgBGY8F3V5PhH0IIIYT4zzEYDHh7e5OSkoLRaESnk35G4U7TNDIzM0lOTiYwMDDfJ1HmkKRaCCGEEP85iqIQGhpKTEwMJ0+eLO1wRBkWGBjoehx7QSSpFkIIIcR/koeHB7Vq1ZIhICJfRqPxqj3UOSSpFkIIIcR/lk6nkycqimIhA4iEEEIIIYQoohJLqidPnkz79u3x9vYmMDAwzzJxcXH069cPb29vKlWqxMsvv4zdbi+pkIQQQgghhCgRJTb8w2q1MnDgQNq1a8eXX36Za77D4aBfv36EhISwadMmzpw5w5AhQzAajbz11lslFZYQQgghhBDFrsR6qidMmMCLL75Io0aN8pz/559/Eh0dzXfffUfTpk259dZbmTRpEh9//LFcMCCEEEIIIW4qpTamevPmzTRq1Ijg4GDXtN69e2M2mzlw4EC+y1ksFsxms9tLCCGEEEKI0lRqSXViYqJbQg243icmJua73JQpUwgICHC9wsPDSzROIYQQQgghruaakurXXnsNRVEKfB06dKikYgVg1KhRpKWluV7x8fEluj4hhBBCCCGu5pouVHzppZcYOnRogWWqV69eqLpCQkLYtm2b27SkpCTXvPyYTCZMJlOh1iGEEEIIIcSNcE1JdVBQEEFBQcWy4nbt2jF58mSSk5OpVKkSACtXrsTf35/69esXyzqEEEIIIYS4EUrslnpxcXGcP3+euLg4HA4He/bsAaBmzZr4+vrSq1cv6tevz//+9z+mTZtGYmIio0eP5plnnpGeaCGEEEIIcVMpsaR67NixfP311673zZo1A+Dvv/+ma9eu6PV6fvvtN5566inatWuHj48PDz30EBMnTiypkIQQQgghhCgRiqZpWmkHURRms5mAgADS0tLw9/cvtnojX/u92OoSQgghbnaxU/sVa30l9f0tRGkptVvqCSGEEEII8W8hSbUQQgghhBBFJEm1EEIIIYQQRSRJtRBCCCGEEEUkSXU+bmscWtohCCGEEGXC4DYRpR2CEGWeJNX5GNgyvLRDEEIIIcqEGkG+pR2CEGWeJNVCCCGEEEIUkSTV+WhUOaC0QxBCCCHKhMZV5DtRiKuRpDof5X08MOiU0g5DCCGEKHUtI8uXdghClHkl9pjyf4P1r97Cw3O2cyLlIm/e1ZCMbDtRp9NYsuc0I3vVoX6oPyaDjq83x/LHgSRaVyvPFw+1ZMnu0xxLzuCbzSdpVDmAe1tWobyPie2x52lbvTxPfrfLtY7KgV5kWOyEBXoxbUBjPvzrKPXD/GkeUY4TKRnc1zqCC5lWdp68QP1Qf7rNWEslPxNd6wTxYs/aZFod/HUwmTuahvHN5pO0rlaeUYuiOJ2aRUVfD+5oWpkH2kRgzrJx1yebaFjZn8OJ6dzRtDK+JgOJadlk2x3MGdqK1Ewb87bHcVujMH6LSuBAgpnR/erhbTTQbcYa6of5c/6ilQyLnS+GtKRmJecYO5tDY9+pVI6nZBBR3ocgPw+mLj/MqoNJAAT5mUhJt9CpVkVMBh1nM6xUKefFb/vO0DwiEEVR2HnyAs0iAnmwTVXG/LKfTKuD+1qFM397PE3DA9kTn8orferwv7ZVeWPxfpbuTaBfo1AaVwmgb6NQPl9/goc7VOPPA4lMWX6I8f3rM/7XaFc7fzK4OUv3JFAnxI8DCWZmDGzC1BWHmLctjl+e6cDri6MI8DJSO9gPh6rRr3Eo7648Qv1Qf565pSYeeh1NJv4JwM7RPfg96gynU7MY2j6SLKuDrTHnOXjGzIietbE6VIJ8TdwzazM7T14A4OtHWvPcvN2kZdlYOrwDt3+0kaoVvOnfOIwtJ86x41K50f3q8ebvB4ko702PesGcu2jhf22r8sFfx1h3JIUgPxMXLXY0DbJsDgBe6lmb5HQL3245SYMwf+5sWpnoM2YW7z5N9Yo+1A7249Vb65KebSO8nDd6vcKcDbEs3Xuae1qEc1vjUIL8TIxesp/9p9P44qGW9Jm5ngyLnfY1KnAuw/mZN69ajl71g/l93xlWHEjk7QGNqFnJj8gK3hw8k8787XH8tu8MAPVD/Zn1YAsq+nngZdRzIMHMqEVReHnoef++pvy04xT9GofSbcZaACr6enA2w+r29zewRRUW7jzFnKGteHjudgDG3lafX/acJsjP07V/+XjoWf58ZzpP/xuACbc3YNzSA656TAYdFrvKLXWC2HsqjfMXrdzeJIwXe9Zmye7TtK9RgUyrgwXb49l84hzta1Sgd4MQLmRa2XcqjQo+HnyxIca1XUeT07GrGq/fWo/pfxzG6lABmH5PY/o3CeNYcgZfboihe71K/HEgiV/3JgBQtYI397YMp3pFH576fpfrb+P1vnXZfPwcz3WvxclzmRxISOPWhqGcTs1C0+D8RSsf/nWUQ4npADzdtYar7N2fbOSi1UE5byMXMm00CPPnoweas/ZwMr/sTWB3XCoAQ9tHoijwQo/a+Hsa+HFHPIcS09E06Nc4lN/3nWF4t5r8cSCRORtjSc200qtBCCN71eGHrScJ9vckOd3Cox2rcTbDwg9b4/hkzXEA2lYvT40gX9pUr8CB02n8tu8MDSv7UzvYjw//Oub2mU66syE1g3ypUs6Lk+cyefDLrQDUCPLhljqV+PtwMv2bhPHF+hgyLHbublaZ53vUYuGOU3z09zGmDWhM9BkzczfF5jped60TRGJaNlPubkSdED8ajPuDK58X/Hz3Wry/+qjruLtyRGe+XB/DibMXCfb3ZNZa5/b0rB+MpmmsOphMk/BALlrs+HjouaVuJfbEp7L/tJnQAE/6NQ6lc60gok6nckvdSmRZHcSfz+LBL7fSs34w9UL9+WHrSbw9DNQL9WNPfCp+nkZe6FGLL9bHsCc+lXtaVKFpeCBpWTaC/T1ZtOsU5y9auWi1E38+i8ZVAuhQsyJ741M5lJjOPS2q0LdRKIFeRmLOXUSnKCyPOsPhpHT6NQrlWHIGA1tWIcDLSEiAF4lpWfy+L5GOtSqwYn8iAV5Gdp68wINtq5JktrA99jyLd592tdGgluGMvq0eX26I4dvNJ6lawZtZ/2tBBR9TrvYWQuQmjykXQgghxA0n39/i3+am76nOOScwm82lHIkQQgghCivne/sm79sTwuWmT6rT050/iYaHyy3whBBCiJtNeno6AQFyIaS4+d30wz9UVSUhIQE/Pz8UpXgvLDSbzYSHhxMfHy8/TZUgaecbQ9r5xpB2vjGknW+MkmxnTdNIT08nLCwMnU7umyBufjd9T7VOp6NKlSolug5/f385aN8A0s43hrTzjSHtfGNIO98YJdXO0kMt/k3k1FAIIYQQQogikqRaCCGEEEKIIpKkugAmk4lx48ZhMsk9OkuStPONIe18Y0g73xjSzjeGtLMQhXfTX6gohBBCCCFEaZOeaiGEEEIIIYpIkmohhBBCCCGKSJJqIYQQQgghikiSaiGEEEIIIYpIkmohhBBCCCGKSJLqfHz88cdERkbi6elJmzZt2LZtW2mHVGZMmTKFVq1a4efnR6VKlbjzzjs5fPiwW5ns7GyeeeYZKlSogK+vLwMGDCApKcmtTFxcHP369cPb25tKlSrx8ssvY7fb3cqsWbOG5s2bYzKZqFmzJnPnzs0Vz3/ls5o6dSqKovDCCy+4pkk7F4/Tp0/z4IMPUqFCBby8vGjUqBE7duxwzdc0jbFjxxIaGoqXlxc9evTg6NGjbnWcP3+ewYMH4+/vT2BgII8++igZGRluZfbt20enTp3w9PQkPDycadOm5Ypl4cKF1K1bF09PTxo1asSyZctKZqNvMIfDwZgxY6hWrRpeXl7UqFGDSZMmceUNqKSdr8+6devo378/YWFhKIrCkiVL3OaXpXYtTCxC3LQ0kcv8+fM1Dw8P7auvvtIOHDigDRs2TAsMDNSSkpJKO7QyoXfv3tqcOXO0/fv3a3v27NH69u2rRUREaBkZGa4yTz75pBYeHq6tXr1a27Fjh9a2bVutffv2rvl2u11r2LCh1qNHD2337t3asmXLtIoVK2qjRo1ylTlx4oTm7e2tjRgxQouOjtY+/PBDTa/XaytWrHCV+a98Vtu2bdMiIyO1xo0ba88//7xrurRz0Z0/f16rWrWqNnToUG3r1q3aiRMntD/++EM7duyYq8zUqVO1gIAAbcmSJdrevXu122+/XatWrZqWlZXlKtOnTx+tSZMm2pYtW7T169drNWvW1O6//37X/LS0NC04OFgbPHiwtn//fm3evHmal5eXNnv2bFeZjRs3anq9Xps2bZoWHR2tjR49WjMajVpUVNSNaYwSNHnyZK1ChQrab7/9psXExGgLFy7UfH19tffff99VRtr5+ixbtkx74403tEWLFmmAtnjxYrf5ZaldCxOLEDcrSarz0Lp1a+2ZZ55xvXc4HFpYWJg2ZcqUUoyq7EpOTtYAbe3atZqmaVpqaqpmNBq1hQsXusocPHhQA7TNmzdrmub8EtDpdFpiYqKrzKeffqr5+/trFotF0zRNe+WVV7QGDRq4rWvQoEFa7969Xe//C59Venq6VqtWLW3lypValy5dXEm1tHPxePXVV7WOHTvmO19VVS0kJESbPn26a1pqaqpmMpm0efPmaZqmadHR0Rqgbd++3VVm+fLlmqIo2unTpzVN07RPPvlEK1eunKvdc9Zdp04d1/t7771X69evn9v627Rpoz3xxBNF28gyoF+/ftojjzziNu3uu+/WBg8erGmatHNx+WdSXZbatTCxCHEzk+Ef/2C1Wtm5cyc9evRwTdPpdPTo0YPNmzeXYmRlV1paGgDly5cHYOfOndhsNrc2rFu3LhEREa423Lx5M40aNSI4ONhVpnfv3pjNZg4cOOAqc2UdOWVy6vivfFbPPPMM/fr1y9UW0s7FY+nSpbRs2ZKBAwdSqVIlmjVrxueff+6aHxMTQ2Jiotv2BwQE0KZNG7d2DgwMpGXLlq4yPXr0QKfTsXXrVleZzp074+Hh4SrTu3dvDh8+zIULF1xlCvosbmbt27dn9erVHDlyBIC9e/eyYcMGbr31VkDauaSUpXYtTCxC3MwMpR1AUamqSkJCAn5+fiiKUuT6zpw5g8PhwNfXF7PZ7JoeGBjIgQMH3KYJZ/s/88wztGnThoiICMxmMydOnMBoNKLT6dzaq2LFipw8eRKz2UxcXBwVKlRwm+/l5QXAiRMnqFGjBqdPn+aWW25xK+Pn54fZbCYpKYnU1NR//Wf1008/sX37dtasWYPZbMZut2OxWKSdi9Hx48f55JNPGD58OD///DO7du3i2WefxeFw8MADD3D8+HEAvL293ba1fPnyxMfHYzabiY2NzdXO4GyjmJgYzGYzp06domrVqm5lfHx8ADh27Bh16tThzJkzrrbP4e/vT0JCwk3fzk8//TQpKSnUqVMHvV6Pw+Fg7Nix9O/fH7PZLO1cjDIzM13bUZba9Z+xaJpGeno6QUFBJCYmlkxjCHED3fSPKT916hTh4eGlHYYQQgghrkO/fv3w8fFhwYIFpR2KEEVy0/dU+/n5ARAfH4+/v38pRyOEEEKIwjCbzYSHh3P+/Hlq1KhR2uEIUWQ3fVKdM+TD39//v5lU2y2QnQa+lUo7kqLJugCqCj4VilbP4eWgaVC3b/HE9U/ZaeAZUDJ132hpp8EvBHT60o7kMmsmXIiBSvWhoOFcmgYrRkFwfWg+5MbFB5BthvkPQP07oPWwklmH3QI6Q+l8NpnnITkaqnYo+DO4EWI3gH9lKF/t8rRTO53vvcuXXlxlxcWz4FOx5Oo/vRM2fgA9J0C5yBJbzc6dO3nuuedKrH4hbhS5ULEgKUdg/mAwnwFbFqydBhvfh8T9zi+9fQth1zcQtxUWPQGJUc4ve3CWt1vc67NlgyXd+f85feG7Ac7/a5rzlRp3uWx2GpzaAaoDLBmwYabzyyTb7JyW4+M28E4t+PYueKsKfHs3nD3qTFB3fQtxW+DDlrDm7fy3MyemKzlszm2wZUPyIfjlGbgQ69ymK0cMZZsheqkzxrgtMLsznD12uc6P28Dc22D3dzA+AD5p71wGwGF3Lqeq8HYkTK/u/JJIOeJM+HLqz2+E0uld8P29zvhy2nzefTD/fmeSfu745fa1ZYE5AbZ97ozx7ynO+VtmwYWT+bcNONtbVWHrZzA1AmbUdX4+mgbH/4LPu0PsRmcb//GGs41y1r11tvOzOXsMMpJhzzyY1dH5ObnqVyHtlHM9sRud+9gvw+HQ785pJzfBxXNw8Fc4tMx9ub0L4L2GsOMr5/u82K2X553cBPt+hBNr4L368P3Ay+VsWc7P/eI55/tVE+DTjs5tsWQ4tzd+Oxz5I5/1XNruY6svf2aOS/fDtmZe/pzyomkQvw3eCoVP28OEQPjmjsuf3ZXMZ2D7F7D1U1j6LJyPgXXTnftDTlmHzbndlgzn/69cDzj/hmPWO/+fkQzH/3Zfly3LGc87tSH6l8vLX4iFNVMhdj0sGwkJu+GvybD7e+c25lBVZ1v89Khzn74W1kyYUgUmloek6Nz75z/b1mFznkweWnZ5niUdTm52vj+xFn4cAjvnOl+xG5zHiH0/OrczI+Vy3SlHYFo1mNsPVo6F9ETntlgvOutKPuRcb9opOH/icjw5sVx5LNE05/HjyvfnY+DoKviip/P4lJ7k3F+2fQ7Ta8Hmjy+337HVzjg+aAqHVzjrOv4XfNHNGeOVjv/lbKt/slud/57c7Iw9frtzv75yn0o77fwMUw7D/kVwZq8zzv2LnPu06nBv65TDkLAHNn10eftyPvvM8/kfr3bMcR4DxwdcPgZe6cw+Z3vnRVWd27BvofMzWjvNue9Pr+E85iRGOY8dp3c5YyloVKemOdt52Suw/t28y6oOSDoAn3eD6CWw4H/511cMQkNDufPOO0t0HULcCDf9mGqz2UxAQABpaWnF21NtPgPv1i2++soar/KQdb60oyi8kEbOL/acL/K8lKvm7OW8klc5Z4J9rere5vziNJ+69mX/S+r0hQo1nEl6YtS1L9/mSdg66+rl6t8BfmGgqbBt9rWvB6BibTh75OrlDJ5gz756uWtx5d9bvf7OEyQAozfoPSA7tXD1RLSDuCvuklCpvrNX+UpNHoC9PxQ55FJTpRXojBC3qWj1+FcpG3+/QXUhJZ8TygFfws+P5p7e+nG4mAIHFhd9/eWrQ7MHYfXEq5et0R2Or8573vi0osfyDznf3zt37qR58+bFXr8QN1qZSKo//vhjpk+fTmJiIk2aNOHDDz+kdevWhVq2xJLq8f+Sn/iFEEKIorrnK2g4oFirLLHv72ukqipWq7XU1i/KNqPRiF5fuKF4pT6mesGCBYwYMYJZs2bRpk0bZs6c6br3ZaVKN/k4YSGEEOLf4KdHij2pLgusVisxMTGo+Q2fEwLn7SVDQkKueuvmUu+pbtOmDa1ateKjjz4CnGeM4eHhPPvss7z22mtXXV56qoUQQogboJiHgJR2T7WmacTFxWGz2QgLC0Onk8vMhDtN08jMzCQ5OZnAwEBCQ0MLLF+qPdU5T2obNWqUa9rVntRmsViwWC5fAPhvuFG/EEIIIW4su91OZmYmYWFheHt7l3Y4oozKeWBacnIylSpVKnAoSKmelp09exaHw+H2CGWA4ODgfJ+uNGXKFAICAlwvefCLEEIIIa6Vw+G8k9aVj14XIi85J102m63Acjfdbx2jRo0iLS3N9YqPjy/tkIQQQghxk7raOFkhCruPlOrwj4oVK6LX60lKSnKbnpSUREhISJ7LmEwmTCZTice2ztGIzvrruEWYEEII8S9z0lidqqUdhBBlXKn2VHt4eNCiRQtWr758X0xVVVm9ejXt2rUrxchgiG0USVpgqcZQWKpWuDOozY76JRxJ3jI0zzynR6mRNzYQIYQQ12Vy+SmlHYIQZV6p31JvxIgRPPTQQ7Rs2ZLWrVszc+ZMLl68yMMPP1zaodHG8gkARux8ZPyA3vodTLT9jycMvxKspHK7ZRJddHv52tGLhR4TycaDA2ok79vv5mHDHzxp+JXXbI+xwNGVipipqTvNvfo1bHA04pQWxGvGecy0D2Ct2gTQMGHDh2xGGX6gl34HTS2f4Us26XgBCi8aFqJH5VbdNp60vUiiVp6mumNsVBuiouNVwzy663ZRW3ea5tmz8FcukqiVJ5srevZt0Ew5ipdiob5ykm1qXcx4E6u5X9FaHjPfe7xFPZ3zKY+Nsz9HQyFMOUsgF8nAizitEunkXNyhUZ50WukOM9vjPQZYxtFEd4JTWkX+VFtRiQtkYSIDTyKVJGK0EEDhdt1G+um3MsfRh3a6aD6y34kvmXhi4zx+KGj4kM0g/Rp667dzh3USoFAOMwHKRYJIY7bHu/S3TAaginIWM97cod/Ex/Y7yMKDR/XLsaNjpdqSM1oFQpVzrDO9CMAj1pHsVmvyqGE5d+o30tMyjaH6P3nEsJz37Xfzo6MrHthwoCMLT4bpf+MN4w/Ms9/C/Ya/2arWZYOjIT5KNp/ZbyMNH1oqRzhNBZK08nxmnMEt+r1ubdso+wuWeYwiXJdCV8sMYrVQ2igHmefxJgsdXfjAfhcbPZ8H4JgaRi/rNFR0tFIOcZt+M+vUxnz5//buOzyKan3g+He2pwdCCoGE0GsQEAhFEKVEzKUoP1CIChZQhCuo4AWVIoggiGChWEEvKupVuV5FFFEQEWnSQWogtCQoJCGE7ef3R8zCmgQCSdgE3s/z7PNkzpw5887Zze47Z2fPmGYCMMrxMIm6DbzlTOIM/tTXjrBR1aexdoggLZfB+q+przvKdEc//uO6meeN73KTbgf+mo2N7npEa38QrZ3iTttEzuDPWMOHHFNVcKHjN3c9Dqu8aS3763+gu349IVouox1D+NkVTxNdCgP0K5jp7EtV7RQ36vZyTFXhH/pf+czVgWcMH9DbPpkUVZUJhve435B3J8Z77WOoqx0jVUVwWEWyT1XHiJMQzvIHIfhjZZflAa8+G+t4kI9cnfP+L7XdfGye7Fl3h+05Oum3sNSVwF5VHYXGEP1X7FJxtNbt5jHDEtJUJd53dmWDuwH3Gb6jh/5Xz/bb3DVpqsu7aVCc9UMqkY0FB3G6NDa66+NCR23tOJMMC0nQ7WaI4wnWuBsTqZ3miIrAhZ5E3Qb2q2isysRQw5fcY1jBL65GzHH1YrThYza76zLZeS89db+wRdXmkKrKg/qvySIQp9LzrbslDbVUZhjfoLbuhCe2X90NOaEqU187yh3257CRd+3nUP2XJOo3kGx/mhjtJP/Qr+U15x2Ea5m01n7nB3dz+uhXo8ONERcxWgbr3A1xYOBrdwLPGRayT1VnkasrjbUUHjN8wSvOO2mp28MyV2ta6vZgw8g7ppnsd0fTzz4eAy666Tey2h3PKvMTABxwV+Uj160cUeF8776RSuTQSvc7y9034sSAP1YaaKkYcLFeNcCMAxtGIG8gQMONDoVCQ4+bUHI4h4ll5jF87UrgbWcSOVg4R96JuQEnFuzssDzEWWWmlW0erXR72O6uyX2G71jluoHNqi5GnLTU7WGXuwZZBFJTO0GmCuA0QVTjDwyai8MqiuH6Lxhl/BSA+c5/sMjVhabaQQ6qaI6rMHLwoxJnqKr9ydOGD3Gj8ahjJJ+YJrHO3YAJzrzPqXraET43TeBe+1g2q7oAtNdt5wPT+UT0FecdPKj/hvGOQXzlbstey0AAJjgG4oeNMcbFnrrZyh8zdsxa3p0cu9im87D+K5a427PGHY8FG421Q4w3/hsHBpLtT7PHMsjrf+ZD5y3MdfXiqApHQxGrZXBKBeNA7/lMOGQZ4Knf0PoudoxsMQ8hSDvHcteNDHY8SV/9StxKR66xEuL6FRcXx8iRIxk5cqSvQynXfD6lHsDrr7/uuflLs2bNePXVV0lISCjWtmU1JU/cmK+9lg04qaMd53cVQ/4HgjdVoDyYHLIJLLWYrjYNN4/qv2Sjuz7rVENfh1NumLF7kptLCSOL543v0lBLJU6XzsP2kXzrbo0ZOyGcJYPCP6ieMizmUcOX3GmbyG+qXoH1dbSjHFNVPMnGlfLDSpR2mhR18WmCSkO8dpBMAjiiIi9Z14wdHW4MuD0nlYWtv/TxK8LJ4iShXqUabhQ6z3PZQbeNoyq8VPrBhIOOum386m5IDpc3o0Akp+inX8lHrs78Qfmd1tOMnSpkcYxwn+w/mLPYMBb7/9BX2up28qLhTcY6H2KNO97zuisOP6zco/+e79wtOawKvxwyrz2N8/8fitt169itahTrtTxE/z+eNn7EaMcQPnV1AiCEHG7Tb2CpK+GCQRPoULcK/36weJ/LxeXrKfWsVispKSnUrFkTi6Vk76VX08mTJxk/fjxff/016enpVKpUiRtuuIHx48fTvn17AB5++GG+//57jh8/TmBgIO3atePFF1+kQYOi7xTdqVMnVq1aVaDc4XBw+vRpAgICrniWlEOHDlGzZk02b95Ms2bNCq0zceJEnnvuuYu246uUtbivlXKRVJfE1UqqhSgZRSXOcJriv0YvJ3kXQogrEUSuV/JclJpVAvhxVKdS3bck1VemY8eO2O12pk6dSq1atUhPT2fFihU0btyYnj17AvDmm2/SoEEDYmNjOXXqFBMnTmTLli2kpKQUOSVcp06dqFevHpMmed/SvqjfuF3I4XBgNBqLXF+cpDonJ4ecnBzPcqtWrRgyZAiDBw++rFjy2e32UpvZpbivlQo3+4cQFZN2WQk1IAm1EKLMFSehBkj542wZR+J7Sily7U6fPIo7vpmZmcnq1at58cUXueWWW6hRowatW7dm7NixnoQaYMiQIXTs2JG4uDhatGjB888/z5EjRzh06NBF2/f39ycqKsrrAXmXf8yePdtTT9M05s2bR8+ePQkICGDKlCmcPn2a5ORkwsPD8fPzo27duixYsACAmjVrAtC8eXM0TaNTp04F9h0YGOi1X71eT1BQkGfZ4XDQr18/QkNDqVy5Mr169fI6nkGDBtG7d2+mTJlCdHQ09evX59ChQ2iaxieffEKHDh3w8/OjVatW7N27lw0bNtCyZUsCAwPp3r07J0+eLNZzcDE+v6ZaCCGEEMLXzjlcNBr/rU/2vWtSIv6mS6dkgYGBBAYGsmTJEtq0aVOs2dDOnj3LggULqFmzZqne22PixIlMmzaN2bNnYzAYGDduHLt27eKbb76hSpUq7N+/n3PnzgGwfv16Wrduzffff0/jxo0vewTZ4XCQmJhI27ZtWb16NQaDgeeff57bbruNbdu2edpbsWIFwcHBLF++3Gv7CRMmMHv2bGJjY3nggQcYMGAAQUFBvPLKK/j7+9OvXz/Gjx/PvHnzStQnklQLIYQQQlQABoOBhQsXMnjwYObPn0+LFi24+eabufvuu2natKlX3blz5/LUU09x9uxZ6tevz/Llyy+ZzM6dO5e3337bs/zwww8zc+bMQusOGDDAa1KJ1NRUmjdvTsuWLYG80e184eF5v78ICwu7rEs48n388ce43W7efvttz5zRCxYsIDQ0lJUrV9KtWzcAAgICePvttz3HmT+SPWrUKBITEwEYMWIE/fv3Z8WKFZ5r0B988EEWLlx42XH9nSTVQgghhLju+Rn17JqU6LN9F1efPn1ISkpi9erV/Prrr3zzzTdMnz6dt99+m0GDBnnqJScn07VrV06cOMFLL71Ev379WLNmzUWvCU5OTuaZZ57xLIeGhhZZNz95zjd06FD69OnDb7/9Rrdu3ejduzft2rUr9nFdzNatW9m/fz9BQUFe5VarlQMHDniW4+PjCz1xuPCEI/8u3vHx8V5lGRkZJY5TkmohhBBCXPc0TSvWJRjlgcVioWvXrnTt2pVx48bx0EMPMWHCBK+kOiQkhJCQEOrWrUubNm2oVKkSX3zxBf379y+y3ZCQEOrUqVOsGAICAryWu3fvzuHDh1m6dCnLly+nc+fODBs2jJdeeumKjvFCOTk53HjjjXzwwQcF1uWPghcWU74Lf0SZP9L99zK3213iOOWHikIIIYQQFVijRo04e7boH5MqpVBKYbPZyjSO8PBwBg4cyKJFi5g9ezZvvvkmgGf02OVyXVG7LVq0YN++fURERFCnTh2vR0hI+Zl+VJJqIYQQQogK4M8//+TWW29l0aJFbNu2jZSUFD799FOmT59Or169ADh48CBTp05l06ZNpKam8ssvv9C3b1/8/Py4/fbbyyy28ePH89///pf9+/ezc+dOvvrqKxo2zLvHRUREBH5+fixbtoz09HSysrIuq+3k5GSqVKlCr169WL16NSkpKaxcuZLHHnuMo0ePlsXhXBFJqoUQQgghKoDAwEASEhKYNWsWHTt2pEmTJowbN47Bgwfz+uuvA3mXhqxevZrbb7+dOnXqcNdddxEUFMQvv/xCREREmcVmMpkYO3YsTZs2pWPHjuj1ehYvzrtTqMFg4NVXX+WNN94gOjracwJQXP7+/vz000/ExsZy55130rBhQx588EGsVqtP5jgvitz8pQhy8xchhBDivEPTkkq1Pbn5i6go5OYvQgghhBBCXCWSVAshhBBCCFFCklQLIYQQQghRQpJUCyGEEEIIUUKSVAshhBBCCFFCklQLIYQQQghRQpJUCyGEEEIIUUKSVAshhBBCCFFCklQLIYQQQghRQpJUCyGEEEJch+Li4pg9e7avw7hmSFIthBBCCFGBDBo0CE3TmDZtmlf5kiVL0DSt2O1s2LCBIUOGlHZ4Hj///DPt27cnLCwMPz8/GjRowKxZs8psf75m8HUAQgghhBDi8lgsFl588UUefvhhKlWqdEVthIeHl3JU3gICAhg+fDhNmzYlICCAn3/+mYcffpiAgIAyTeZ9RUaqhRBCCCGUAvtZ3zyUuuxwu3TpQlRUFFOnTi2yzmeffUbjxo0xm83ExcUxc+ZMr/UXXv6hlGLixInExsZiNpuJjo7mscce89S12WyMGjWKatWqERAQQEJCAitXrrxojM2bN6d///40btyYuLg47rnnHhITE1m9evVlH29FICPVQgghhBCOXHgh2jf7fvo4mAIuaxO9Xs8LL7zAgAEDeOyxx6hevbrX+k2bNtGvXz8mTpzIXXfdxS+//MKjjz5KWFgYgwYNKtDeZ599xqxZs1i8eDGNGzcmLS2NrVu3etYPHz6cXbt2sXjxYqKjo/niiy+47bbb2L59O3Xr1i1WzJs3b+aXX37h+eefv6xjrSgkqRZCCCGEqIDuuOMOmjVrxoQJE3jnnXe81r388st07tyZcePGAVCvXj127drFjBkzCk2qU1NTiYqKokuXLhiNRmJjY2ndurVn3YIFC0hNTSU6Ou/EY9SoUSxbtowFCxbwwgsvXDTO6tWrc/LkSZxOJxMnTuShhx4qhaMvfySpFkIIIYQw+ueNGPtq31foxRdf5NZbb2XUqFFe5bt376ZXr15eZe3bt2f27Nm4XC70er3Xur59+zJ79mxq1arFbbfdxu23306PHj0wGAxs374dl8tFvXr1vLax2WyEhYUBEBgY6Cm/5557mD9/vmd59erV5OTk8OuvvzJmzBjq1KlD//79r/iYyytJqoUQQgghNO2yL8EoDzp27EhiYiJjx44tdAS6uGJiYtizZw/ff/89y5cv59FHH2XGjBmsWrWKnJwc9Ho9mzZtKpCM5yfTW7Zs8ZQFBwd71alZsyYA8fHxpKenM3HiREmqhRBCCCFE+TJt2jSaNWtG/fr1PWUNGzZkzZo1XvXWrFlDvXr1CiTG+fz8/OjRowc9evRg2LBhNGjQgO3bt9O8eXNcLhcZGRl06NCh0G3r1KlTrFjdbjc2m62YR1axSFIthBBCCFGBxcfHk5yczKuvvuope/LJJ2nVqhWTJ0/mrrvuYu3atbz++uvMnTu30DYWLlyIy+UiISEBf39/Fi1ahJ+fHzVq1CAsLIzk5GTuu+8+Zs6cSfPmzTl58iQrVqygadOmJCUlFdrmnDlziI2NpUGDBgD89NNPvPTSS16zilxLJKkWQgghhKjgJk2axMcff+xZbtGiBZ988gnjx49n8uTJVK1alUmTJhV5iUhoaCjTpk3jiSeewOVyER8fz//+9z/PNdMLFizg+eef58knn+TYsWNUqVKFNm3a8I9//KPImNxuN2PHjiUlJQWDwUDt2rU9c2tfizSlrmByxHIkOzubkJAQsrKyClzDUxJxY74utbaEEEKIiu7QtMJHI69UWX1+F5fVaiUlJYWaNWtisViu+v5FxVHc14rc/EUIIYQQQogSkqRaCCGEEEKIEpKkWgghhBBCiBKSpFoIIYQQQogSkqRaCCGEENetCj5fg7gKivsakaRaCCGEENed/Bug2O12H0ciyrvc3FwAjEbjRevJPNVCCCGEuO4YDAb8/f05efIkRqMRnU7GGYU3pRS5ublkZGQQGhpa5J0o80lSLYQQQojrjqZpVK1alZSUFA4fPuzrcEQ5FhoaSlRU1CXrSVIthBBCiOuSyWSibt26cgmIKJLRaLzkCHU+SaqFEEIIcd3S6XRyR0VRKuQCIiGEEEIIIUpIkmohhBBCCCFKSJJqIYQQQgghSkiSaiGEEEIIIUpIkmohhBBCCCFKSJJqIYQQQgghSkiSaiGEEEIIIUpIkmohhBBCCCFKSJJqIYQQQgghSkiSaiGEEEIIIUpIkmohhBBCCCFKSJJqIYQQQgghSkiSaiGEEEIIIUqozJLqKVOm0K5dO/z9/QkNDS20TmpqKklJSfj7+xMREcHo0aNxOp1lFZIQQgghhBBlwlBWDdvtdvr27Uvbtm155513Cqx3uVwkJSURFRXFL7/8wokTJ7jvvvswGo288MILZRWWEEIIIYQQpa7MRqqfe+45Hn/8ceLj4wtd/91337Fr1y4WLVpEs2bN6N69O5MnT2bOnDnY7fayCksIIYQQQohS57NrqteuXUt8fDyRkZGessTERLKzs9m5c2eR29lsNrKzs70eQgghhBBC+JLPkuq0tDSvhBrwLKelpRW53dSpUwkJCfE8YmJiyjROIYQQQgghLuWykuoxY8agadpFH7///ntZxQrA2LFjycrK8jyOHDlSpvsTQgghhBDiUi7rh4pPPvkkgwYNumidWrVqFautqKgo1q9f71WWnp7uWVcUs9mM2Wwu1j6EEEIIIYS4Gi4rqQ4PDyc8PLxUdty2bVumTJlCRkYGERERACxfvpzg4GAaNWpUKvsoiVf7N+exjzb7OgwhhBDC56bdWfikA0KI88psSr3U1FROnTpFamoqLpeLLVu2AFCnTh0CAwPp1q0bjRo14t5772X69OmkpaXx7LPPMmzYsHIxEt3zhmgq+5v4cP1h+rWMISPbBsCxzHPYXW4Ass45qBbqh1KKo6fPoRREhVjQ6zQ0QKfTsBj1AASa9Rz6MxejTiPQYmDJ5uPUiwwk0GIgvloI2eec2JwuYir7s/vEGWpW8Uev01E/MojPNx8ltrI/LWIrsebAH4QFmLAY9dSqEsi6lD+xGPUc+uMsN9WtQpDFwBurDlInIpDezauRmevgjxwbof5GvvjtGDGV/akXGcTO41nYnW62H8vi3jY1OGt34XC5ybE6qRMRiF6nceqsnUN/nsWo1/HLgT9oFhNKoNmIxaijkr+JI6dyMRt16DSNrHMOjp4+h8Pl5u5Wsew+kY1Rr2F3ubEY9Ww4dJpAs54m1ULItbmwmPSEBZg4kJFDZIgFnaZROcDI1iNZ2JxuDv1xlgCzAZfbTUKtMJxuxU97T3JzvXDSsqyk/HmWG6qHePZ9R/NqfLH5GJUDTHRtFEl6tpVfD57C5nQTFWwh44yVszYnsWEBLN+VTqsalTAadGRk2+jWOJJf9v9BVIgfqadyaRYTisutOHo6l0r+JrKtDox6HQFmA3/k2NAAu9NNaIAJs15HZIiF/Rk5RATlvW5P59rZdPg01Sv5US3Un8oBJnYdzyI82MLR07nUjQjC5Xbz1bYT3FwvHLMxry++35VOfPUQTp21E1PJnwMnc2hYNZizdieV/E0c+vMs4YFmvt5+gvpRQRzIOMutDSLYfiyTjGwbVqeL2+OrEhZgZtPhU4QFmlm97yQOp+L/WlanSqCJQ3/kEmg2cMbmxKDTAEg9lYvzr+cp2+rA7YaTOTaiQiy0iqtEXFgAQ/69iZ43RNOoajCnc+18syONALOBDnWqEFPZj/Upp3ErReUAE/sycmgQFcSNNSqRfc7BscxzZJyxUTMsgGA/AwdOnqVqiAWLUc/+jByCLQbOOdyczrUTaDZQJdCMWylaxFbi8J9nWZdyCqNeY9vRLO5pU4PPfzvKbU2i2J+RQ/Y5JzfWqESA2cDxzHNUCTJRJdBMRraNb3em0al+BJX8jWScsbE59TQ3xISSlm0l2GLEbMh7HW85kolepxEX5k+lABOVA0x8uC6VsEATVUP8qBbqx4GTOUSFWHC7FesPnSb1z7MMv7UuRr2G1eHC32Tg251ptK5ZmaxzDixGPXvSzmBzuoivFoJBpyPU38iBkzlEh/ph0Gms2nuSncezSWwcxZ85dkL9jZy1O7mheijrU07RplZlzAY9adlWss852J+Rg8Wop1lMKAo4cioXnabhb9Kz7VgWVUMsVA4wsftENg2igtl2NJNb6kdwzuEiNswfgFA/IxsOnWLn8WyqhfqRY3MSaDbQPLYSASY9P/yeQdY5B5UDTdyTUIP31x7iprrhHD2di1JQs0oAZ21Ojpw+x4nMc5zItvLYrXXZfiyL7HMOwgJNVPI38dlvR2lZoxJmgx69TuObHWlYHS6SmlYlMtjMV1tP4FKKUD8jgRYDEUEWIoLMHPzjLNGhFrLPOcm1u+jcMAI/o54Faw5RPyqQ9Smn6dUsmm93pjGoXRwzv9tLfPUQfvg9gxaxofiZDEQFW4gOtbByz0nWp5zizhbV0DSN1nGVmb/qAKH+Rro3qcrpXDvLd6XTPDaUw3/mUj8qiK+2Hadd7SrUDg/gjNVJ1jkH9SKD2J+RQ52IQA6fyiXX5sTfbCAt6xx3NK/GG6sO0iquMk63wqjX+POsnUN/nKVZTCi1IwI5Z3eRdc7Bb6mncbkVmbkO6kYEYtDraBlXiUN/nKVWeAAOl2LrkUyqV/LHz6Tjh98zuKF6KCF+ea+L39POEGgyUK2SH2nZVqKCLUSFWFi19yQulyLrnIPezauRY3OS+mcuEcFmcu0u9qXn/T9WCTJx6qwDo15jy5FM6kUG4WfUk3oqF71Oo3KAifhqIaRnW9l2NIs2tcLIOudgfcopbmsSReuala/qZ7AQFZGmlFJl0fCgQYN47733CpT/+OOPdOrUCYDDhw8zdOhQVq5cSUBAAAMHDmTatGkYDMXP9bOzswkJCSErK4vg4ODSCl8IIYQQZUg+v8W1psyS6qslKyuL0NBQjhw5Iv+UQgghRAWRnZ1NTEwMmZmZhISE+DocIUqszC7/uFrOnDkDIFPrCSGEEBXQmTNnJKkW14QKP1Ltdrs5fvw4QUFBaJpWqm3nn0XLKHjZkn6+OqSfrw7p56tD+vnqKMt+Vkpx5swZoqOj0el8dtsMIUpNhR+p1ul0VK9evUz3ERwcLG/aV4H089Uh/Xx1SD9fHdLPV0dZ9bOMUItriZwaCiGEEEIIUUKSVAshhBBCCFFCklRfhNlsZsKECeVi3uxrmfTz1SH9fHVIP18d0s9Xh/SzEMVX4X+oKIQQQgghhK/JSLUQQgghhBAlJEm1EEIIIYQQJSRJtRBCCCGEECUkSbUQQgghhBAlJEl1EebMmUNcXBwWi4WEhATWr1/v65DKjalTp9KqVSuCgoKIiIigd+/e7Nmzx6uO1Wpl2LBhhIWFERgYSJ8+fUhPT/eqk5qaSlJSEv7+/kRERDB69GicTqdXnZUrV9KiRQvMZjN16tRh4cKFBeK5Xp6radOmoWkaI0eO9JRJP5eOY8eOcc899xAWFoafnx/x8fFs3LjRs14pxfjx46latSp+fn506dKFffv2ebVx6tQpkpOTCQ4OJjQ0lAcffJCcnByvOtu2baNDhw5YLBZiYmKYPn16gVg+/fRTGjRogMViIT4+nqVLl5bNQV9lLpeLcePGUbNmTfz8/KhduzaTJ0/mwt/KSz9fmZ9++okePXoQHR2NpmksWbLEa3156tfixCJEhaVEAYsXL1Ymk0m9++67aufOnWrw4MEqNDRUpaen+zq0ciExMVEtWLBA7dixQ23ZskXdfvvtKjY2VuXk5HjqPPLIIyomJkatWLFCbdy4UbVp00a1a9fOs97pdKomTZqoLl26qM2bN6ulS5eqKlWqqLFjx3rqHDx4UPn7+6snnnhC7dq1S7322mtKr9erZcuWeepcL8/V+vXrVVxcnGratKkaMWKEp1z6ueROnTqlatSooQYNGqTWrVunDh48qL799lu1f/9+T51p06apkJAQtWTJErV161bVs2dPVbNmTXXu3DlPndtuu03dcMMN6tdff1WrV69WderUUf379/esz8rKUpGRkSo5OVnt2LFDffTRR8rPz0+98cYbnjpr1qxRer1eTZ8+Xe3atUs9++yzymg0qu3bt1+dzihDU6ZMUWFhYeqrr75SKSkp6tNPP1WBgYHqlVde8dSRfr4yS5cuVc8884z6/PPPFaC++OILr/XlqV+LE4sQFZUk1YVo3bq1GjZsmGfZ5XKp6OhoNXXqVB9GVX5lZGQoQK1atUoppVRmZqYyGo3q008/9dTZvXu3AtTatWuVUnkfAjqdTqWlpXnqzJs3TwUHByubzaaUUuqpp55SjRs39trXXXfdpRITEz3L18NzdebMGVW3bl21fPlydfPNN3uSaunn0vGvf/1L3XTTTUWud7vdKioqSs2YMcNTlpmZqcxms/roo4+UUkrt2rVLAWrDhg2eOt98843SNE0dO3ZMKaXU3LlzVaVKlTz9nr/v+vXre5b79eunkpKSvPafkJCgHn744ZIdZDmQlJSkHnjgAa+yO++8UyUnJyulpJ9Ly9+T6vLUr8WJRYiKTC7/+Bu73c6mTZvo0qWLp0yn09GlSxfWrl3rw8jKr6ysLAAqV64MwKZNm3A4HF592KBBA2JjYz19uHbtWuLj44mMjPTUSUxMJDs7m507d3rqXNhGfp38Nq6X52rYsGEkJSUV6Avp59Lx5Zdf0rJlS/r27UtERATNmzfnrbfe8qxPSUkhLS3N6/hDQkJISEjw6ufQ0FBatmzpqdOlSxd0Oh3r1q3z1OnYsSMmk8lTJzExkT179nD69GlPnYs9FxVZu3btWLFiBXv37gVg69at/Pzzz3Tv3h2Qfi4r5alfixOLEBWZwdcBlJTb7eb48eMEBQWhaVqJ2ztx4gQul4vAwECys7M95aGhoezcudOrTOT1/7Bhw0hISCA2Npbs7GwOHjyI0WhEp9N59VeVKlU4fPgw2dnZpKamEhYW5rXez88PgIMHD1K7dm2OHTvGLbfc4lUnKCiI7Oxs0tPTyczMvOafq//85z9s2LCBlStXkp2djdPpxGazST+XogMHDjB37lyGDx/OZ599xm+//cY///lPXC4XAwYM4MCBAwD4+/t7HWvlypU5cuQI2dnZHDp0qEA/Q14fpaSkkJ2dzdGjR6lRo4ZXnYCAAAD2799P/fr1OXHihKfv8wUHB3P8+PEK38+PPvooJ0+epH79+uj1elwuF+PHj6dHjx5kZ2dLP5ei3Nxcz3GUp379eyxKKc6cOUN4eDhpaWll0xlCXEUV/o6KR48eJSYmxtdhCCGEEOIKJCUlERAQwMcff+zrUIQokQo/Uh0UFATAkSNHCA4O9nE0QgghhCiO7OxsYmJiOHXqFLVr1/Z1OEKUWIVPqvMv+QgODpakWoir7IzVwfe70+nSMJIgi9HX4QghKqBNmzbx2GOP+ToMIUqswifVZcXudFPv2W98HYYQF9U4Opidxyv+NaBXS8saldh4+LSvwxCiQqkW6seaMbeWWftVq1ald+/eZda+EFeLzP5RhFGfbvV1CEJckiTUl0cSaiEu37HMc6xPOVVm7X/++edYLJYya1+Iq0WS6iLUjwrydQhCCCFEuTBr+d4ya7tOnTpl1rYQV5Nc/lGEUpidTwghhLgmXMufiW63G7vd7uswRDllNBrR6/XFqitJdRHO2V2+DkEIIYQQZchut5OSkoLb7fZ1KKIcCw0NJSoq6pL3QynTpPqnn35ixowZbNq0iRMnTvDFF194/RhBKcWECRN46623yMzMpH379sybN4+6deuWZVjF8r+tx30dghBCCCHKiFKKEydOoNfriYmJQaeTK2KFN6UUubm5ZGRkAHk/qr2YMk2qz549yw033MADDzzAnXfeWWD99OnTefXVV3nvvfeoWbMm48aNIzExkV27dvn8RwsOV4W+J44QQghRanTX4PUfTqeT3NxcoqOj8ff393U4opzKvwtxRkYGERERF70UpEyT6u7du9O9e/dC1ymlmD17Ns8++yy9evUC4P333ycyMpIlS5Zw9913l2Vol+Su2DeaFEIIIcRFuFx5l3maTCYfRyLKu/yTLofDcdGk2mffdaSkpJCWlkaXLl08ZSEhISQkJLB27doit7PZbGRnZ3s9ysKJLGuZtCuEEEJUNNfgQLXHpa6TFaK4rxGfJdVpaWkAREZGepVHRkZ61hVm6tSphISEeB4xMTFlGqcQQgghhBCXUuGuyh87dixZWVmex5EjR3wdkhBCCCHENSsuLo7Zs2f7Ooxyz2dJdVRUFADp6ele5enp6Z51hTGbzQQHB3s9hBBCCCGuBydPnmTo0KHExsZiNpuJiooiMTGRNWvWFKirlKJ79+5omsaSJUsu2m6nTp3QNK3Aw+l0smHDBoYMGXLFMR86dAhN09iyZUuRdSZOnFjo/i98lHc+S6pr1qxJVFQUK1as8JRlZ2ezbt062rZt66uwhBBCCPE3mbkOX4cg/tKnTx82b97Me++9x969e/nyyy/p1KkTf/75Z4G6s2fPvqxkdPDgwZw4ccLrYTAYCA8Pv+gMKQ5HyV8fo0aN8tpv9erVmTRpklfZ5fDFDX3KNKnOyclhy5YtnjOTlJQUtmzZQmpqKpqmMXLkSJ5//nm+/PJLtm/fzn333Ud0dLTXXNZCCCGE8K20bPnxfnmQmZnJ6tWrefHFF7nllluoUaMGrVu3ZuzYsfTs2dOr7pYtW5g5cybvvvtusdv39/cnKirK6wEFL//QNI158+bRs2dPAgICmDJlCqdPnyY5OZnw8HD8/PyoW7cuCxYsAPIGUgGaN2+Opml06tSpwL4DAwO99qvX6wkKCvIsOxwO+vXrR2hoKJUrV6ZXr14cOnTIs/2gQYPo3bs3U6ZMITo6mvr163tGyD/55BM6dOiAn58frVq1Yu/evWzYsIGWLVsSGBhI9+7dOXnyZLH7qShlOqXexo0bueWWWzzLTzzxBAADBw5k4cKFPPXUU5w9e5YhQ4aQmZnJTTfdxLJly3w+R7UQQgghzss6d+2PVCulOOfwzd2U/Yz6Yo0oBwYGEhgYyJIlS2jTpg1ms7nQerm5uQwYMIA5c+Zc9JLakpg4cSLTpk1j9uzZGAwGxo0bx65du/jmm2+oUqUK+/fv59y5cwCsX7+e1q1b8/3339O4cePLnsbQ4XCQmJhI27ZtWb16NQaDgeeff57bbruNbdu2edpbsWIFwcHBLF++3Gv7CRMmMHv2bGJjY3nggQcYMGAAQUFBvPLKK/j7+9OvXz/Gjx/PvHnzStQnZZpUd+rUCXWR+Z41TWPSpElMmjSpLMMQQgghRAnYndf+bbzPOVw0Gv+tT/a9a1Ii/qZLp2QGg4GFCxcyePBg5s+fT4sWLbj55pu5++67adq0qafe448/Trt27Tz3ASmuuXPn8vbbb3uWH374YWbOnFlo3QEDBnD//fd7llNTU2nevDktW7YE8ka384WHhwMQFhZ2RUn+xx9/jNvt5u233/acfCxYsIDQ0FBWrlxJt27dAAgICODtt9/2JNn5I9mjRo0iMTERgBEjRtC/f39WrFhB+/btAXjwwQdZuHDhZcf1d2WaVAshhBBCiNLTp08fkpKSWL16Nb/++ivffPMN06dP5+2332bQoEF8+eWX/PDDD2zevPmy205OTuaZZ57xLIeGhhZZNz95zjd06FD69OnDb7/9Rrdu3ejduzft2rW77BgKs3XrVvbv309QUJBXudVq5cCBA57l+Pj4QkfBLzzhyJ/KOT4+3qss/1bkJSFJtRBCCCGue35GPbsmJfps35fDYrHQtWtXunbtyrhx43jooYeYMGECgwYN4ocffuDAgQMFEuI+ffrQoUMHVq5cWWS7ISEh1KlTp1gxBAQEeC13796dw4cPs3TpUpYvX07nzp0ZNmwYL7300mUdW2FycnK48cYb+eCDDwqsyx8FLyymfEaj0fN3/kj338vc7pJ/GyNJtRBCCCGue5qmFesSjPKoUaNGninzxowZw0MPPeS1Pj4+nlmzZtGjR48yjSM8PJyBAwcycOBAOnTowOjRo3nppZc8o8f5t4a/XC1atODjjz8mIiKiXE+lXDFfPUIIIYQQ15k///yTvn378sADD9C0aVOCgoLYuHEj06dP91w/feGsHReKjY31zMJRFsaPH8+NN95I48aNsdlsfPXVVzRs2BCAiIgI/Pz8WLZsGdWrV8disRASElLstpOTk5kxYwa9evVi0qRJVK9encOHD/P555/z1FNPUb169bI6rMtS4e6oKIQQQghxPQoMDCQhIYFZs2bRsWNHmjRpwrhx4xg8eDCvv/66T2MzmUyMHTuWpk2b0rFjR/R6PYsXLwbyfmD56quv8sYbbxAdHX3ZP6D09/fnp59+IjY2ljvvvJOGDRvy4IMPYrVay9XItaYuNj1HBZCdnU1ISAhZWVml2rFxY74utbaEEEKIiu7QtKRSba+sPr+Ly2q1kpKSQs2aNWUqX3FRxX2tyEi1EEIIIYQQJSRJtRBCCCGEECUkSbUQQgghhBAlJEm1EEKISwrgHB102zDg9HUoQghRLklSXUyhnGGs4QPqaUcuWTeMLG7X/Yrxij98Svbb0eraSQI4V6I2rgYTDsLIKqXWLt1nOtzMMs5hkH5ZKe3zyvyffhVvGWfih9WncVxNVcjiTt1PmLFfsq4eV7Hq5avGSUw4ShJemWuiHaQaJ6nGSQLJ9XU4V+Rd0wz+bZrGCMPnvg7FJ4LIJYLTvg6jUN1163jGsAiNa/9W4kKUZ5JUF9Pzxnd52PA135n/dcm6S0zjmWt6lWGGJYWu1+OinW4HEwzv0Ua3y2vd3fof2GR+hMbaoSuKM047wc/mEWwwP3pF21+pSE5RXctLGu7Vf8cs4xz0nJ/kPYo/OWQZwMP6/wFgwMley0A2WYZSjZOFtnmDtp+uuo2X3Hd33TrWm4fRUvu9wLpAcvnG9C+eMHxCV90m7tCvYaLx/Ss4wuKc6BTvZOgl4xt01W/igQuS++rayTJLsmO1dNrrtl/mVopOus1EcuqitZpr+wgn07Oc9/eF/aAYa/iAjZahvGyaz1OGjy+551Xmx9lqHowZO0HkcrNuK1MNbxHM2QJ1b9D2s8Yygv+ani3eYV1EYy2Fh/RfY8FW4rbOU8Rq6XxlfpY1lhGssYxgk3koYWTRWbcJ3QVJUDA5JTgRP6++lkpf/UpitXRu1f12WbFeTIIu7/9rgH7FlQf3N0acVCnGiXUt7TixWnqB8oH6b5lmeBMNN8P0S5hgeM9rfWWymWmcR6tC3huaa/t4UP91sRPR7ZaHWG8ZRmWy/7bG9xNozTO9wmDDUhKL8X5ZfIrG2qFSeU0Kcb2Qm78UQwDnaKnb+7dShQEXzgu6MJBcdLiJ0eUliY/o/8ds5/8VaO8pw2IeNuRN2Xc/3xJn/dCzbprxbQC+Nj9NT9tktqnaBbY34KSbbiOHVRSD9Mv4t6srDXSpJOh+J/ivUTB/rfiJQQg5tNHt5kbdXuK0NCY57+OoCvcckx0jLXT7+M1dFw2FDSOgYcaODRNGnKyzDC/Q7k+upnzh7kCsls5P5scBGGv8iA3u+nxunuipd7N+Gx+6OgN5o9d1tWPsVDX4r3k8AOvd9XnG8SAudOhxs0+dn+Rdj4t5plcAeN/0Io1sC7xiuFv/Iw11R2ioO8KT9kc85f5Yedbwb5a62xDMWQYZvuVXd0OqaX8w2vEI7gvON1tru3nd9BrjHPfTS78Gf2wMcjwFaJ46Zux8ZXqGg6oq37pa8rW7DTZMnvWVyUaH4g/OT3YfouUliXW1oyw3PwXAJnddJjnuZauqQxC5hGpnOKIigbwExHHB662udpQ++p+Y5+xJFoGFPreAp+972SaxVZ2//Wwd7SiZKohYLZ1mugOYcHBUhdNK9zt7VCwvGN8BuOD1qYjXUtirqmPDRIK2m4/NkwEY7RjCDOObAPzX1Y4RjrzXQ4L2u+e1DtBNt5HJ3FtEpIpgcqmu/QFAY+2Q1+vErDl43nEPpwjmBm0/mQRyp341AA11R7zaCcCKCx02jCh0zDTOo49+NQfdUWxRdXja8SBWTOQ/h4m6DbxhmgVApHaaKc57vCJrrB3ChIPNqm6BqC3YvNrKp8PNf03PEq875FVu1hxssgwF4FnH/SxydSWMLDZZhnJCVaatreBcs+10O8hSAexUNRlp+A8aMMvZhwf1S9mtavCLuwlddRs5oKL51jzGa9t77GP52R1fWId7zDHOJkm/nubW+TgwsNb8T4K0c9xim0mKqupVN0w747Ws4SaCTNKp7Dnuhtphdqsanv+jv792831tGks93TE622ZwQFUrNDZ/rPxgHgXAt66WzHD2Y7zh38xz9eQ5Y14S/bW7DaONnwCwyNXF09ZE43v01K+lj3611/sswBfmCQCEa9nY0fMf182k/vW/djG369ex0V2f31UsXXSbeMk4n5GOYax0Nytym+H6L6inO8oIxzA08Hp/MWPHjINszt9eOYws2up28a27FfHaQY6rMNIII4LTZBAKaPhh5RxmLnzdVdEufYISQg463Jzm4lPYDdJ/y0Tj+3zvas5DjtGXbFcIIUn1JVUhi41/fQDmG2f4Nw8avgHgXvsYVrubAoodFu/bglo0B0P0/8OFjpMqlJ0qDhsmryQD8kZ+UlQUa92Nvcq/NI8jzvohGm5qa8c5qsLxx8Zow8f0N/zoqdfX8FOhsb9hfJnRjofJxcyHpinsdMdxQlXmHv33xP6V+De1vsVWyxCv7brpN1HfupAbtAN88lfSBJCmKhGlnWajux4fuzoxw/gm37ha0V2/odD919cdYbFhMm10u73KL0yUAF4wvsNBVZVG2mHGG/8NQIYK9axvrdvjSToB+tufYa27MctNo6mrO+Yp99ds/FP/OV+722DHyFEV7vVNQGf9+VG7XZYHABjA+X5srdsDQLqqzBp3Y9a443nS8An//Osbh/ykC+CQPpmW1nn8SRDJ+hXEahnU1R2jLsdI1G/kX2oxEx0DeciwlB3uOAYalgPQw/a8pw3trxGuN40zPWU36vZ5TiY8/eW6ie9cLZlvmo1NGTBrTqY6+jPW+BEAdbRjhX7otdD28oDh/Gj4f83j+dLVlmcd9xOineX7C/r0YhYYXyRQO8dmd12G/PXaHW7/J6+bXvPUyU+oAXrpf2GhM5E6umNe5QBBWi5ddRvppf+FkyqE2c4+1NaO83/6VURpp7lVv8VTN0bL8Nr2Tv3P3Kn/mcftQ5llmlcgznBOs8EyzKtsq7sWH7o60+ev5LuWLo1apHGn/mcAkmwv0FO/hvv0yz3bDDYs5U1nEiepRIyWzuq/TkoAmlnfIJNA8hIZxZOGTz2vjwbWBVgxY8CJEwOJug0FEuq/u1+/jJ/cTZltnANAVe0Uc4yzednZFz9sTDC+zzxnT941vQTACPujjPzr8osUdxTjjB8AMNL+KLNNcwvdxyLTVH52NeYDVxfmmV7hsDuC913dSNRv4L+u9nzkupUk/XoANlse8dp2tnEOyfanec/0old5W91Oamjp9NWv4kbdvvPHYx/NTbodPGj4hgXORJ5zDqSNbheLTc8zzXE38109/6qpCOIc9f76/71Nt4E5rmhAo6F2mInG99jqrs2rzjv4l2Gxp/1E/UYS9XmjsR315799ibjg2xL/C75pqKMd9/zdSDvEQ4aldNb9xp325zzljxjyvj17zLAEh9Iz1vkQS1ztCSKXaO0UC03T2Ow+fzL1vDHvxL25dT5vm/L+dxeaphNn/ZBKZHOaIPITXRMOQjjLKOOnAPTUr83rV+edZKsAGutS6PPXaxHyTnzDtSxPu8dUGNW0PwH4wdXM8/8x3/kPHjF8xVeuBIY7Rni2b6fbyTJXa8K1TOpqR1FotNTtYY6zN+FaFr+rGM/7/QpXcx51jMCGiRgtnVpaGqvcTT2x53+j10W/mXJ+dZUQ5Ybc/KUIcWO+pptuA29ekEgV5TPXTWx112aS8b1L1r2YfzkG86LxLa+y95xdPQnZlXrZ8X88YfxPidoobxY5O3OPofS+hi5MW+trrLX8s8j1p1Ug05z9CzxnvuJSGnpN8a7zNu7W/3hZ31aIgsY5BjHZuLBA+SmV961AZS3nsre93rzq7M0g/XcEaxe/jnyPuzr1dUc55I4kTlfwMo+KZqLjviu8zOzyPe140POt0pXY4q5FM91Bz/JRVYVTKoimuhRP2cP2x9nk156N47qWKNa/k5u/iIqiuK8VSaqL8NjTz/Cqybe3/BRCCCHKg1mBT/D4qAml2qYk1b4XFxfHyJEjGTlypK9DKdfkjoolJAm1EEIIkefxnJd9HYK4wKBBg9A0jWnTpnmVL1myBE3TitiqoA0bNjBkyJBLVywFa9aswWAw0KxZs6uyP1+QpFoIIYQQooKxWCy8+OKLnD595VM9hoeH4+/vX4pRFS4zM5P77ruPzp07l/m+fEmSaiGEEEKICqZLly5ERUUxderUIut89tlnNG7cGLPZTFxcHDNnzvRaHxcXx+zZswFQSjFx4kRiY2Mxm81ER0fz2GOPeerabDZGjRpFtWrVCAgIICEhgZUrVxYr1kceeYQBAwbQtm3byz7OiqRcJNVz5swhLi4Oi8VCQkIC69ev93VIQgghhLieKAX2s755XMHP2/R6PS+88AKvvfYaR48eLbB+06ZN9OvXj7vvvpvt27czceJExo0bx8KFCwtt77PPPmPWrFm88cYb7Nu3jyVLlhAff346zuHDh7N27VoWL17Mtm3b6Nu3L7fddhv79u0rtL18CxYs4ODBg0yYULrX5JdHPp9S7+OPP+aJJ55g/vz5JCQkMHv2bBITE9mzZw8RERG+Dk8IIYQQ1wNHLrwQ7Zt9P30cTAGXrvc3d9xxB82aNWPChAm88473LDAvv/wynTt3Zty4cQDUq1ePXbt2MWPGDAYNGlSgrdTUVKKioujSpQtGo5HY2Fhat27tWbdgwQJSU1OJjs7ro1GjRrFs2TIWLFjACy+8UGh8+/btY8yYMaxevRqDwecpZ5nz+Uj1yy+/zODBg7n//vtp1KgR8+fPx9/fn3fffdfXoQkhhBBClGsvvvgi7733Hrt3e98TYvfu3bRv396rrH379uzbtw+Xy8Xf9e3bl3PnzlGrVi0GDx7MF198gdOZd0fN7du343K5qFevHoGBgZ7HqlWrOHDgAIBX+SOPPILL5WLAgAE899xz1KtXr4yOvnzx6WmD3W5n06ZNjB071lOm0+no0qULa9euLXQbm82GzXZ+/t3s7L/fMlYIIYQQ4jIZ/fNGjH217yvUsWNHEhMTGTt2bKEj0MUVExPDnj17+P7771m+fDmPPvooM2bMYNWqVeTk5KDX69m0aRN6vd5ru8DAvLn7t2zZ4ikLDg7mzJkzbNy4kc2bNzN8eN5ddt1uN0opDAYD3333HbfeeusVx1se+TSp/uOPP3C5XERGet8aNjIykt9//73QbaZOncpzzz1X6DohhBBCiCuiaVd0CUZ5MG3aNJo1a0b9+vU9ZQ0bNmTNmjVe9dasWUO9evUKJMb5/Pz86NGjBz169GDYsGE0aNCA7du307x5c1wuFxkZGXTo0KHQbevUqeO17Ha72b59u1fZ3Llz+eGHH/jPf/5DzZo1r+RQy7UKd4HL2LFjeeKJJzzL2dnZxMTE+DAiIYQQQgjfiY+PJzk5mVdffdVT9uSTT9KqVSsmT57MXXfdxdq1a3n99deZO3duoW0sXLgQl8tFQkIC/v7+LFq0CD8/P2rUqEFYWBjJycncd999zJw5k+bNm3Py5ElWrFhB06ZNSUpKKtCeTqejSZMmXmURERFYLJYC5dcKn15TXaVKFfR6Penp3relTU9PJyoqqtBtzGYzwcHBXg8hhBBCiOvZpEmTcLvdnuUWLVrwySefsHjxYpo0acL48eOZNGlSkZeIhIaG8tZbb9G+fXuaNm3K999/z//+9z/CwsKAvFk87rvvPp588knq169P79692bBhA7GxsVfj8CoEn9+mPCEhgdatW/Paa68BeV8XxMbGMnz4cMaMGXPJ7cvsNqcTQ0qvLSGEEKKim5hVqs3JbcpFRVHc14rPL/944oknGDhwIC1btqR169bMnj2bs2fPcv/99/s6NCGEEEIIIYrF50n1XXfdxcmTJxk/fjxpaWk0a9aMZcuWFfjxohBCCCGEEOWVz5NqyLtLT/50K0IIIYQQQlQ0Pr/5S3m13+2juyqVA1+52vg6BCGEEOXI+0GDfR2CEOWeJNVF0PD+/eZCZ7eL1neqS3flMRV2RbEcVVWKXNfVNv2K2ryYWc4+XstTHAPY565W6vspLw64qxYoa2Gdz+22wm+7ejFdbdO52fYyja3vXLryFfjF1ahY9W62vUxN6yJaWecQZ/2QjrZZZRJPUf7ranfV9jXZkVysev91teOIOxyAk+r8D5H/YXu+2PvKVWY62WbS1za+0PW/uBrxvCOZTBVQ7NhS3AUvdXvWcfm/KZnh6FfkupccfZnkuJcPnbfyhrPg1FdXIltd2c0qspQ/N9teZpj9MU/ZXGdPdrlreNWLt75Nb9ukAtvHWT8otF/3uKsTZ/2QaY67C93vLEefQstL4pQK9FquY32/1PdxKe87u15W/TRV6bLqv+7sxRxnT74OLP3+Ky98PF+DqACK+xqRpLqYJjvv9fw9zXE3i5ydaWc9Px/kLOf/FdjmG1crz98Nre/S1Taj0LbXuBrT2jqHofYRnrJf3Q0BWO5qQY+LfOinqghaWefykqNvkXXutj9baHlRCflRFe61/JbrH3S1n499vvMfdLDNoqn1Lc4qs1fddBXKAmciLzv+j3nOHtiUgXvtY2hhne+pY1MGBtr/5Vm+MMHJl638iLN+6FVW2/pvRjuGcE6ZOKlCPCcyj9hHepIY8D4BylZ+hR7jhda5G3p9EL/g6M8pgtml4rjJ9grdbVMZYH/aa5ss5c+N1nk8Zh9Oa+sctrvjOKqqkKKiOKyiOMv5/Y60P8rtthcYah/BclcLT/mhQpIpgI3ugrdz/djZicmOexjgeIY464fEWT9kpP1RFjq7scddncfsw2hqfYuW1nl0s73IYRWFQsdJ8j5AU1Ukraxzedd5m6fNe+1j2O2OpbdtEvWtC71iu9BNtleK7LsPnXl3w/rM1YF61veIs35AnPVDRjjOX841xTGARNs0z/Jbzts9fy92duIrVxuvsuJ4zdkbgLGOB3nHlVTgZGOE/VF62yYxyD6asY4Hme64ixGO4XSwv0Kc9UNa2eZ5+nGHqlWg/dGOIXznutEraapvXUgj2wIOqarsVdW96g+3/5M46wcMcDzL264kmtneIs76Ie+4ziewT9gf4RXnHcx39vhr33l9dYvd+4THofQscnXluKoMwH9cHdnrrubp63wNre/yurMX/e15r4k5rt7EWT/gLts4r3o3WufxuusO3nV152nnQ0x1eiekv/71+r/w/ac4brHN5FnH/Qy3/9Or/C3n7dS1vs9XrgS2u+O40TrPs26nuwZtbK9zWEXxtbsNvW2T6GybwXTn3dxun8poxxBP3TP4s0XV4bA7wlNmVUZAY4O7QYF47rXn3Zl3vqunp6y//RkG2Z+ijvV9/udu6yk/q8xMcAz0LH/s7FTkcQ6xP85ox5ACJy2dbTN44YK+nOIYgBMDf6i8WSwuPEmIs37It66WnuWH7E/S0TbL6z0uXYV6/u5im15ogr7hr/eGNFWJJX+duC50JXrVWe3ynv83/3WWb6D9X5xSgbiVxg3WNz3vJRf6xtWKetb3aGt9jZecdzHDeTfXYtqZfwMUu93u40hEeZebmwuA0Wi8aD2fT6lXUmU1Jc+B8Q2orTsBwHRHP+a6evOa8VUaa4fobp+GDRMATbSD1NOOssp9A5ssQ73aiLN+SG3tGE70HFZR+GFlt+UBIC9RsSoTdgxk4w9oAPTR/UQuZr5xJ1BbO8YhFYWL83c+OmQZAOQlokPsT7BZ1fWs0+FmlOET6mpH6ar/DYAk2wvsVHG00+3gTxVMJS0Ht9LYq6qTSZCnvUH2p5hoeI83Xf/gQ1dnT/lZZaaxbQEAn5kmcKNuH91sL7JX5d1w5zH95zxh/A9rXY0Y5xzEflXNcyx5lGc5nEyGGr7kQ9et7FfVPfuY5ribnvq1GHBST3cMyBvxcWJgmelfNNAd8fRn0RSjDR+z3V2LZe7WdNNt4HcVS6qK9Oq3wjzruJ8PXJ15wzgLOwaGOx772zGAHhf/MT1Hc91+ANpZX+U4579B0HCjAe4LzlPz9/mYfThfus+P3OpwE0YWJ6lUIK6PnLcw0TmQPZZBXuUXP/bLoRhjWMxOdw3+5y44mtxUO8Ak40I+d93EJON7nn3fo1/O88YFXnXfct7Oi867aaodZKuq7fU6hfPHP8UxgLdc/yCSU9ygO8By943U0k4Qwll+U3lJgoabvvpVbHHXIUOFEqWdZpl5DG87u3OTbgcNdEd4yjGYZP0K5jp78a27Ff5YySV/aiNFFKf41ZKX4I20P8oS903F7hU/rNyp/5m97ursUdXJJi+Z1uHmH7pf+U3V4aiK8NrmacMH3K3/gST7CxxRRf+wupX2O610e5jn6oEqYhyjrnaU5eangLz/7Va2eYXWO2hORqflvWVf7DXxiek5Wuv2sMtdg9vtUwus98dKgm43q93xOC/4aU11LYMsFYg/Voyak9MqiMpaNqvNjwOwyV2XJa72ZKkAvnS392yX/1wfU2G0t712wZ7y/v/jtBN01G1jsetW7BT9oaThppfuFzarOhxWefcqMOGgle539rhjySIABwaaagf40nz+5OEj5y2MdZ6/PCE/ntttL7BLxQFgwMl+y30A3GmbyG+qLk20FPap6tgwMcMwn76Gn+hkm8lK85NA3klA0l/9F6Ole/oBoLl1PjoUmyxD+d0dw232FwGI4DTtdTv42R3PBsujnFTBtLLNx4zd83/dxPo2OeSP9CtMOLFj5FH9EgI1K9Odd3vWVdf+4ISqjBkHdgx0021kvbshfxCMBTtWzF7vI11s00lRVfmn4QvWuhqzTuUN0Nyt/4FITvOKqw9GnGgor+cimj/4k2Ba6Pbxm7uu5zMuX+u4ynzySFtKk6+n1FNKkZqaisPhIDo6Gp1OxhmFN6UUubm5ZGRkEBoaStWqBb/ZvpAk1UWY8+y9DDN8yVFVhZts50ekNdxFfjA20g7RWfcbTxr/AxT80LNg43dL3te6N9lmF/iQLo6Ouq2EkuP1gVaYFtpewrRslrtbXrReom4D4Vomi1zeXyG20n7nGeMixjvuZ5uqDeQlliGc5RTn+1mHm1baHraqWljxHrW+lAuTrrddt6PQeMrwMWeVhTmu3gC01e3kI9MU5jh7MsNZ+Ne6xfGG8WUS9Rt51nE/Fux87WrDaOPH/KFCeNF5d4GEsCgGnARyjkyCin18HW2zPMn93yXpfmWU4WOGOx7j0AUj3A21w3xjHuupV3pJdfHdqfuJ41ThV3cjDDiZbFjAz+545pjy/h/utY9htbtpkdvnH39/+zOsdTe+7P0bceLAgA43lTnDH1x67vj8fT5hf4TP3R0ve5++1Fa3kzGGj3ja8SA7VeG3762unWSI/ivedCVd9P2jClncpf+RT103k8Hlfd1fmPx+XeNqTLLjmQLrnzZ8wBDD1zxqf4yl7rL/TUYd7Sjf/3USUtv67yJP6FpZ53i+rYG8/183Oq+T38IsN42mru4YUx39ecPVA8h77//K9AyNdYe5xz6Wn93xAASSyznMhb6HBHMWKyZP8nqr7jf0uC/5vny58o/3Q+etPO18qFTbzte6ZmU+efjaSqohb5Q6JSXF66YpQvxdaGgoUVFRaJp20XqSVBeh3pgldNNt5Bd3Y68k8lLu0K1mlilvlKlgIqRYZhqDP1Y62Wdd8o39Wpf/QXC3/Vl+dRd9rbAF22Un7H+n4SacrFJJMIqrCllU0s6w72+XChRXe9123jdOY5zzAT50dS7l6K7cFvNgQrWzNLW+6RnRLUxt7Ri1teN8525VZJ3S9o3pXzTUHeFG6zz+LEYSLopng3ko4VoWkxz38q6re6F1Qsgh6yKvh9JkwMkG86OcUX50tM/m798stdPtIADrFSevweTQQreP1e6mXsly3qCKVmB/vpb/Xvqw/XG+LaP/t2s1qYa8m87JJSCiKEaj0XOp0KVIUl2EuDFfX9F2/lj5wfwkG931/7qMwJuGGx2q2COj17Ja2nFqa8dLfdTmWqLHVe5eK2bsmLFfNKH2FT0uLNi9rmkXJRdGFi10+1jhblFuBgOMOHGjlbv/D1/IT6oH258os/fTa/HyDyFKW7mYp/pakouFdrbXivzgUehwXeWYyquDKpqD6vqdurA4ymPCYMNU4HrL8sKFXhLqMvAnIeXu5NchH18FqHI2gi7E9aZ8DDlcY8rLSI4QQojrh1uSaiF8SrI/IYQQ4hogI9VC+JYk1UIIIUQFtt0dh13pWV/I/N2lRV2TM1ULUbrkojQhhBCiAutpfx4TznL7WwchrhcyUi2EEEJUYAqdJNRClAOSVAshhBBCCFFCklQLIYQQQghRQpJUCyGEEEIIUUKSVAshhBDioir2vZeFuDokqRZCCCGEEKKEJKkWQgghhBCihMosqZ4yZQrt2rXD39+f0NDQQuukpqaSlJSEv78/ERERjB49GqfTWVYhCSGEEEIIUSbK7OYvdrudvn370rZtW955550C610uF0lJSURFRfHLL79w4sQJ7rvvPoxGIy+88EJZhSWEEEKIyySXVAtxaWU2Uv3cc8/x+OOPEx8fX+j67777jl27drFo0SKaNWtG9+7dmTx5MnPmzMFut5dVWEIIIYQQQpQ6n11TvXbtWuLj44mMjPSUJSYmkp2dzc6dO4vczmazkZ2d7fUQQgghRNlRMv2HEJfks6Q6LS3NK6EGPMtpaWlFbjd16lRCQkI8j5iYmDKNUwghhLje/Zaa6esQhCj3LiupHjNmDJqmXfTx+++/l1WsAIwdO5asrCzP48iRI2W6PyGEEEIIIS7lsn6o+OSTTzJo0KCL1qlVq1ax2oqKimL9+vVeZenp6Z51RTGbzZjN5mLtQwghhBBCiKvhspLq8PBwwsPDS2XHbdu2ZcqUKWRkZBAREQHA8uXLCQ4OplGjRqWyDyGEEEIIIa6GMrumOjU1lS1btpCamorL5WLLli1s2bKFnJwcALp160ajRo2499572bp1K99++y3PPvssw4YNKxcj0TfEhPo6BCGEEEIIUUFoqox+0jto0CDee++9AuU//vgjnTp1AuDw4cMMHTqUlStXEhAQwMCBA5k2bRoGQ/EH0LOzswkJCSErK4vg4ODSCp/MXDvNJi0vtfaEEEKIimrP87dhNuhLtc2y+vwWwlfKLKm+WuSfUgghhKh45PNbXGvK7I6KV0v+OYHMVy2EEEJUHPmf2xV8bE8IjwqfVJ85cwZA5qsWQgghKqAzZ84QEhLi6zCEKLEKf/mH2+3m+PHjBAUFoWlaqbadnZ1NTEwMR44cka+mypD089Uh/Xx1SD9fHdLPV0dZ9rNSijNnzhAdHY1O57N70QlRair8SLVOp6N69epluo/g4GB5074KpJ+vDunnq0P6+eqQfr46yqqfZYRaXEvk1FAIIYQQQogSkqRaCCGEEEKIEpKk+iLMZjMTJkwoFzejuZZJP18d0s9Xh/Tz1SH9fHVIPwtRfBX+h4pCCCGEEEL4moxUCyGEEEIIUUKSVAshhBBCCFFCklQLIYQQQghRQpJUCyGEEEIIUUKSVAshhBBCCFFCklQXYc6cOcTFxWGxWEhISGD9+vW+DqncmDp1Kq1atSIoKIiIiAh69+7Nnj17vOpYrVaGDRtGWFgYgYGB9OnTh/T0dK86qampJCUl4e/vT0REBKNHj8bpdHrVWblyJS1atMBsNlOnTh0WLlxYIJ7r5bmaNm0amqYxcuRIT5n0c+k4duwY99xzD2FhYfj5+REfH8/GjRs965VSjB8/nqpVq+Ln50eXLl3Yt2+fVxunTp0iOTmZ4OBgQkNDefDBB8nJyfGqs23bNjp06IDFYiEmJobp06cXiOXTTz+lQYMGWCwW4uPjWbp0adkc9FXmcrkYN24cNWvWxM/Pj9q1azN58mQunIBK+vnK/PTTT/To0YPo6Gg0TWPJkiVe68tTvxYnFiEqLCUKWLx4sTKZTOrdd99VO3fuVIMHD1ahoaEqPT3d16GVC4mJiWrBggVqx44dasuWLer2229XsbGxKicnx1PnkUceUTExMWrFihVq48aNqk2bNqpdu3ae9U6nUzVp0kR16dJFbd68WS1dulRVqVJFjR071lPn4MGDyt/fXz3xxBNq165d6rXXXlN6vV4tW7bMU+d6ea7Wr1+v4uLiVNOmTdWIESM85dLPJXfq1ClVo0YNNWjQILVu3Tp18OBB9e2336r9+/d76kybNk2FhISoJUuWqK1bt6qePXuqmjVrqnPnznnq3HbbbeqGG25Qv/76q1q9erWqU6eO6t+/v2d9VlaWioyMVMnJyWrHjh3qo48+Un5+fuqNN97w1FmzZo3S6/Vq+vTpateuXerZZ59VRqNRbd++/ep0RhmaMmWKCgsLU1999ZVKSUlRn376qQoMDFSvvPKKp47085VZunSpeuaZZ9Tnn3+uAPXFF194rS9P/VqcWISoqCSpLkTr1q3VsGHDPMsul0tFR0erqVOn+jCq8isjI0MBatWqVUoppTIzM5XRaFSffvqpp87u3bsVoNauXauUyvsQ0Ol0Ki0tzVNn3rx5Kjg4WNlsNqWUUk899ZRq3Lix177uuusulZiY6Fm+Hp6rM2fOqLp166rly5erm2++2ZNUSz+Xjn/961/qpptuKnK92+1WUVFRasaMGZ6yzMxMZTab1UcffaSUUmrXrl0KUBs2bPDU+eabb5SmaerYsWNKKaXmzp2rKlWq5On3/H3Xr1/fs9yvXz+VlJTktf+EhAT18MMPl+wgy4GkpCT1wAMPeJXdeeedKjk5WSkl/Vxa/p5Ul6d+LU4sQlRkcvnH39jtdjZt2kSXLl08ZTqdji5durB27VofRlZ+ZWVlAVC5cmUANm3ahMPh8OrDBg0aEBsb6+nDtWvXEh8fT2RkpKdOYmIi2dnZ7Ny501Pnwjby6+S3cb08V8OGDSMpKalAX0g/l44vv/ySli1b0rdvXyIiImjevDlvvfWWZ31KSgppaWlexx8SEkJCQoJXP4eGhtKyZUtPnS5duqDT6Vi3bp2nTseOHTGZTJ46iYmJ7Nmzh9OnT3vqXOy5qMjatWvHihUr2Lt3LwBbt27l559/pnv37oD0c1kpT/1anFiEqMgkqf6bP/74A5fL5ZWEAERGRpKWluajqMovt9vNyJEjad++PU2aNAEgLS0Nk8lEaGioV90L+zAtLa3QPs5fd7E62dnZnDt37rp4rhYvXsxvv/3G1KlTC6yTfi4dBw8eZN68edStW5dvv/2WoUOH8thjj/Hee+8B5/vpYseflpZGRESE13qDwUDlypVL5bm4Fvp5zJgx3H333TRo0ACj0Ujz5s0ZOXIkycnJgPRzWSlP/VqcWISoyAy+DkBUbMOGDWPHjh38/PPPvg7lmnPkyBFGjBjB8uXLsVgsvg7nmuV2u2nZsiUvvPACAM2bN2fHjh3Mnz+fgQMH+ji6a8cnn3zCBx98wIcffkjjxo3ZsmULI0eOJDo6WvpZCHFNkJHqv6lSpQp6vb7ADArp6elERUX5KKryafjw4Xz11Vf8+OOPVK9e3VMeFRWF3W4nMzPTq/6FfRgVFVVoH+evu1id4OBg/Pz8rvnnatOmTWRkZNCiRQsMBgMGg4FVq1bx6quvYjAYiIyMlH4uBVWrVqVRo0ZeZQ0bNiQ1NRU4308XO/6oqCgyMjK81judTk6dOlUqz8W10M+jR4/2jFbHx8dz77338vjjj3u+hZF+LhvlqV+LE4sQFZkk1X9jMpm48cYbWbFihafM7XazYsUK2rZt68PIyg+lFMOHD+eLL77ghx9+oGbNml7rb7zxRoxGo1cf7tmzh9TUVE8ftm3blu3bt3u9kS9fvpzg4GBPgtO2bVuvNvLr5LdxrT9XnTt3Zvv27WzZssXzaNmyJcnJyZ6/pZ9Lrn379gWmhNy7dy81atQAoGbNmkRFRXkdf3Z2NuvWrfPq58zMTDZt2uSp88MPP+B2u0lISPDU+emnn3A4HJ46y5cvp379+lSqVMlT52LPRUWWm5uLTuf9kaPX63G73YD0c1kpT/1anFiEqNB8/UvJ8mjx4sXKbDarhQsXql27dqkhQ4ao0NBQrxkUrmdDhw5VISEhauXKlerEiROeR25urqfOI488omJjY9UPP/ygNm7cqNq2bavatm3rWZ8/1Vu3bt3Uli1b1LJly1R4eHihU72NHj1a7d69W82ZM6fQqd6up+fqwtk/lJJ+Lg3r169XBoNBTZkyRe3bt0998MEHyt/fXy1atMhTZ9q0aSo0NFT997//Vdu2bVO9evUqdEqy5s2bq3Xr1qmff/5Z1a1b12tKsszMTBUZGanuvfdetWPHDrV48WLl7+9fYEoyg8GgXnrpJbV79241YcKECj3V24UGDhyoqlWr5plS7/PPP1dVqlRRTz31lKeO9POVOXPmjNq8ebPavHmzAtTLL7+sNm/erA4fPqyUKl/9WpxYhKioJKkuwmuvvaZiY2OVyWRSrVu3Vr/++quvQyo3gEIfCxYs8NQ5d+6cevTRR1WlSpWUv7+/uuOOO9SJEye82jl06JDq3r278vPzU1WqVFFPPvmkcjgcXnV+/PFH1axZM2UymVStWrW89pHvenqu/p5USz+Xjv/973+qSZMmymw2qwYNGqg333zTa73b7Vbjxo1TkZGRymw2q86dO6s9e/Z41fnzzz9V//79VWBgoAoODlb333+/OnPmjFedrVu3qptuukmZzWZVrVo1NW3atAKxfPLJJ6pevXrKZDKpxo0bq6+//rr0D9gHsrOz1YgRI1RsbKyyWCyqVq1a6plnnvGaok36+cr8+OOPhb4nDxw4UClVvvq1OLEIUVFpSl1wOyshhBBCCCHEZZNrqoUQQgghhCghSaqFEEIIIYQoIUmqhRBCCCGEKCFJqoUQQgghhCghSaqFEEIIIYQoIUmqhRBCCCGEKCFJqoUQQgghhCghSaqFEEIIIYQoIUmqhRBCCCGEKCFJqoUQQgghhCghSaqFEEIIIYQoof8HIYH3e9l7kZgAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.subplot(4, 1, 1)\n",
        "plt.plot(source1_1st_term, label='S1 First Term')\n",
        "plt.plot(source1_noise_term, label='Noise-1')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
        "\n",
        "plt.subplot(4, 1, 2)\n",
        "plt.plot(source2_1st_term, label='S2 First Term')\n",
        "plt.plot(noise2, label='Noise-2')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
        "\n",
        "plt.subplot(4, 1, 3)\n",
        "plt.plot(source3_1st_term, label='S3 First Term')\n",
        "plt.plot(noise3, label='Noise-3')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
        "\n",
        "plt.subplot(4, 1, 4)\n",
        "plt.plot(source4_1st_term, label='S4 First Term')\n",
        "plt.plot(noise4, label='Noise-4')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cdt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8S9iW6BIFQsw",
        "outputId": "e0c9bd48-0d74-490a-c608-b704e66f2ad0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cdt\n",
            "  Downloading cdt-0.6.0-py3-none-any.whl (921 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m921.1/921.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from cdt) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from cdt) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from cdt) (1.2.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from cdt) (1.4.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from cdt) (2.0.3)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (from cdt) (0.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from cdt) (3.3)\n",
            "Collecting skrebate (from cdt)\n",
            "  Downloading skrebate-0.62.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from cdt) (4.66.2)\n",
            "Collecting GPUtil (from cdt)\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from cdt) (2.31.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->cdt) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->cdt) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->cdt) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->cdt) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->cdt) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->cdt) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->cdt) (2024.2.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->cdt) (3.4.0)\n",
            "Requirement already satisfied: patsy>=0.5.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels->cdt) (0.5.6)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels->cdt) (24.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.4->statsmodels->cdt) (1.16.0)\n",
            "Building wheels for collected packages: GPUtil, skrebate\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7394 sha256=6fddca251a90e86ead112b409de0d6b4faa793d66709cb50fba77fbc1608717d\n",
            "  Stored in directory: /root/.cache/pip/wheels/a9/8a/bd/81082387151853ab8b6b3ef33426e98f5cbfebc3c397a9d4d0\n",
            "  Building wheel for skrebate (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for skrebate: filename=skrebate-0.62-py3-none-any.whl size=29255 sha256=43884900f45bccb3c72446b916b33d8ff9293f837cdd49ebf1a91bff0a8a0a09\n",
            "  Stored in directory: /root/.cache/pip/wheels/dd/67/40/683074a684607162bd0e34dcf7ccdfcab5861c3b2a83286f3a\n",
            "Successfully built GPUtil skrebate\n",
            "Installing collected packages: GPUtil, skrebate, cdt\n",
            "Successfully installed GPUtil-1.4.0 cdt-0.6.0 skrebate-0.62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cdt"
      ],
      "metadata": {
        "id": "9zOqtLmEFTV5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "467b35fa-8f27-4c1b-93b0-3631595c4745"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No GPU automatically detected. Setting SETTINGS.GPU to 0, and SETTINGS.NJOBS to cpu_count.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Pre-processing"
      ],
      "metadata": {
        "id": "_c4DRyupElS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def data_preprocessing(data, max_lag=5):\n",
        "  from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "  #convert to numpy array\n",
        "  syn_data_np = data.to_numpy()\n",
        "\n",
        "  #normalize the dataset\n",
        "  scaler_X = MinMaxScaler(feature_range=(0, 1))\n",
        "  syn_data_np_nor = scaler_X.fit_transform(syn_data_np)\n",
        "  syn_data_np = syn_data_np_nor\n",
        "\n",
        "  #transform into 2D data\n",
        "  syn_data_np_T= syn_data_np.T\n",
        "  syn_data_pro = np.zeros((syn_data_np.shape[0]-max_lag,syn_data_np.shape[1],(max_lag+1)))\n",
        "  for i in range(0, (syn_data_np.shape[0]-max_lag)):\n",
        "    syn_data_pro[i,:,:]= syn_data_np_T[:, i:i+(max_lag+1)]\n",
        "  syn_data_2d = np.expand_dims(syn_data_pro, axis =-1)\n",
        "\n",
        "  #make datafrom with normalized data\n",
        "  syn_data_nor_df =pd.DataFrame(data = syn_data_np,\n",
        "                  columns = data.columns)\n",
        "\n",
        "  #transform normalized data into 1D shape with lagged and current time values\n",
        "  size_1d = syn_data_np.shape[1]*(max_lag+1)\n",
        "  print(size_1d)\n",
        "  syn_data_1d = np.zeros((syn_data_np.shape[0]-max_lag,size_1d))\n",
        "  for i in range(0, (syn_data_np.shape[0]-max_lag)):\n",
        "    for j in range(0,(max_lag+1)):\n",
        "      j_end = j * syn_data_np.shape[1]\n",
        "      syn_data_1d[i,j_end:j_end+syn_data_np.shape[1]]= syn_data_np[i+j, :]\n",
        "\n",
        "  #transform non-normalized data into 1D shape with lagged and current time values\n",
        "  syn_data_np_2 = data.to_numpy()\n",
        "  syn_data_1d_not_norm = np.zeros((syn_data_np_2.shape[0]-max_lag,size_1d))\n",
        "  for i in range(0, (syn_data_np_2.shape[0]-max_lag)):\n",
        "    for j in range(0,(max_lag+1)):\n",
        "      j_end = j * syn_data_np_2.shape[1]\n",
        "      syn_data_1d_not_norm[i,j_end:j_end+syn_data_np_2.shape[1]]= syn_data_np_2[i+j, :]\n",
        "\n",
        "\n",
        "  return syn_data_np_nor, syn_data_2d, syn_data_nor_df, syn_data_1d,  syn_data_1d_not_norm"
      ],
      "metadata": {
        "id": "4DVdqlr5lFs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "syn_data_np, syn_data_2d, syn_data_nor_df, syn_data_1d,  syn_data_1d_not_norm = data_preprocessing(data, max_lag=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJH74-HmjFE5",
        "outputId": "5d6202d4-1a56-441c-c0a4-13e165b93c94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "syn_data_2d.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-sM8O9yjX_k",
        "outputId": "e9837a3f-b622-4922-de29-ccd298cd72f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(99998, 4, 3, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "syn_data_np_nor.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrkQ-tqyjbgM",
        "outputId": "a976bd30-afe8-4564-d4be-9be67838d030"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100000, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "syn_data_nor_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7G3LI5yjeDz",
        "outputId": "c1612608-6423-4f53-eafb-1f0c19af0092"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100000, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "syn_data_1d.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHgkpfgljzhB",
        "outputId": "788a07eb-7c61-4cc2-bbb9-04737d427390"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(99998, 12)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "syn_data_1d_not_norm.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bP6kicJpj13B",
        "outputId": "506f9cc4-e2a8-4521-d7c3-e35233b4bfe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(99998, 12)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CvHxTQArupm",
        "outputId": "a9dbb837-9530-4ac5-f752-8b181e6fc298"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((100000, 4), (99995, 4, 6, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ],
      "source": [
        "syn_data_np.shape, syn_data_2d.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "510yT1YNJHlt",
        "outputId": "f219f039-520c-4242-8a77-81cf3caa6971"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(99995, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ],
      "source": [
        "data_y_syn = syn_data_np[5:,0:4]\n",
        "data_y_syn.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KVllbLUkHjt"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyklufMFkH7O"
      },
      "outputs": [],
      "source": [
        "import keras,os\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten, Dropout, AveragePooling2D, LSTM, Activation, ConvLSTM2D, TimeDistributed, Input, Reshape\n",
        "from keras.layers import UpSampling1D, Conv2DTranspose, UpSampling2D, Conv1D, AveragePooling1D, LeakyReLU\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras import callbacks\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "from keras.layers import concatenate\n",
        "from keras.regularizers import l1, l2\n",
        "from time import time\n",
        "\n",
        "keras.utils.set_random_seed(1001)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_probability as tfp"
      ],
      "metadata": {
        "id": "QPNiH7qHKe0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalConv2D(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_outputs, counter, *args, **kwargs):\n",
        "        super(CausalConv2D, self).__init__()\n",
        "        self.conv2d = tf.keras.layers.Conv2D(*args, **kwargs)\n",
        "        self.num_outputs = num_outputs\n",
        "        self.counter = counter\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W=self.add_weight(name='kernel',\n",
        "                           shape=(input_shape[1], input_shape[2],input_shape[3], 1),\n",
        "                           #initializer = keras.initializers.RandomUniform(minval=0.05, maxval=0.5),\n",
        "                           #initializer ='uniform',\n",
        "                           initializer = tf.keras.initializers.glorot_uniform(seed=8),\n",
        "                           trainable=True)\n",
        "        self.mask = np.ones(shape=self.W.shape)\n",
        "        print(self.W)\n",
        "        self.mask[self.counter,(input_shape[2]-1),...] = 0.0\n",
        "\n",
        "    #def masked_convolution_op(self, filters, kernel, mask):\n",
        "    #    return self._convolution_op(filters, tf.math.multiply(kernel, tf.reshape(mask, mask.shape + [1,1] )))\n",
        "\n",
        "    def get_weights(self):\n",
        "        return super().get_weights()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        self.W.assign(tf.math.multiply(self.W, self.mask))\n",
        "        #self.conv2d._convolution_op = functools.partial(self.masked_convolution_op, mask=mask)\n",
        "        #return self.conv2d.call(x)\n",
        "        return self.conv2d.convolution_op(inputs, self.W)"
      ],
      "metadata": {
        "id": "z2o2-R2mFIm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2D\n",
        "def get_model_2d(input_dims):\n",
        "    input_batch = Input(shape = input_dims)\n",
        "\n",
        "    conv_model = Sequential()\n",
        "    conv_model = Conv2D(filters=4, kernel_size=1, strides=(1,1), padding='valid', activation=\"linear\", name='conv1')(input_batch)\n",
        "    #conv_model = AveragePooling2D(pool_size=(1,1), strides=None, padding='valid', name='pool1')(conv_model) activation=LeakyReLU(0.05)\n",
        "    conv_model = tf.math.reduce_mean(conv_model, axis=-1)\n",
        "    conv_model = Reshape((4, 6, 1))(conv_model)\n",
        "    #conv_model = Flatten()(conv_model )\n",
        "    pooled_outputs = []\n",
        "    for i in range(0, 4):\n",
        "      #layer = CausalConv2D(num_outputs=1, counter=i, name=\"parr\"+str(i))(conv_model) # , kernel_regularizer = l1(0.2)\n",
        "      layer = CausalConv2D(filters=1, kernel_size=(4,6), num_outputs=1, counter=i, padding='valid', activation=\"tanh\",)(conv_model)\n",
        "      #den1 = layer(tf.ones([481, 30]))\n",
        "      #conv = Conv2D(1, kernel_size=filter_sizes[i], padding='valid', activation='relu')(conv_model)\n",
        "      pooled_outputs.append(layer)\n",
        "    output = concatenate(pooled_outputs)\n",
        "    output = Flatten()(output)\n",
        "\n",
        "    model = Model(inputs=input_batch, outputs=output, name='cpred')\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "WwughzzGFJIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with gradient tape\n",
        "\n",
        "class CausalNNModel(object):\n",
        "    def __init__(self,\n",
        "                 dims,\n",
        "                 alpha=0.0,\n",
        "                 rho = 1.0,\n",
        "                 rho_max = 10e20,\n",
        "                 h_tol = 1e-8,\n",
        "                 init='glorot_uniform'):\n",
        "\n",
        "        super(CausalNNModel, self).__init__()\n",
        "\n",
        "        self.dims = dims\n",
        "        self.n_stacks = len(self.dims) - 1\n",
        "        self.alpha = alpha\n",
        "        self.rho = rho\n",
        "        self.h_p = np.Inf\n",
        "        self.rho_max = rho_max\n",
        "        self.h_tol = h_tol\n",
        "        self.model_2d = get_model_2d(self.dims)\n",
        "        print(\"====Model created=====\")\n",
        "\n",
        "        self.model = Model(inputs=self.model_2d.input, outputs=self.model_2d.output)\n",
        "\n",
        "\n",
        "    def extract_features(self, x):\n",
        "        return self.model_cnn.predict(x)\n",
        "\n",
        "    def predict(self, x):  # predict cluster labels using the output of clustering layer\n",
        "        q = self.model.predict(x, verbose=0)[1]\n",
        "        return q.argmax(1)\n",
        "\n",
        "    def custom_loss_function(self, y_true, y_pred):\n",
        "      mse = keras.losses.mean_squared_error(y_true, y_pred)\n",
        "      h_val = self.causal_loss_h()\n",
        "      h_loss = 0.5 * self.rho * h_val * h_val + self.alpha * h_val\n",
        "      lambda1 = 0.1\n",
        "      adj_mat = self.get_mat()\n",
        "      sparse_loss = lambda1 * tf.math.reduce_sum(tf.abs(adj_mat))\n",
        "      #neg_weight = np.sum(adj_mat, where=adj_mat<0)\n",
        "      #neg_loss = 0.5 * tf.abs(neg_weight)\n",
        "      print('MSE Loss is: {}, h Loss is: {}, L1 loss: {}, Total Loss is: {}'.format(tf.reduce_mean(mse), h_loss, sparse_loss, tf.reduce_mean(mse)+h_loss))\n",
        "      return mse + h_loss + sparse_loss #+ neg_loss\n",
        "\n",
        "    def causal_loss_h(self):\n",
        "      mat = self.get_mat()\n",
        "      h_val = self.h_acy_1(mat[:, 20:])\n",
        "      return h_val\n",
        "\n",
        "    def get_mat(self):\n",
        "      w1_2d_s = self.model.get_layer(index=-6).get_weights()\n",
        "      w2_2d_s = self.model.get_layer(index=-5).get_weights()\n",
        "      w3_2d_s = self.model.get_layer(index=-4).get_weights()\n",
        "      w4_2d_s = self.model.get_layer(index=-3).get_weights()\n",
        "      arr1_2d_s = np.expand_dims(np.squeeze(np.array(w1_2d_s), axis=(0,3,4)).flatten('F'), axis=0)\n",
        "      arr2_2d_s = np.expand_dims(np.squeeze(np.array(w2_2d_s), axis=(0,3,4)).flatten('F'), axis=0)\n",
        "      arr3_2d_s = np.expand_dims(np.squeeze(np.array(w3_2d_s), axis=(0,3,4)).flatten('F'), axis=0)\n",
        "      arr4_2d_s = np.expand_dims(np.squeeze(np.array(w4_2d_s), axis=(0,3,4)).flatten('F'), axis=0)\n",
        "      mat_2d_s = np.concatenate((arr1_2d_s, arr2_2d_s, arr3_2d_s, arr4_2d_s))\n",
        "      #print(mat_2d_s)\n",
        "      return mat_2d_s\n",
        "\n",
        "    def h_acy_1(self, A):\n",
        "      n_var = A.shape[0]\n",
        "      h = tf.linalg.trace(tf.linalg.expm(A * A)) - n_var\n",
        "      return h\n",
        "\n",
        "\n",
        "    def h_acy(self, A):\n",
        "      '''Calculate the constraint of A ensure that it's a DAG'''\n",
        "      #(Yu et al. 2019 DAG-GNN)\n",
        "      # h(w) = tr[(I + kA*A)^n_variables] - n_variables\n",
        "      n_var = A.shape[0]\n",
        "      M = tf.eye(n_var, num_columns = n_var) + A/n_var\n",
        "      E = M\n",
        "      for _ in range(n_var - 2):\n",
        "        E = tf.linalg.matmul(E, M)\n",
        "      h = tf.math.reduce_sum(tf.transpose(E) * M) - n_var\n",
        "      return h\n",
        "\n",
        "    def compile(self, optimizer='adam'):\n",
        "        self.model.compile(optimizer=optimizer, loss= self.custom_loss_function) # ['mse', self.causal_loss()])\n",
        "\n",
        "    def fit(self, x, y=None, maxiter=100, batch_size=512, save_dir='./results/temp'):\n",
        "        t1 = time()\n",
        "\n",
        "\n",
        "        # Step 2: deep clustering\n",
        "        # logging file\n",
        "        import csv\n",
        "        logfile = open(save_dir + '/causalnn_log.csv', 'w')\n",
        "        logwriter = csv.DictWriter(logfile, fieldnames=['iter','loss'])\n",
        "        logwriter.writeheader()\n",
        "        train_loader = tf.data.Dataset.from_tensor_slices((x, y)).batch(batch_size)\n",
        "        optimizer = tf.keras.optimizers.Adam(1e-2)\n",
        "        w1_2d_s = self.model.get_layer(index=-6).get_weights()\n",
        "        arr1_2d_s = np.expand_dims(np.squeeze(np.array(w1_2d_s), axis=(0,3,4)).flatten('F'), axis=0)\n",
        "\n",
        "\n",
        "        for epoch in range(int(maxiter)):\n",
        "          print('Epoch: {}', epoch)\n",
        "          h_n = None\n",
        "          for (x, y) in train_loader:\n",
        "            #eval loss and compute gradients\n",
        "            with tf.GradientTape() as tape:\n",
        "              tape.watch(self.model.trainable_variables)\n",
        "              #passing through neural network\n",
        "              output = self.model(x)\n",
        "              #calculate loss\n",
        "              loss = self.custom_loss_function(y, output)\n",
        "              gradients = tape.gradient(loss, self.model.trainable_variables)\n",
        "              optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
        "              h_n = self.causal_loss_h()\n",
        "              #print('New h_val is :', h_n)\n",
        "              #if h_n > 0.25 * self.h_p:\n",
        "              #  self.rho = self.rho*10\n",
        "              #else:\n",
        "              #  break\n",
        "\n",
        "          if h_n > 0.25 * self.h_p:\n",
        "                self.rho = self.rho*1.1\n",
        "          self.h_p = h_n\n",
        "          print('New h_val is :', h_n)\n",
        "          self.alpha += self.rho * self.h_p\n",
        "\n",
        "          if self.h_p <= self.h_tol or self.rho >= self.rho_max:\n",
        "            print('Before the loop end # h_val is: {}, rho is: {}'.format(self.h_p, self.rho))\n",
        "            break\n",
        "\n",
        "        #for ite in range(int(maxiter)):\n",
        "        #  print('Epoch: {}', ite)\n",
        "        #  self.model.fit(x, y, epochs=1, batch_size=batch_size, verbose=True)\n",
        "\n",
        "        # save the trained model\n",
        "        logfile.close()\n",
        "        file_name  = \"/CausalNN_model_final_\" + str(round(time()))+ \".h5\"\n",
        "        print('saving model to:', save_dir + file_name)\n",
        "        self.model.save_weights(save_dir + file_name)\n",
        "\n",
        "        w1_2d_s_1 = self.model.get_layer(index=-6).get_weights()\n",
        "        arr1_2d_s_1 = np.expand_dims(np.squeeze(np.array(w1_2d_s_1), axis=(0,3,4)).flatten('F'), axis=0)\n",
        "\n",
        "        y_pred = self.model.predict(x)\n",
        "        adj_mat = self.get_mat()\n",
        "\n",
        "        print('The conv layer 1 weights before training :', arr1_2d_s)\n",
        "        print('The conv layer 1 weights after training :', arr1_2d_s_1)\n",
        "\n",
        "        return y_pred, adj_mat"
      ],
      "metadata": {
        "id": "HbY_kEfhJ2HV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model creation and training:"
      ],
      "metadata": {
        "id": "PM6hvqYoVHG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.set_random_seed(1001)\n",
        "\n",
        "cnnmodel = CausalNNModel(dims=syn_data_2d.shape[1:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rxv1FjaM_u6",
        "outputId": "e141c55a-d17f-4f1b-c9b3-b0729c9e4856"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tf.Variable 'causal_conv2d/kernel:0' shape=(4, 6, 1, 1) dtype=float32, numpy=\n",
            "array([[[[ 0.01729116]],\n",
            "\n",
            "        [[ 0.05369946]],\n",
            "\n",
            "        [[-0.11558445]],\n",
            "\n",
            "        [[-0.1930318 ]],\n",
            "\n",
            "        [[-0.22727013]],\n",
            "\n",
            "        [[-0.10506889]]],\n",
            "\n",
            "\n",
            "       [[[-0.03241783]],\n",
            "\n",
            "        [[ 0.34967968]],\n",
            "\n",
            "        [[-0.22822307]],\n",
            "\n",
            "        [[ 0.18624505]],\n",
            "\n",
            "        [[-0.03919902]],\n",
            "\n",
            "        [[-0.03960952]]],\n",
            "\n",
            "\n",
            "       [[[ 0.20739552]],\n",
            "\n",
            "        [[-0.14644605]],\n",
            "\n",
            "        [[ 0.21469983]],\n",
            "\n",
            "        [[-0.17902574]],\n",
            "\n",
            "        [[ 0.06509387]],\n",
            "\n",
            "        [[ 0.17076865]]],\n",
            "\n",
            "\n",
            "       [[[ 0.21393666]],\n",
            "\n",
            "        [[-0.3412853 ]],\n",
            "\n",
            "        [[ 0.0880048 ]],\n",
            "\n",
            "        [[-0.3153036 ]],\n",
            "\n",
            "        [[-0.07369781]],\n",
            "\n",
            "        [[ 0.15640828]]]], dtype=float32)>\n",
            "<tf.Variable 'causal_conv2d_1/kernel:0' shape=(4, 6, 1, 1) dtype=float32, numpy=\n",
            "array([[[[ 0.01729116]],\n",
            "\n",
            "        [[ 0.05369946]],\n",
            "\n",
            "        [[-0.11558445]],\n",
            "\n",
            "        [[-0.1930318 ]],\n",
            "\n",
            "        [[-0.22727013]],\n",
            "\n",
            "        [[-0.10506889]]],\n",
            "\n",
            "\n",
            "       [[[-0.03241783]],\n",
            "\n",
            "        [[ 0.34967968]],\n",
            "\n",
            "        [[-0.22822307]],\n",
            "\n",
            "        [[ 0.18624505]],\n",
            "\n",
            "        [[-0.03919902]],\n",
            "\n",
            "        [[-0.03960952]]],\n",
            "\n",
            "\n",
            "       [[[ 0.20739552]],\n",
            "\n",
            "        [[-0.14644605]],\n",
            "\n",
            "        [[ 0.21469983]],\n",
            "\n",
            "        [[-0.17902574]],\n",
            "\n",
            "        [[ 0.06509387]],\n",
            "\n",
            "        [[ 0.17076865]]],\n",
            "\n",
            "\n",
            "       [[[ 0.21393666]],\n",
            "\n",
            "        [[-0.3412853 ]],\n",
            "\n",
            "        [[ 0.0880048 ]],\n",
            "\n",
            "        [[-0.3153036 ]],\n",
            "\n",
            "        [[-0.07369781]],\n",
            "\n",
            "        [[ 0.15640828]]]], dtype=float32)>\n",
            "<tf.Variable 'causal_conv2d_2/kernel:0' shape=(4, 6, 1, 1) dtype=float32, numpy=\n",
            "array([[[[ 0.01729116]],\n",
            "\n",
            "        [[ 0.05369946]],\n",
            "\n",
            "        [[-0.11558445]],\n",
            "\n",
            "        [[-0.1930318 ]],\n",
            "\n",
            "        [[-0.22727013]],\n",
            "\n",
            "        [[-0.10506889]]],\n",
            "\n",
            "\n",
            "       [[[-0.03241783]],\n",
            "\n",
            "        [[ 0.34967968]],\n",
            "\n",
            "        [[-0.22822307]],\n",
            "\n",
            "        [[ 0.18624505]],\n",
            "\n",
            "        [[-0.03919902]],\n",
            "\n",
            "        [[-0.03960952]]],\n",
            "\n",
            "\n",
            "       [[[ 0.20739552]],\n",
            "\n",
            "        [[-0.14644605]],\n",
            "\n",
            "        [[ 0.21469983]],\n",
            "\n",
            "        [[-0.17902574]],\n",
            "\n",
            "        [[ 0.06509387]],\n",
            "\n",
            "        [[ 0.17076865]]],\n",
            "\n",
            "\n",
            "       [[[ 0.21393666]],\n",
            "\n",
            "        [[-0.3412853 ]],\n",
            "\n",
            "        [[ 0.0880048 ]],\n",
            "\n",
            "        [[-0.3153036 ]],\n",
            "\n",
            "        [[-0.07369781]],\n",
            "\n",
            "        [[ 0.15640828]]]], dtype=float32)>\n",
            "<tf.Variable 'causal_conv2d_3/kernel:0' shape=(4, 6, 1, 1) dtype=float32, numpy=\n",
            "array([[[[ 0.01729116]],\n",
            "\n",
            "        [[ 0.05369946]],\n",
            "\n",
            "        [[-0.11558445]],\n",
            "\n",
            "        [[-0.1930318 ]],\n",
            "\n",
            "        [[-0.22727013]],\n",
            "\n",
            "        [[-0.10506889]]],\n",
            "\n",
            "\n",
            "       [[[-0.03241783]],\n",
            "\n",
            "        [[ 0.34967968]],\n",
            "\n",
            "        [[-0.22822307]],\n",
            "\n",
            "        [[ 0.18624505]],\n",
            "\n",
            "        [[-0.03919902]],\n",
            "\n",
            "        [[-0.03960952]]],\n",
            "\n",
            "\n",
            "       [[[ 0.20739552]],\n",
            "\n",
            "        [[-0.14644605]],\n",
            "\n",
            "        [[ 0.21469983]],\n",
            "\n",
            "        [[-0.17902574]],\n",
            "\n",
            "        [[ 0.06509387]],\n",
            "\n",
            "        [[ 0.17076865]]],\n",
            "\n",
            "\n",
            "       [[[ 0.21393666]],\n",
            "\n",
            "        [[-0.3412853 ]],\n",
            "\n",
            "        [[ 0.0880048 ]],\n",
            "\n",
            "        [[-0.3153036 ]],\n",
            "\n",
            "        [[-0.07369781]],\n",
            "\n",
            "        [[ 0.15640828]]]], dtype=float32)>\n",
            "====Model created=====\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnnmodel.model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tqt4qRYdNCdx",
        "outputId": "9f4f1243-51e0-40b3-d150-1a6da69ab6b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 4, 6, 1)]            0         []                            \n",
            "                                                                                                  \n",
            " conv1 (Conv2D)              (None, 4, 6, 4)              8         ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean (TFOpL  (None, 4, 6)                 0         ['conv1[0][0]']               \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " reshape (Reshape)           (None, 4, 6, 1)              0         ['tf.math.reduce_mean[0][0]'] \n",
            "                                                                                                  \n",
            " causal_conv2d (CausalConv2  (None, 1, 1, 1)              24        ['reshape[0][0]']             \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " causal_conv2d_1 (CausalCon  (None, 1, 1, 1)              24        ['reshape[0][0]']             \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " causal_conv2d_2 (CausalCon  (None, 1, 1, 1)              24        ['reshape[0][0]']             \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " causal_conv2d_3 (CausalCon  (None, 1, 1, 1)              24        ['reshape[0][0]']             \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 1, 1, 4)              0         ['causal_conv2d[0][0]',       \n",
            "                                                                     'causal_conv2d_1[0][0]',     \n",
            "                                                                     'causal_conv2d_2[0][0]',     \n",
            "                                                                     'causal_conv2d_3[0][0]']     \n",
            "                                                                                                  \n",
            " flatten (Flatten)           (None, 4)                    0         ['concatenate[0][0]']         \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 104 (416.00 Byte)\n",
            "Trainable params: 104 (416.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnnmodel.compile()"
      ],
      "metadata": {
        "id": "cH2nDfu0NFUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred, mat = cnnmodel.fit(x=syn_data_2d, y=data_y_syn, maxiter=20, batch_size=2048, save_dir='/content/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQOuTYC6y9yR",
        "outputId": "fe06f39e-48ff-436e-84f8-6b2a63890d01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: {} 0\n",
            "MSE Loss is: 0.3329330384731293, h Loss is: 1.003495640361507e-06, L1 loss: 1.4565690755844116, Total Loss is: 0.33293405175209045\n",
            "MSE Loss is: 0.32225242257118225, h Loss is: 1.1561634210011107e-06, L1 loss: 1.4485690593719482, Total Loss is: 0.32225358486175537\n",
            "MSE Loss is: 0.30539023876190186, h Loss is: 1.372049609926762e-06, L1 loss: 1.4406925439834595, Total Loss is: 0.3053916096687317\n",
            "MSE Loss is: 0.3028430938720703, h Loss is: 1.6720179019102943e-06, L1 loss: 1.4330716133117676, Total Loss is: 0.302844762802124\n",
            "MSE Loss is: 0.2951931655406952, h Loss is: 2.0884040168311913e-06, L1 loss: 1.4307571649551392, Total Loss is: 0.29519525170326233\n",
            "MSE Loss is: 0.2803803086280823, h Loss is: 2.670891262823716e-06, L1 loss: 1.4430142641067505, Total Loss is: 0.28038299083709717\n",
            "MSE Loss is: 0.26060664653778076, h Loss is: 3.500576895021368e-06, L1 loss: 1.4561830759048462, Total Loss is: 0.2606101334095001\n",
            "MSE Loss is: 0.237020343542099, h Loss is: 4.6974473661975935e-06, L1 loss: 1.4695138931274414, Total Loss is: 0.23702503740787506\n",
            "MSE Loss is: 0.2099171131849289, h Loss is: 6.432456302718492e-06, L1 loss: 1.4846194982528687, Total Loss is: 0.20992355048656464\n",
            "MSE Loss is: 0.17917214334011078, h Loss is: 8.956632882473059e-06, L1 loss: 1.506151556968689, Total Loss is: 0.17918109893798828\n",
            "MSE Loss is: 0.1448347419500351, h Loss is: 1.262965270143468e-05, L1 loss: 1.5279995203018188, Total Loss is: 0.14484737813472748\n",
            "MSE Loss is: 0.11232661455869675, h Loss is: 1.7934507923200727e-05, L1 loss: 1.5506314039230347, Total Loss is: 0.11234454810619354\n",
            "MSE Loss is: 0.08275044709444046, h Loss is: 2.5538627596688457e-05, L1 loss: 1.5793589353561401, Total Loss is: 0.08277598768472672\n",
            "MSE Loss is: 0.059317342936992645, h Loss is: 3.624754026532173e-05, L1 loss: 1.615552544593811, Total Loss is: 0.05935359001159668\n",
            "MSE Loss is: 0.04439707100391388, h Loss is: 5.091721686767414e-05, L1 loss: 1.6509002447128296, Total Loss is: 0.04444798827171326\n",
            "MSE Loss is: 0.04093463718891144, h Loss is: 7.006299711065367e-05, L1 loss: 1.6844195127487183, Total Loss is: 0.04100469872355461\n",
            "MSE Loss is: 0.04721132293343544, h Loss is: 9.297177894040942e-05, L1 loss: 1.7194645404815674, Total Loss is: 0.047304295003414154\n",
            "MSE Loss is: 0.06282941997051239, h Loss is: 0.00011712215928127989, L1 loss: 1.7475948333740234, Total Loss is: 0.06294654309749603\n",
            "MSE Loss is: 0.07535946369171143, h Loss is: 0.0001382654590997845, L1 loss: 1.7660356760025024, Total Loss is: 0.07549773156642914\n",
            "MSE Loss is: 0.08273175358772278, h Loss is: 0.0001526817213743925, L1 loss: 1.7745685577392578, Total Loss is: 0.08288443833589554\n",
            "MSE Loss is: 0.08070331811904907, h Loss is: 0.00015881657600402832, L1 loss: 1.7752046585083008, Total Loss is: 0.0808621346950531\n",
            "MSE Loss is: 0.0735318586230278, h Loss is: 0.0001572736509842798, L1 loss: 1.7684599161148071, Total Loss is: 0.07368913292884827\n",
            "MSE Loss is: 0.06131637096405029, h Loss is: 0.00015010128845460713, L1 loss: 1.7562084197998047, Total Loss is: 0.06146647408604622\n",
            "MSE Loss is: 0.05109301209449768, h Loss is: 0.00013971238513477147, L1 loss: 1.7402046918869019, Total Loss is: 0.051232725381851196\n",
            "MSE Loss is: 0.042891886085271835, h Loss is: 0.00012824859004467726, L1 loss: 1.7230247259140015, Total Loss is: 0.043020132929086685\n",
            "MSE Loss is: 0.039058610796928406, h Loss is: 0.00011725356307579204, L1 loss: 1.706209421157837, Total Loss is: 0.039175864309072495\n",
            "MSE Loss is: 0.037274740636348724, h Loss is: 0.00010767987259896472, L1 loss: 1.6899446249008179, Total Loss is: 0.037382420152425766\n",
            "MSE Loss is: 0.036907896399497986, h Loss is: 0.00010005929652834311, L1 loss: 1.6755037307739258, Total Loss is: 0.037007953971624374\n",
            "MSE Loss is: 0.04110759496688843, h Loss is: 9.455852705286816e-05, L1 loss: 1.6636326313018799, Total Loss is: 0.041202154010534286\n",
            "MSE Loss is: 0.04507966339588165, h Loss is: 9.124379721470177e-05, L1 loss: 1.654435396194458, Total Loss is: 0.04517090693116188\n",
            "MSE Loss is: 0.04763764888048172, h Loss is: 8.996004180517048e-05, L1 loss: 1.648097276687622, Total Loss is: 0.04772760719060898\n",
            "MSE Loss is: 0.04963112249970436, h Loss is: 9.065213816938922e-05, L1 loss: 1.6446994543075562, Total Loss is: 0.04972177371382713\n",
            "MSE Loss is: 0.04936745762825012, h Loss is: 9.328414307674393e-05, L1 loss: 1.6441701650619507, Total Loss is: 0.04946074262261391\n",
            "MSE Loss is: 0.04872618988156319, h Loss is: 9.772563498700038e-05, L1 loss: 1.6462223529815674, Total Loss is: 0.04882391542196274\n",
            "MSE Loss is: 0.046320803463459015, h Loss is: 0.00010403744090581313, L1 loss: 1.6506203413009644, Total Loss is: 0.04642483964562416\n",
            "MSE Loss is: 0.04497803375124931, h Loss is: 0.00011216208076803014, L1 loss: 1.6569098234176636, Total Loss is: 0.045090194791555405\n",
            "MSE Loss is: 0.041515011340379715, h Loss is: 0.00012210756540298462, L1 loss: 1.6646978855133057, Total Loss is: 0.0416371189057827\n",
            "MSE Loss is: 0.03943517059087753, h Loss is: 0.00013377482537180185, L1 loss: 1.6735025644302368, Total Loss is: 0.0395689457654953\n",
            "MSE Loss is: 0.036139629781246185, h Loss is: 0.00014692098193336278, L1 loss: 1.6827707290649414, Total Loss is: 0.036286551505327225\n",
            "MSE Loss is: 0.0362175777554512, h Loss is: 0.00016120501095429063, L1 loss: 1.692023515701294, Total Loss is: 0.036378782242536545\n",
            "MSE Loss is: 0.03606480360031128, h Loss is: 0.00017609789210837334, L1 loss: 1.7007406949996948, Total Loss is: 0.03624090179800987\n",
            "MSE Loss is: 0.037081100046634674, h Loss is: 0.00019087459077127278, L1 loss: 1.7094509601593018, Total Loss is: 0.037271976470947266\n",
            "MSE Loss is: 0.03675990551710129, h Loss is: 0.00020472885807976127, L1 loss: 1.7167824506759644, Total Loss is: 0.03696463257074356\n",
            "MSE Loss is: 0.03773926943540573, h Loss is: 0.00021690793801099062, L1 loss: 1.7219680547714233, Total Loss is: 0.037956178188323975\n",
            "MSE Loss is: 0.03938767686486244, h Loss is: 0.0002268923562951386, L1 loss: 1.7248731851577759, Total Loss is: 0.03961456939578056\n",
            "MSE Loss is: 0.03832530602812767, h Loss is: 0.00023444033286068588, L1 loss: 1.725480079650879, Total Loss is: 0.038559745997190475\n",
            "MSE Loss is: 0.03833308070898056, h Loss is: 0.00023966269509401172, L1 loss: 1.724005103111267, Total Loss is: 0.03857274353504181\n",
            "MSE Loss is: 0.03592028096318245, h Loss is: 0.00024281532387249172, L1 loss: 1.7207828760147095, Total Loss is: 0.0361630953848362\n",
            "MSE Loss is: 0.03601723909378052, h Loss is: 0.0002446788130328059, L1 loss: 1.7162363529205322, Total Loss is: 0.036261916160583496\n",
            "New h_val is : tf.Tensor(0.022193432, shape=(), dtype=float32)\n",
            "Epoch: {} 1\n",
            "MSE Loss is: 0.03493686765432358, h Loss is: 0.0007380396127700806, L1 loss: 1.7114113569259644, Total Loss is: 0.03567490726709366\n",
            "MSE Loss is: 0.03509514406323433, h Loss is: 0.000740177696570754, L1 loss: 1.7062345743179321, Total Loss is: 0.03583532199263573\n",
            "MSE Loss is: 0.03481821343302727, h Loss is: 0.0007436542073264718, L1 loss: 1.701267123222351, Total Loss is: 0.03556186705827713\n",
            "MSE Loss is: 0.03390682488679886, h Loss is: 0.000749263446778059, L1 loss: 1.6968204975128174, Total Loss is: 0.034656088799238205\n",
            "MSE Loss is: 0.03492831438779831, h Loss is: 0.0007581985555589199, L1 loss: 1.6932411193847656, Total Loss is: 0.03568651154637337\n",
            "MSE Loss is: 0.03513308987021446, h Loss is: 0.0007711059879511595, L1 loss: 1.6907585859298706, Total Loss is: 0.03590419515967369\n",
            "MSE Loss is: 0.034696340560913086, h Loss is: 0.0007880824850872159, L1 loss: 1.6894243955612183, Total Loss is: 0.035484421998262405\n",
            "MSE Loss is: 0.0352063812315464, h Loss is: 0.0008098842808976769, L1 loss: 1.6893023252487183, Total Loss is: 0.03601626679301262\n",
            "MSE Loss is: 0.03458181768655777, h Loss is: 0.0008360580541193485, L1 loss: 1.6902917623519897, Total Loss is: 0.03541787713766098\n",
            "MSE Loss is: 0.035061802715063095, h Loss is: 0.0008666428038850427, L1 loss: 1.692270278930664, Total Loss is: 0.03592844679951668\n",
            "MSE Loss is: 0.033667393028736115, h Loss is: 0.0009014200768433511, L1 loss: 1.6950534582138062, Total Loss is: 0.03456881269812584\n",
            "MSE Loss is: 0.03278640657663345, h Loss is: 0.0009398424299433827, L1 loss: 1.6985267400741577, Total Loss is: 0.033726248890161514\n",
            "MSE Loss is: 0.03449463099241257, h Loss is: 0.0009811172494664788, L1 loss: 1.702535629272461, Total Loss is: 0.035475749522447586\n",
            "MSE Loss is: 0.033535171300172806, h Loss is: 0.0010241551790386438, L1 loss: 1.7064746618270874, Total Loss is: 0.034559328109025955\n",
            "MSE Loss is: 0.03333505243062973, h Loss is: 0.0010682527208700776, L1 loss: 1.710140585899353, Total Loss is: 0.03440330550074577\n",
            "MSE Loss is: 0.03334618732333183, h Loss is: 0.001112480997107923, L1 loss: 1.7133105993270874, Total Loss is: 0.034458667039871216\n",
            "MSE Loss is: 0.03200012817978859, h Loss is: 0.0011555214878171682, L1 loss: 1.7159687280654907, Total Loss is: 0.0331556499004364\n",
            "MSE Loss is: 0.03325129300355911, h Loss is: 0.0011973108630627394, L1 loss: 1.7181915044784546, Total Loss is: 0.03444860503077507\n",
            "MSE Loss is: 0.0328679233789444, h Loss is: 0.001237193588167429, L1 loss: 1.7195465564727783, Total Loss is: 0.03410511836409569\n",
            "MSE Loss is: 0.03395587205886841, h Loss is: 0.00127533043269068, L1 loss: 1.7200809717178345, Total Loss is: 0.035231202840805054\n",
            "MSE Loss is: 0.03343921899795532, h Loss is: 0.001311980769969523, L1 loss: 1.7199162244796753, Total Loss is: 0.034751199185848236\n",
            "MSE Loss is: 0.034155022352933884, h Loss is: 0.0013476130552589893, L1 loss: 1.7192178964614868, Total Loss is: 0.035502634942531586\n",
            "MSE Loss is: 0.03209111839532852, h Loss is: 0.0013831581454724073, L1 loss: 1.7181822061538696, Total Loss is: 0.03347427770495415\n",
            "MSE Loss is: 0.032955773174762726, h Loss is: 0.001419232925400138, L1 loss: 1.7169803380966187, Total Loss is: 0.03437500447034836\n",
            "MSE Loss is: 0.032388195395469666, h Loss is: 0.0014572886284440756, L1 loss: 1.7159117460250854, Total Loss is: 0.033845484256744385\n",
            "MSE Loss is: 0.03277287632226944, h Loss is: 0.0014980831183493137, L1 loss: 1.7151066064834595, Total Loss is: 0.034270960837602615\n",
            "MSE Loss is: 0.032238833606243134, h Loss is: 0.001541897188872099, L1 loss: 1.7147282361984253, Total Loss is: 0.03378073126077652\n",
            "MSE Loss is: 0.0309685580432415, h Loss is: 0.0015896627446636558, L1 loss: 1.7150455713272095, Total Loss is: 0.032558221369981766\n",
            "MSE Loss is: 0.03198173642158508, h Loss is: 0.0016413964331150055, L1 loss: 1.7158130407333374, Total Loss is: 0.03362313285470009\n",
            "MSE Loss is: 0.0326484851539135, h Loss is: 0.0016978014027699828, L1 loss: 1.7171071767807007, Total Loss is: 0.034346286207437515\n",
            "MSE Loss is: 0.03238901495933533, h Loss is: 0.0017583263106644154, L1 loss: 1.7188732624053955, Total Loss is: 0.034147340804338455\n",
            "MSE Loss is: 0.03238580375909805, h Loss is: 0.0018229987472295761, L1 loss: 1.7211395502090454, Total Loss is: 0.03420880436897278\n",
            "MSE Loss is: 0.03211445361375809, h Loss is: 0.0018916901899501681, L1 loss: 1.7237436771392822, Total Loss is: 0.03400614485144615\n",
            "MSE Loss is: 0.03187350183725357, h Loss is: 0.0019627390429377556, L1 loss: 1.7266438007354736, Total Loss is: 0.0338362418115139\n",
            "MSE Loss is: 0.03158882260322571, h Loss is: 0.0020363344810903072, L1 loss: 1.729522705078125, Total Loss is: 0.03362515568733215\n",
            "MSE Loss is: 0.0324973464012146, h Loss is: 0.002111561596393585, L1 loss: 1.7322269678115845, Total Loss is: 0.034608907997608185\n",
            "MSE Loss is: 0.03161739930510521, h Loss is: 0.0021881083957850933, L1 loss: 1.7346290349960327, Total Loss is: 0.03380550816655159\n",
            "MSE Loss is: 0.032209061086177826, h Loss is: 0.002266080118715763, L1 loss: 1.7366737127304077, Total Loss is: 0.034475140273571014\n",
            "MSE Loss is: 0.030428919941186905, h Loss is: 0.0023446637205779552, L1 loss: 1.7382458448410034, Total Loss is: 0.03277358412742615\n",
            "MSE Loss is: 0.03140328451991081, h Loss is: 0.0024243872612714767, L1 loss: 1.7394638061523438, Total Loss is: 0.03382766991853714\n",
            "MSE Loss is: 0.03132934868335724, h Loss is: 0.002505389740690589, L1 loss: 1.7404230833053589, Total Loss is: 0.03383473679423332\n",
            "MSE Loss is: 0.031585149466991425, h Loss is: 0.002588104922324419, L1 loss: 1.7410919666290283, Total Loss is: 0.034173253923654556\n",
            "MSE Loss is: 0.03046036884188652, h Loss is: 0.002672518603503704, L1 loss: 1.7416142225265503, Total Loss is: 0.0331328883767128\n",
            "MSE Loss is: 0.0304409172385931, h Loss is: 0.0027594277635216713, L1 loss: 1.7419769763946533, Total Loss is: 0.03320034593343735\n",
            "MSE Loss is: 0.031568851321935654, h Loss is: 0.0028496377635747194, L1 loss: 1.742366075515747, Total Loss is: 0.034418489784002304\n",
            "MSE Loss is: 0.030716512352228165, h Loss is: 0.002944380510598421, L1 loss: 1.7428330183029175, Total Loss is: 0.0336608923971653\n",
            "MSE Loss is: 0.03095657005906105, h Loss is: 0.003044418292120099, L1 loss: 1.7434767484664917, Total Loss is: 0.03400098904967308\n",
            "MSE Loss is: 0.029551323503255844, h Loss is: 0.003149457508698106, L1 loss: 1.7443430423736572, Total Loss is: 0.032700780779123306\n",
            "MSE Loss is: 0.030508125200867653, h Loss is: 0.0032606979366391897, L1 loss: 1.745369791984558, Total Loss is: 0.03376882150769234\n",
            "New h_val is : tf.Tensor(0.062983036, shape=(), dtype=float32)\n",
            "Epoch: {} 2\n",
            "MSE Loss is: 0.030031049624085426, h Loss is: 0.007930641993880272, L1 loss: 1.7465800046920776, Total Loss is: 0.0379616916179657\n",
            "MSE Loss is: 0.030606722459197044, h Loss is: 0.008154337294399738, L1 loss: 1.7482212781906128, Total Loss is: 0.03876106068491936\n",
            "MSE Loss is: 0.030493857339024544, h Loss is: 0.008387484587728977, L1 loss: 1.7500749826431274, Total Loss is: 0.038881342858076096\n",
            "MSE Loss is: 0.029510993510484695, h Loss is: 0.008627534843981266, L1 loss: 1.7519992589950562, Total Loss is: 0.03813852742314339\n",
            "MSE Loss is: 0.03029821813106537, h Loss is: 0.008874703198671341, L1 loss: 1.7539433240890503, Total Loss is: 0.03917292132973671\n",
            "MSE Loss is: 0.03010886162519455, h Loss is: 0.009128646925091743, L1 loss: 1.755862832069397, Total Loss is: 0.039237506687641144\n",
            "MSE Loss is: 0.029704969376325607, h Loss is: 0.00938592292368412, L1 loss: 1.7579281330108643, Total Loss is: 0.03909089416265488\n",
            "MSE Loss is: 0.030062425881624222, h Loss is: 0.00964999571442604, L1 loss: 1.7600948810577393, Total Loss is: 0.03971242159605026\n",
            "MSE Loss is: 0.029510241001844406, h Loss is: 0.009917267598211765, L1 loss: 1.7621395587921143, Total Loss is: 0.0394275076687336\n",
            "MSE Loss is: 0.030112896114587784, h Loss is: 0.010188087821006775, L1 loss: 1.7639979124069214, Total Loss is: 0.04030098393559456\n",
            "MSE Loss is: 0.029258528724312782, h Loss is: 0.010464849881827831, L1 loss: 1.765740990638733, Total Loss is: 0.03972337767481804\n",
            "MSE Loss is: 0.02869626320898533, h Loss is: 0.010746540501713753, L1 loss: 1.7672855854034424, Total Loss is: 0.03944280371069908\n",
            "MSE Loss is: 0.0304886344820261, h Loss is: 0.0110337408259511, L1 loss: 1.7687164545059204, Total Loss is: 0.041522376239299774\n",
            "MSE Loss is: 0.029649004340171814, h Loss is: 0.011327054351568222, L1 loss: 1.7700722217559814, Total Loss is: 0.040976058691740036\n",
            "MSE Loss is: 0.029463550075888634, h Loss is: 0.011627543717622757, L1 loss: 1.7714117765426636, Total Loss is: 0.04109109193086624\n",
            "MSE Loss is: 0.02933395653963089, h Loss is: 0.011936753056943417, L1 loss: 1.7728108167648315, Total Loss is: 0.04127071052789688\n",
            "MSE Loss is: 0.02802145481109619, h Loss is: 0.012252884916961193, L1 loss: 1.774240493774414, Total Loss is: 0.04027434065937996\n",
            "MSE Loss is: 0.029202338308095932, h Loss is: 0.012580939568579197, L1 loss: 1.7757530212402344, Total Loss is: 0.041783276945352554\n",
            "MSE Loss is: 0.028808273375034332, h Loss is: 0.012919262051582336, L1 loss: 1.7772892713546753, Total Loss is: 0.04172753542661667\n",
            "MSE Loss is: 0.02996079996228218, h Loss is: 0.013269172981381416, L1 loss: 1.7789005041122437, Total Loss is: 0.043229974806308746\n",
            "MSE Loss is: 0.02955055609345436, h Loss is: 0.013631206005811691, L1 loss: 1.7806599140167236, Total Loss is: 0.04318176209926605\n",
            "MSE Loss is: 0.030341356992721558, h Loss is: 0.014003640040755272, L1 loss: 1.7826111316680908, Total Loss is: 0.04434499889612198\n",
            "MSE Loss is: 0.028476499021053314, h Loss is: 0.014388883486390114, L1 loss: 1.7845820188522339, Total Loss is: 0.04286538064479828\n",
            "MSE Loss is: 0.02948637306690216, h Loss is: 0.01478422712534666, L1 loss: 1.7864646911621094, Total Loss is: 0.044270601123571396\n",
            "MSE Loss is: 0.028946731239557266, h Loss is: 0.015191130340099335, L1 loss: 1.7883013486862183, Total Loss is: 0.0441378615796566\n",
            "MSE Loss is: 0.02917056530714035, h Loss is: 0.01561041921377182, L1 loss: 1.7900470495224, Total Loss is: 0.04478098452091217\n",
            "MSE Loss is: 0.02868783473968506, h Loss is: 0.016037076711654663, L1 loss: 1.7916926145553589, Total Loss is: 0.04472491145133972\n",
            "MSE Loss is: 0.02780434489250183, h Loss is: 0.016476325690746307, L1 loss: 1.7931820154190063, Total Loss is: 0.04428067058324814\n",
            "MSE Loss is: 0.02860340103507042, h Loss is: 0.01692364364862442, L1 loss: 1.7944484949111938, Total Loss is: 0.04552704468369484\n",
            "MSE Loss is: 0.02904462441802025, h Loss is: 0.017382532358169556, L1 loss: 1.7955976724624634, Total Loss is: 0.046427156776189804\n",
            "MSE Loss is: 0.02894846349954605, h Loss is: 0.01785123161971569, L1 loss: 1.7966893911361694, Total Loss is: 0.04679969698190689\n",
            "MSE Loss is: 0.028940893709659576, h Loss is: 0.018331371247768402, L1 loss: 1.7977228164672852, Total Loss is: 0.04727226495742798\n",
            "MSE Loss is: 0.02878705784678459, h Loss is: 0.018825819715857506, L1 loss: 1.7987667322158813, Total Loss is: 0.04761287569999695\n",
            "MSE Loss is: 0.02858862094581127, h Loss is: 0.0193265900015831, L1 loss: 1.7997883558273315, Total Loss is: 0.04791521281003952\n",
            "MSE Loss is: 0.028343329206109047, h Loss is: 0.019840018823742867, L1 loss: 1.800845742225647, Total Loss is: 0.04818334802985191\n",
            "MSE Loss is: 0.029295679181814194, h Loss is: 0.020366448909044266, L1 loss: 1.8019567728042603, Total Loss is: 0.04966212809085846\n",
            "MSE Loss is: 0.028355572372674942, h Loss is: 0.020905567333102226, L1 loss: 1.8030613660812378, Total Loss is: 0.04926113784313202\n",
            "MSE Loss is: 0.029117170721292496, h Loss is: 0.021463783457875252, L1 loss: 1.8041902780532837, Total Loss is: 0.0505809560418129\n",
            "MSE Loss is: 0.02739252895116806, h Loss is: 0.022036990150809288, L1 loss: 1.8052414655685425, Total Loss is: 0.0494295209646225\n",
            "MSE Loss is: 0.02823515422642231, h Loss is: 0.02262846753001213, L1 loss: 1.806322455406189, Total Loss is: 0.05086362361907959\n",
            "MSE Loss is: 0.02824931964278221, h Loss is: 0.023240013048052788, L1 loss: 1.8073619604110718, Total Loss is: 0.05148933082818985\n",
            "MSE Loss is: 0.02852058783173561, h Loss is: 0.023869961500167847, L1 loss: 1.8082973957061768, Total Loss is: 0.05239054933190346\n",
            "MSE Loss is: 0.02748243883252144, h Loss is: 0.024512995034456253, L1 loss: 1.8092559576034546, Total Loss is: 0.05199543386697769\n",
            "MSE Loss is: 0.027390895411372185, h Loss is: 0.02517148107290268, L1 loss: 1.8101036548614502, Total Loss is: 0.05256237834692001\n",
            "MSE Loss is: 0.028571579605340958, h Loss is: 0.025845449417829514, L1 loss: 1.8109276294708252, Total Loss is: 0.05441702902317047\n",
            "MSE Loss is: 0.02780190482735634, h Loss is: 0.026540223509073257, L1 loss: 1.81169855594635, Total Loss is: 0.054342128336429596\n",
            "MSE Loss is: 0.02781892567873001, h Loss is: 0.027256008237600327, L1 loss: 1.8124243021011353, Total Loss is: 0.05507493391633034\n",
            "MSE Loss is: 0.0268244631588459, h Loss is: 0.027988962829113007, L1 loss: 1.813222885131836, Total Loss is: 0.05481342598795891\n",
            "MSE Loss is: 0.027613043785095215, h Loss is: 0.02874322235584259, L1 loss: 1.813942313194275, Total Loss is: 0.056356266140937805\n",
            "New h_val is : tf.Tensor(0.16300535, shape=(), dtype=float32)\n",
            "Epoch: {} 3\n",
            "MSE Loss is: 0.02717125415802002, h Loss is: 0.06308835744857788, L1 loss: 1.8146339654922485, Total Loss is: 0.0902596116065979\n",
            "MSE Loss is: 0.02762284129858017, h Loss is: 0.06447044014930725, L1 loss: 1.8152700662612915, Total Loss is: 0.09209328144788742\n",
            "MSE Loss is: 0.027586832642555237, h Loss is: 0.06588362157344818, L1 loss: 1.8160055875778198, Total Loss is: 0.09347045421600342\n",
            "MSE Loss is: 0.026717137545347214, h Loss is: 0.06732245534658432, L1 loss: 1.8167736530303955, Total Loss is: 0.09403958916664124\n",
            "MSE Loss is: 0.027572151273489, h Loss is: 0.06879273802042007, L1 loss: 1.8175560235977173, Total Loss is: 0.09636488556861877\n",
            "MSE Loss is: 0.027251582592725754, h Loss is: 0.07029516994953156, L1 loss: 1.8183609247207642, Total Loss is: 0.09754675626754761\n",
            "MSE Loss is: 0.027073189616203308, h Loss is: 0.07181812822818756, L1 loss: 1.8191293478012085, Total Loss is: 0.09889131784439087\n",
            "MSE Loss is: 0.02727198600769043, h Loss is: 0.07338447123765945, L1 loss: 1.8198703527450562, Total Loss is: 0.10065645724534988\n",
            "MSE Loss is: 0.02677265554666519, h Loss is: 0.07497920840978622, L1 loss: 1.820583701133728, Total Loss is: 0.10175186395645142\n",
            "MSE Loss is: 0.027281463146209717, h Loss is: 0.07659850269556046, L1 loss: 1.8212378025054932, Total Loss is: 0.10387996584177017\n",
            "MSE Loss is: 0.026602137833833694, h Loss is: 0.07826181501150131, L1 loss: 1.8220096826553345, Total Loss is: 0.1048639565706253\n",
            "MSE Loss is: 0.02608613483607769, h Loss is: 0.07995755970478058, L1 loss: 1.8226631879806519, Total Loss is: 0.10604369640350342\n",
            "MSE Loss is: 0.02773217484354973, h Loss is: 0.08168568462133408, L1 loss: 1.8232685327529907, Total Loss is: 0.1094178557395935\n",
            "MSE Loss is: 0.026970919221639633, h Loss is: 0.0834524855017662, L1 loss: 1.8238409757614136, Total Loss is: 0.11042340099811554\n",
            "MSE Loss is: 0.026819154620170593, h Loss is: 0.08524879813194275, L1 loss: 1.8243849277496338, Total Loss is: 0.11206795275211334\n",
            "MSE Loss is: 0.026637215167284012, h Loss is: 0.0870848074555397, L1 loss: 1.8249629735946655, Total Loss is: 0.11372202634811401\n",
            "MSE Loss is: 0.025454379618167877, h Loss is: 0.08894702792167664, L1 loss: 1.8255327939987183, Total Loss is: 0.11440140753984451\n",
            "MSE Loss is: 0.026648947969079018, h Loss is: 0.09085938334465027, L1 loss: 1.826116919517517, Total Loss is: 0.11750832945108414\n",
            "MSE Loss is: 0.026204928755760193, h Loss is: 0.09280973672866821, L1 loss: 1.8266537189483643, Total Loss is: 0.1190146654844284\n",
            "MSE Loss is: 0.027343252673745155, h Loss is: 0.09480394423007965, L1 loss: 1.827191948890686, Total Loss is: 0.12214719504117966\n",
            "MSE Loss is: 0.02699422836303711, h Loss is: 0.09684456884860992, L1 loss: 1.8277809619903564, Total Loss is: 0.12383879721164703\n",
            "MSE Loss is: 0.027654416859149933, h Loss is: 0.09892257302999496, L1 loss: 1.828372597694397, Total Loss is: 0.1265769898891449\n",
            "MSE Loss is: 0.025993477553129196, h Loss is: 0.10106085240840912, L1 loss: 1.8289902210235596, Total Loss is: 0.1270543336868286\n",
            "MSE Loss is: 0.027007970958948135, h Loss is: 0.10324503481388092, L1 loss: 1.8295387029647827, Total Loss is: 0.13025300204753876\n",
            "MSE Loss is: 0.026468820869922638, h Loss is: 0.10548192262649536, L1 loss: 1.8300694227218628, Total Loss is: 0.1319507360458374\n",
            "MSE Loss is: 0.026617366820573807, h Loss is: 0.10778577625751495, L1 loss: 1.830575942993164, Total Loss is: 0.13440313935279846\n",
            "MSE Loss is: 0.02614002674818039, h Loss is: 0.11012369394302368, L1 loss: 1.8310426473617554, Total Loss is: 0.13626372814178467\n",
            "MSE Loss is: 0.02555992640554905, h Loss is: 0.11253752559423447, L1 loss: 1.8314392566680908, Total Loss is: 0.13809745013713837\n",
            "MSE Loss is: 0.02619893290102482, h Loss is: 0.1149931326508522, L1 loss: 1.8316847085952759, Total Loss is: 0.14119206368923187\n",
            "MSE Loss is: 0.026478849351406097, h Loss is: 0.1175055205821991, L1 loss: 1.8318763971328735, Total Loss is: 0.1439843773841858\n",
            "MSE Loss is: 0.026477135717868805, h Loss is: 0.12006771564483643, L1 loss: 1.8320728540420532, Total Loss is: 0.14654484391212463\n",
            "MSE Loss is: 0.026441439986228943, h Loss is: 0.12268543243408203, L1 loss: 1.8323558568954468, Total Loss is: 0.14912687242031097\n",
            "MSE Loss is: 0.026345260441303253, h Loss is: 0.12536755204200745, L1 loss: 1.832708716392517, Total Loss is: 0.1517128050327301\n",
            "MSE Loss is: 0.026134392246603966, h Loss is: 0.12806189060211182, L1 loss: 1.8330610990524292, Total Loss is: 0.15419627726078033\n",
            "MSE Loss is: 0.02594824507832527, h Loss is: 0.130803182721138, L1 loss: 1.8334639072418213, Total Loss is: 0.15675142407417297\n",
            "MSE Loss is: 0.02689778059720993, h Loss is: 0.133591428399086, L1 loss: 1.8339035511016846, Total Loss is: 0.16048920154571533\n",
            "MSE Loss is: 0.02593076601624489, h Loss is: 0.13641925156116486, L1 loss: 1.83429753780365, Total Loss is: 0.16235001385211945\n",
            "MSE Loss is: 0.02682093158364296, h Loss is: 0.13932786881923676, L1 loss: 1.8346918821334839, Total Loss is: 0.16614879667758942\n",
            "MSE Loss is: 0.02514675073325634, h Loss is: 0.14229752123355865, L1 loss: 1.8349800109863281, Total Loss is: 0.16744427382946014\n",
            "MSE Loss is: 0.025883939117193222, h Loss is: 0.14534704387187958, L1 loss: 1.835267424583435, Total Loss is: 0.1712309867143631\n",
            "MSE Loss is: 0.02594934031367302, h Loss is: 0.14849118888378143, L1 loss: 1.8355177640914917, Total Loss is: 0.17444053292274475\n",
            "MSE Loss is: 0.02620159089565277, h Loss is: 0.1517208367586136, L1 loss: 1.8356590270996094, Total Loss is: 0.17792242765426636\n",
            "MSE Loss is: 0.02522588148713112, h Loss is: 0.15500101447105408, L1 loss: 1.8358433246612549, Total Loss is: 0.1802268922328949\n",
            "MSE Loss is: 0.02506307139992714, h Loss is: 0.15835478901863098, L1 loss: 1.8359647989273071, Total Loss is: 0.18341785669326782\n",
            "MSE Loss is: 0.026282142847776413, h Loss is: 0.16178134083747864, L1 loss: 1.836096167564392, Total Loss is: 0.18806348741054535\n",
            "MSE Loss is: 0.025556985288858414, h Loss is: 0.16530904173851013, L1 loss: 1.8362152576446533, Total Loss is: 0.19086602330207825\n",
            "MSE Loss is: 0.025441352277994156, h Loss is: 0.1689370572566986, L1 loss: 1.8363227844238281, Total Loss is: 0.19437840580940247\n",
            "MSE Loss is: 0.024730287492275238, h Loss is: 0.17264297604560852, L1 loss: 1.8364849090576172, Total Loss is: 0.19737327098846436\n",
            "MSE Loss is: 0.025394629687070847, h Loss is: 0.1764371693134308, L1 loss: 1.8365226984024048, Total Loss is: 0.20183180272579193\n",
            "New h_val is : tf.Tensor(0.35716534, shape=(), dtype=float32)\n",
            "Epoch: {} 4\n",
            "MSE Loss is: 0.024983618408441544, h Loss is: 0.3576563894748688, L1 loss: 1.8365516662597656, Total Loss is: 0.38264000415802\n",
            "MSE Loss is: 0.02533382922410965, h Loss is: 0.36424773931503296, L1 loss: 1.8365100622177124, Total Loss is: 0.389581561088562\n",
            "MSE Loss is: 0.02533971145749092, h Loss is: 0.37094223499298096, L1 loss: 1.836554765701294, Total Loss is: 0.3962819576263428\n",
            "MSE Loss is: 0.02457810565829277, h Loss is: 0.37771880626678467, L1 loss: 1.83661687374115, Total Loss is: 0.40229690074920654\n",
            "MSE Loss is: 0.02544330433011055, h Loss is: 0.3846096694469452, L1 loss: 1.8366889953613281, Total Loss is: 0.41005298495292664\n",
            "MSE Loss is: 0.025079626590013504, h Loss is: 0.3916093409061432, L1 loss: 1.8367919921875, Total Loss is: 0.4166889786720276\n",
            "MSE Loss is: 0.024992316961288452, h Loss is: 0.39867931604385376, L1 loss: 1.8368743658065796, Total Loss is: 0.4236716330051422\n",
            "MSE Loss is: 0.02510087564587593, h Loss is: 0.40592101216316223, L1 loss: 1.8369289636611938, Total Loss is: 0.43102189898490906\n",
            "MSE Loss is: 0.02467789314687252, h Loss is: 0.41327664256095886, L1 loss: 1.8369722366333008, Total Loss is: 0.43795454502105713\n",
            "MSE Loss is: 0.025101296603679657, h Loss is: 0.42071327567100525, L1 loss: 1.83696448802948, Total Loss is: 0.4458145797252655\n",
            "MSE Loss is: 0.02455255389213562, h Loss is: 0.4283442497253418, L1 loss: 1.836984634399414, Total Loss is: 0.4528968036174774\n",
            "MSE Loss is: 0.024053499102592468, h Loss is: 0.4361124038696289, L1 loss: 1.8369029760360718, Total Loss is: 0.46016591787338257\n",
            "MSE Loss is: 0.025581473484635353, h Loss is: 0.4440130293369293, L1 loss: 1.836798071861267, Total Loss is: 0.4695945084095001\n",
            "MSE Loss is: 0.024890750646591187, h Loss is: 0.45208972692489624, L1 loss: 1.8366680145263672, Total Loss is: 0.4769804775714874\n",
            "MSE Loss is: 0.024761388078331947, h Loss is: 0.46026983857154846, L1 loss: 1.8365176916122437, Total Loss is: 0.48503121733665466\n",
            "MSE Loss is: 0.024573566392064095, h Loss is: 0.4686163067817688, L1 loss: 1.8364118337631226, Total Loss is: 0.49318987131118774\n",
            "MSE Loss is: 0.023458482697606087, h Loss is: 0.4770616888999939, L1 loss: 1.8363075256347656, Total Loss is: 0.5005201697349548\n",
            "MSE Loss is: 0.024665646255016327, h Loss is: 0.48570430278778076, L1 loss: 1.8362244367599487, Total Loss is: 0.5103699564933777\n",
            "MSE Loss is: 0.024191100150346756, h Loss is: 0.49447906017303467, L1 loss: 1.8360977172851562, Total Loss is: 0.5186701416969299\n",
            "MSE Loss is: 0.025284137576818466, h Loss is: 0.5034013986587524, L1 loss: 1.8359718322753906, Total Loss is: 0.5286855101585388\n",
            "MSE Loss is: 0.025008046999573708, h Loss is: 0.5124788880348206, L1 loss: 1.8358854055404663, Total Loss is: 0.5374869108200073\n",
            "MSE Loss is: 0.02554836869239807, h Loss is: 0.5216646790504456, L1 loss: 1.8357994556427002, Total Loss is: 0.547213077545166\n",
            "MSE Loss is: 0.024053506553173065, h Loss is: 0.5310930013656616, L1 loss: 1.835740327835083, Total Loss is: 0.5551465153694153\n",
            "MSE Loss is: 0.025039736181497574, h Loss is: 0.5406888723373413, L1 loss: 1.8356090784072876, Total Loss is: 0.5657286047935486\n",
            "MSE Loss is: 0.02450665831565857, h Loss is: 0.5504787564277649, L1 loss: 1.835465669631958, Total Loss is: 0.5749853849411011\n",
            "MSE Loss is: 0.024630431085824966, h Loss is: 0.5605428814888, L1 loss: 1.8353121280670166, Total Loss is: 0.5851733088493347\n",
            "MSE Loss is: 0.024154867976903915, h Loss is: 0.5707159638404846, L1 loss: 1.835105538368225, Total Loss is: 0.5948708057403564\n",
            "MSE Loss is: 0.023770706728100777, h Loss is: 0.5812180042266846, L1 loss: 1.8348362445831299, Total Loss is: 0.604988694190979\n",
            "MSE Loss is: 0.024287721142172813, h Loss is: 0.5918742418289185, L1 loss: 1.8344287872314453, Total Loss is: 0.6161619424819946\n",
            "MSE Loss is: 0.024479752406477928, h Loss is: 0.6027334928512573, L1 loss: 1.833984613418579, Total Loss is: 0.6272132396697998\n",
            "MSE Loss is: 0.02454361692070961, h Loss is: 0.6137785315513611, L1 loss: 1.833553671836853, Total Loss is: 0.6383221745491028\n",
            "MSE Loss is: 0.024490371346473694, h Loss is: 0.6250317096710205, L1 loss: 1.8330968618392944, Total Loss is: 0.649522066116333\n",
            "MSE Loss is: 0.024440430104732513, h Loss is: 0.6365057826042175, L1 loss: 1.8326904773712158, Total Loss is: 0.6609461903572083\n",
            "MSE Loss is: 0.024212833493947983, h Loss is: 0.6479500532150269, L1 loss: 1.832274317741394, Total Loss is: 0.6721628904342651\n",
            "MSE Loss is: 0.024083014577627182, h Loss is: 0.6595395803451538, L1 loss: 1.831918716430664, Total Loss is: 0.6836225986480713\n",
            "MSE Loss is: 0.02500302717089653, h Loss is: 0.6712684631347656, L1 loss: 1.8315969705581665, Total Loss is: 0.6962714791297913\n",
            "MSE Loss is: 0.024036124348640442, h Loss is: 0.68308025598526, L1 loss: 1.8312126398086548, Total Loss is: 0.7071163654327393\n",
            "MSE Loss is: 0.025007173418998718, h Loss is: 0.6952034831047058, L1 loss: 1.8308357000350952, Total Loss is: 0.7202106714248657\n",
            "MSE Loss is: 0.0233929343521595, h Loss is: 0.7075618505477905, L1 loss: 1.8303558826446533, Total Loss is: 0.7309547662734985\n",
            "MSE Loss is: 0.024038221687078476, h Loss is: 0.7202293872833252, L1 loss: 1.82989501953125, Total Loss is: 0.7442675828933716\n",
            "MSE Loss is: 0.024142175912857056, h Loss is: 0.7332889437675476, L1 loss: 1.8294159173965454, Total Loss is: 0.757431149482727\n",
            "MSE Loss is: 0.024375811219215393, h Loss is: 0.7466825246810913, L1 loss: 1.8288300037384033, Total Loss is: 0.7710583209991455\n",
            "MSE Loss is: 0.023438677191734314, h Loss is: 0.7602088451385498, L1 loss: 1.8282854557037354, Total Loss is: 0.7836475372314453\n",
            "MSE Loss is: 0.02323577180504799, h Loss is: 0.7740153074264526, L1 loss: 1.8276933431625366, Total Loss is: 0.7972511053085327\n",
            "MSE Loss is: 0.024468615651130676, h Loss is: 0.7880690693855286, L1 loss: 1.8271169662475586, Total Loss is: 0.812537670135498\n",
            "MSE Loss is: 0.023769060149788857, h Loss is: 0.8024959564208984, L1 loss: 1.8265314102172852, Total Loss is: 0.8262650370597839\n",
            "MSE Loss is: 0.023597754538059235, h Loss is: 0.8172789812088013, L1 loss: 1.825979232788086, Total Loss is: 0.8408767580986023\n",
            "MSE Loss is: 0.023046568036079407, h Loss is: 0.832307755947113, L1 loss: 1.8256107568740845, Total Loss is: 0.8553543090820312\n",
            "MSE Loss is: 0.02365337871015072, h Loss is: 0.8476061820983887, L1 loss: 1.825121521949768, Total Loss is: 0.8712595701217651\n",
            "New h_val is : tf.Tensor(0.7011719, shape=(), dtype=float32)\n",
            "Epoch: {} 5\n",
            "MSE Loss is: 0.023265879601240158, h Loss is: 1.6150727272033691, L1 loss: 1.8246253728866577, Total Loss is: 1.638338565826416\n",
            "MSE Loss is: 0.023536931723356247, h Loss is: 1.6404364109039307, L1 loss: 1.8243252038955688, Total Loss is: 1.663973331451416\n",
            "MSE Loss is: 0.023567885160446167, h Loss is: 1.6660104990005493, L1 loss: 1.8241016864776611, Total Loss is: 1.6895784139633179\n",
            "MSE Loss is: 0.02290014550089836, h Loss is: 1.6917730569839478, L1 loss: 1.823889136314392, Total Loss is: 1.7146731615066528\n",
            "MSE Loss is: 0.023740097880363464, h Loss is: 1.7178850173950195, L1 loss: 1.823691725730896, Total Loss is: 1.7416250705718994\n",
            "MSE Loss is: 0.023400094360113144, h Loss is: 1.744309902191162, L1 loss: 1.8235517740249634, Total Loss is: 1.7677099704742432\n",
            "MSE Loss is: 0.023305755108594894, h Loss is: 1.7709648609161377, L1 loss: 1.8234022855758667, Total Loss is: 1.794270634651184\n",
            "MSE Loss is: 0.023373592644929886, h Loss is: 1.798213243484497, L1 loss: 1.823250651359558, Total Loss is: 1.8215868473052979\n",
            "MSE Loss is: 0.023029081523418427, h Loss is: 1.8258764743804932, L1 loss: 1.8231016397476196, Total Loss is: 1.8489055633544922\n",
            "MSE Loss is: 0.023382408544421196, h Loss is: 1.8537609577178955, L1 loss: 1.822906494140625, Total Loss is: 1.877143383026123\n",
            "MSE Loss is: 0.022924378514289856, h Loss is: 1.8823693990707397, L1 loss: 1.8227459192276, Total Loss is: 1.9052938222885132\n",
            "MSE Loss is: 0.022436756640672684, h Loss is: 1.9114468097686768, L1 loss: 1.822495698928833, Total Loss is: 1.933883547782898\n",
            "MSE Loss is: 0.023867156356573105, h Loss is: 1.9409514665603638, L1 loss: 1.8222973346710205, Total Loss is: 1.9648185968399048\n",
            "MSE Loss is: 0.0232376828789711, h Loss is: 1.9710829257965088, L1 loss: 1.8220512866973877, Total Loss is: 1.9943206310272217\n",
            "MSE Loss is: 0.023110590875148773, h Loss is: 2.0014307498931885, L1 loss: 1.821784257888794, Total Loss is: 2.0245413780212402\n",
            "MSE Loss is: 0.022965800017118454, h Loss is: 2.0323073863983154, L1 loss: 1.821559190750122, Total Loss is: 2.0552732944488525\n",
            "MSE Loss is: 0.021870028227567673, h Loss is: 2.0634474754333496, L1 loss: 1.8213169574737549, Total Loss is: 2.085317611694336\n",
            "MSE Loss is: 0.023086458444595337, h Loss is: 2.09517765045166, L1 loss: 1.8210947513580322, Total Loss is: 2.1182641983032227\n",
            "MSE Loss is: 0.022589141502976418, h Loss is: 2.127239227294922, L1 loss: 1.8208141326904297, Total Loss is: 2.1498284339904785\n",
            "MSE Loss is: 0.023620065301656723, h Loss is: 2.1596555709838867, L1 loss: 1.820534110069275, Total Loss is: 2.1832756996154785\n",
            "MSE Loss is: 0.0234084315598011, h Loss is: 2.192444086074829, L1 loss: 1.820286750793457, Total Loss is: 2.2158524990081787\n",
            "MSE Loss is: 0.02385237067937851, h Loss is: 2.2254414558410645, L1 loss: 1.8200489282608032, Total Loss is: 2.249293804168701\n",
            "MSE Loss is: 0.02248319610953331, h Loss is: 2.2592432498931885, L1 loss: 1.819840669631958, Total Loss is: 2.281726360321045\n",
            "MSE Loss is: 0.023419072851538658, h Loss is: 2.293579578399658, L1 loss: 1.81956946849823, Total Loss is: 2.3169987201690674\n",
            "MSE Loss is: 0.022900380194187164, h Loss is: 2.3285107612609863, L1 loss: 1.819308876991272, Total Loss is: 2.3514111042022705\n",
            "MSE Loss is: 0.023030679672956467, h Loss is: 2.3643994331359863, L1 loss: 1.8190444707870483, Total Loss is: 2.387430191040039\n",
            "MSE Loss is: 0.022561565041542053, h Loss is: 2.400545358657837, L1 loss: 1.8187130689620972, Total Loss is: 2.4231069087982178\n",
            "MSE Loss is: 0.022282645106315613, h Loss is: 2.437875747680664, L1 loss: 1.8183157444000244, Total Loss is: 2.460158348083496\n",
            "MSE Loss is: 0.02270776778459549, h Loss is: 2.475649833679199, L1 loss: 1.817850947380066, Total Loss is: 2.4983575344085693\n",
            "MSE Loss is: 0.022862516343593597, h Loss is: 2.5139689445495605, L1 loss: 1.817357063293457, Total Loss is: 2.5368313789367676\n",
            "MSE Loss is: 0.022970058023929596, h Loss is: 2.552807331085205, L1 loss: 1.8168632984161377, Total Loss is: 2.575777292251587\n",
            "MSE Loss is: 0.02290525659918785, h Loss is: 2.5922529697418213, L1 loss: 1.8163238763809204, Total Loss is: 2.6151583194732666\n",
            "MSE Loss is: 0.02288999781012535, h Loss is: 2.6322622299194336, L1 loss: 1.815826416015625, Total Loss is: 2.6551523208618164\n",
            "MSE Loss is: 0.022643204778432846, h Loss is: 2.67183780670166, L1 loss: 1.8152965307235718, Total Loss is: 2.6944808959960938\n",
            "MSE Loss is: 0.02255849540233612, h Loss is: 2.7117133140563965, L1 loss: 1.8148247003555298, Total Loss is: 2.734271764755249\n",
            "MSE Loss is: 0.023429187014698982, h Loss is: 2.7518868446350098, L1 loss: 1.8143768310546875, Total Loss is: 2.775315999984741\n",
            "MSE Loss is: 0.022483527660369873, h Loss is: 2.792102336883545, L1 loss: 1.813872218132019, Total Loss is: 2.8145859241485596\n",
            "MSE Loss is: 0.023497162386775017, h Loss is: 2.833388566970825, L1 loss: 1.813410997390747, Total Loss is: 2.8568856716156006\n",
            "MSE Loss is: 0.02195614203810692, h Loss is: 2.8754947185516357, L1 loss: 1.812869668006897, Total Loss is: 2.8974509239196777\n",
            "MSE Loss is: 0.02251029573380947, h Loss is: 2.918691635131836, L1 loss: 1.8124035596847534, Total Loss is: 2.94120192527771\n",
            "MSE Loss is: 0.022645078599452972, h Loss is: 2.9632842540740967, L1 loss: 1.8119693994522095, Total Loss is: 2.985929250717163\n",
            "MSE Loss is: 0.0228593572974205, h Loss is: 3.008974552154541, L1 loss: 1.811442255973816, Total Loss is: 3.0318338871002197\n",
            "MSE Loss is: 0.021936871111392975, h Loss is: 3.0547990798950195, L1 loss: 1.8109272718429565, Total Loss is: 3.0767359733581543\n",
            "MSE Loss is: 0.021722890436649323, h Loss is: 3.1014485359191895, L1 loss: 1.810365915298462, Total Loss is: 3.123171329498291\n",
            "MSE Loss is: 0.02293967455625534, h Loss is: 3.1487045288085938, L1 loss: 1.8098207712173462, Total Loss is: 3.1716442108154297\n",
            "MSE Loss is: 0.022266589105129242, h Loss is: 3.196990966796875, L1 loss: 1.8092644214630127, Total Loss is: 3.2192575931549072\n",
            "MSE Loss is: 0.022082578390836716, h Loss is: 3.246232032775879, L1 loss: 1.8086737394332886, Total Loss is: 3.2683145999908447\n",
            "MSE Loss is: 0.021609243005514145, h Loss is: 3.2959799766540527, L1 loss: 1.8081045150756836, Total Loss is: 3.317589282989502\n",
            "MSE Loss is: 0.022200729697942734, h Loss is: 3.346315383911133, L1 loss: 1.8074253797531128, Total Loss is: 3.368516206741333\n",
            "New h_val is : tf.Tensor(1.2538767, shape=(), dtype=float32)\n",
            "Epoch: {} 6\n",
            "MSE Loss is: 0.021824024617671967, h Loss is: 6.042323112487793, L1 loss: 1.8067092895507812, Total Loss is: 6.064146995544434\n",
            "MSE Loss is: 0.022039469331502914, h Loss is: 6.1221604347229, L1 loss: 1.806098222732544, Total Loss is: 6.144199848175049\n",
            "MSE Loss is: 0.022079356014728546, h Loss is: 6.202131271362305, L1 loss: 1.8058693408966064, Total Loss is: 6.224210739135742\n",
            "MSE Loss is: 0.021495115011930466, h Loss is: 6.282339096069336, L1 loss: 1.8056446313858032, Total Loss is: 6.303834438323975\n",
            "MSE Loss is: 0.02228480949997902, h Loss is: 6.363541126251221, L1 loss: 1.8054510354995728, Total Loss is: 6.385826110839844\n",
            "MSE Loss is: 0.02200314588844776, h Loss is: 6.445531845092773, L1 loss: 1.805338740348816, Total Loss is: 6.467535018920898\n",
            "MSE Loss is: 0.021853668615221977, h Loss is: 6.528285026550293, L1 loss: 1.8052440881729126, Total Loss is: 6.550138473510742\n",
            "MSE Loss is: 0.021903499960899353, h Loss is: 6.61289119720459, L1 loss: 1.8051750659942627, Total Loss is: 6.63479471206665\n",
            "MSE Loss is: 0.02163052186369896, h Loss is: 6.69883394241333, L1 loss: 1.8051130771636963, Total Loss is: 6.72046422958374\n",
            "MSE Loss is: 0.021923456341028214, h Loss is: 6.785219669342041, L1 loss: 1.8050025701522827, Total Loss is: 6.807143211364746\n",
            "MSE Loss is: 0.021532319486141205, h Loss is: 6.873846054077148, L1 loss: 1.8049087524414062, Total Loss is: 6.895378589630127\n",
            "MSE Loss is: 0.02106035314500332, h Loss is: 6.963789463043213, L1 loss: 1.8047351837158203, Total Loss is: 6.98484992980957\n",
            "MSE Loss is: 0.022403331473469734, h Loss is: 7.054683208465576, L1 loss: 1.8045167922973633, Total Loss is: 7.077086448669434\n",
            "MSE Loss is: 0.02182663418352604, h Loss is: 7.1472883224487305, L1 loss: 1.8042480945587158, Total Loss is: 7.16911506652832\n",
            "MSE Loss is: 0.021684134379029274, h Loss is: 7.239840507507324, L1 loss: 1.8039289712905884, Total Loss is: 7.261524677276611\n",
            "MSE Loss is: 0.02161373943090439, h Loss is: 7.333595275878906, L1 loss: 1.8036489486694336, Total Loss is: 7.355208873748779\n",
            "MSE Loss is: 0.020518353208899498, h Loss is: 7.427690029144287, L1 loss: 1.8033660650253296, Total Loss is: 7.448208332061768\n",
            "MSE Loss is: 0.0217373538762331, h Loss is: 7.523062229156494, L1 loss: 1.8031126260757446, Total Loss is: 7.5447998046875\n",
            "MSE Loss is: 0.02121714875102043, h Loss is: 7.618870735168457, L1 loss: 1.802801489830017, Total Loss is: 7.640088081359863\n",
            "MSE Loss is: 0.022180937230587006, h Loss is: 7.715104103088379, L1 loss: 1.8024829626083374, Total Loss is: 7.737285137176514\n",
            "MSE Loss is: 0.0220154020935297, h Loss is: 7.8117995262146, L1 loss: 1.8021900653839111, Total Loss is: 7.833815097808838\n",
            "MSE Loss is: 0.02238231897354126, h Loss is: 7.908633708953857, L1 loss: 1.802066683769226, Total Loss is: 7.931015968322754\n",
            "MSE Loss is: 0.02111390419304371, h Loss is: 8.007770538330078, L1 loss: 1.8020051717758179, Total Loss is: 8.028884887695312\n",
            "MSE Loss is: 0.021989624947309494, h Loss is: 8.108437538146973, L1 loss: 1.801926612854004, Total Loss is: 8.130427360534668\n",
            "MSE Loss is: 0.021493759006261826, h Loss is: 8.210835456848145, L1 loss: 1.8018540143966675, Total Loss is: 8.232329368591309\n",
            "MSE Loss is: 0.02164408564567566, h Loss is: 8.316203117370605, L1 loss: 1.8017959594726562, Total Loss is: 8.337846755981445\n",
            "MSE Loss is: 0.02119167521595955, h Loss is: 8.422051429748535, L1 loss: 1.8016706705093384, Total Loss is: 8.443243026733398\n",
            "MSE Loss is: 0.020964760333299637, h Loss is: 8.531644821166992, L1 loss: 1.8014957904815674, Total Loss is: 8.55260944366455\n",
            "MSE Loss is: 0.0213150717318058, h Loss is: 8.64228343963623, L1 loss: 1.801245927810669, Total Loss is: 8.66359806060791\n",
            "MSE Loss is: 0.021456625312566757, h Loss is: 8.753937721252441, L1 loss: 1.8012306690216064, Total Loss is: 8.775394439697266\n",
            "MSE Loss is: 0.021593976765871048, h Loss is: 8.866621017456055, L1 loss: 1.8012207746505737, Total Loss is: 8.888215065002441\n",
            "MSE Loss is: 0.021522242575883865, h Loss is: 8.980677604675293, L1 loss: 1.8011795282363892, Total Loss is: 9.00220012664795\n",
            "MSE Loss is: 0.02153502032160759, h Loss is: 9.095467567443848, L1 loss: 1.8012348413467407, Total Loss is: 9.117002487182617\n",
            "MSE Loss is: 0.021267568692564964, h Loss is: 9.20772647857666, L1 loss: 1.8013136386871338, Total Loss is: 9.228994369506836\n",
            "MSE Loss is: 0.021216019988059998, h Loss is: 9.320030212402344, L1 loss: 1.8014450073242188, Total Loss is: 9.341246604919434\n",
            "MSE Loss is: 0.022029921412467957, h Loss is: 9.432552337646484, L1 loss: 1.8016239404678345, Total Loss is: 9.454582214355469\n",
            "MSE Loss is: 0.02112000063061714, h Loss is: 9.544374465942383, L1 loss: 1.801774024963379, Total Loss is: 9.565494537353516\n",
            "MSE Loss is: 0.022152196615934372, h Loss is: 9.659472465515137, L1 loss: 1.801980972290039, Total Loss is: 9.681624412536621\n",
            "MSE Loss is: 0.020695775747299194, h Loss is: 9.777151107788086, L1 loss: 1.8021290302276611, Total Loss is: 9.797846794128418\n",
            "MSE Loss is: 0.021156955510377884, h Loss is: 9.898284912109375, L1 loss: 1.802347183227539, Total Loss is: 9.919442176818848\n",
            "MSE Loss is: 0.02132069505751133, h Loss is: 10.023751258850098, L1 loss: 1.8025554418563843, Total Loss is: 10.045071601867676\n",
            "MSE Loss is: 0.021517550572752953, h Loss is: 10.152318954467773, L1 loss: 1.8026803731918335, Total Loss is: 10.173836708068848\n",
            "MSE Loss is: 0.020589876919984818, h Loss is: 10.28013801574707, L1 loss: 1.8027839660644531, Total Loss is: 10.300727844238281\n",
            "MSE Loss is: 0.020388979464769363, h Loss is: 10.409843444824219, L1 loss: 1.8028202056884766, Total Loss is: 10.430232048034668\n",
            "MSE Loss is: 0.021566158160567284, h Loss is: 10.540447235107422, L1 loss: 1.802855372428894, Total Loss is: 10.562013626098633\n",
            "MSE Loss is: 0.02093186415731907, h Loss is: 10.673216819763184, L1 loss: 1.8028579950332642, Total Loss is: 10.694149017333984\n",
            "MSE Loss is: 0.0207521915435791, h Loss is: 10.807849884033203, L1 loss: 1.8028138875961304, Total Loss is: 10.828601837158203\n",
            "MSE Loss is: 0.02031898871064186, h Loss is: 10.942850112915039, L1 loss: 1.8027803897857666, Total Loss is: 10.96316909790039\n",
            "MSE Loss is: 0.020913805812597275, h Loss is: 11.078620910644531, L1 loss: 1.8027065992355347, Total Loss is: 11.09953498840332\n",
            "New h_val is : tf.Tensor(2.0523872, shape=(), dtype=float32)\n",
            "Epoch: {} 7\n",
            "MSE Loss is: 0.020536761730909348, h Loss is: 19.01072120666504, L1 loss: 1.8026474714279175, Total Loss is: 19.03125762939453\n",
            "MSE Loss is: 0.02071819081902504, h Loss is: 19.21663475036621, L1 loss: 1.8025487661361694, Total Loss is: 19.23735237121582\n",
            "MSE Loss is: 0.020756781101226807, h Loss is: 19.421531677246094, L1 loss: 1.8025261163711548, Total Loss is: 19.442289352416992\n",
            "MSE Loss is: 0.020247608423233032, h Loss is: 19.62624740600586, L1 loss: 1.8025349378585815, Total Loss is: 19.646495819091797\n",
            "MSE Loss is: 0.02097543329000473, h Loss is: 19.83367919921875, L1 loss: 1.802585482597351, Total Loss is: 19.85465431213379\n",
            "MSE Loss is: 0.020763065665960312, h Loss is: 20.042882919311523, L1 loss: 1.802743911743164, Total Loss is: 20.06364631652832\n",
            "MSE Loss is: 0.02055130898952484, h Loss is: 20.254486083984375, L1 loss: 1.802927017211914, Total Loss is: 20.27503776550293\n",
            "MSE Loss is: 0.02058682218194008, h Loss is: 20.471160888671875, L1 loss: 1.8031214475631714, Total Loss is: 20.49174690246582\n",
            "MSE Loss is: 0.02037626877427101, h Loss is: 20.691375732421875, L1 loss: 1.8033140897750854, Total Loss is: 20.71175193786621\n",
            "MSE Loss is: 0.02061828225851059, h Loss is: 20.911823272705078, L1 loss: 1.8034290075302124, Total Loss is: 20.93244171142578\n",
            "MSE Loss is: 0.02028404176235199, h Loss is: 21.137897491455078, L1 loss: 1.8035293817520142, Total Loss is: 21.15818214416504\n",
            "MSE Loss is: 0.019832946360111237, h Loss is: 21.366588592529297, L1 loss: 1.8035211563110352, Total Loss is: 21.38642120361328\n",
            "MSE Loss is: 0.021098239347338676, h Loss is: 21.596040725708008, L1 loss: 1.8034648895263672, Total Loss is: 21.61713981628418\n",
            "MSE Loss is: 0.020567823201417923, h Loss is: 21.828929901123047, L1 loss: 1.8033294677734375, Total Loss is: 21.849496841430664\n",
            "MSE Loss is: 0.020398806780576706, h Loss is: 22.05901527404785, L1 loss: 1.8031517267227173, Total Loss is: 22.07941436767578\n",
            "MSE Loss is: 0.020417816936969757, h Loss is: 22.29065704345703, L1 loss: 1.803012490272522, Total Loss is: 22.31107521057129\n",
            "MSE Loss is: 0.019327467307448387, h Loss is: 22.521621704101562, L1 loss: 1.802891731262207, Total Loss is: 22.54094886779785\n",
            "MSE Loss is: 0.020540323108434677, h Loss is: 22.75433349609375, L1 loss: 1.8028372526168823, Total Loss is: 22.774873733520508\n",
            "MSE Loss is: 0.01999678462743759, h Loss is: 22.986591339111328, L1 loss: 1.8027498722076416, Total Loss is: 23.006587982177734\n",
            "MSE Loss is: 0.02089551091194153, h Loss is: 23.21833610534668, L1 loss: 1.8027015924453735, Total Loss is: 23.23923110961914\n",
            "MSE Loss is: 0.02075444906949997, h Loss is: 23.449750900268555, L1 loss: 1.8027180433273315, Total Loss is: 23.470504760742188\n",
            "MSE Loss is: 0.02106396108865738, h Loss is: 23.680816650390625, L1 loss: 1.8027690649032593, Total Loss is: 23.701881408691406\n",
            "MSE Loss is: 0.019881930202245712, h Loss is: 23.91813850402832, L1 loss: 1.8028849363327026, Total Loss is: 23.938020706176758\n",
            "MSE Loss is: 0.0206964910030365, h Loss is: 24.160037994384766, L1 loss: 1.8029831647872925, Total Loss is: 24.180734634399414\n",
            "MSE Loss is: 0.020233210176229477, h Loss is: 24.407114028930664, L1 loss: 1.803070068359375, Total Loss is: 24.42734718322754\n",
            "MSE Loss is: 0.020405733957886696, h Loss is: 24.662490844726562, L1 loss: 1.8031433820724487, Total Loss is: 24.68289566040039\n",
            "MSE Loss is: 0.019985094666481018, h Loss is: 24.91857147216797, L1 loss: 1.8031162023544312, Total Loss is: 24.938556671142578\n",
            "MSE Loss is: 0.019780535250902176, h Loss is: 25.18457794189453, L1 loss: 1.8030140399932861, Total Loss is: 25.20435905456543\n",
            "MSE Loss is: 0.020068775862455368, h Loss is: 25.451805114746094, L1 loss: 1.8027836084365845, Total Loss is: 25.471874237060547\n",
            "MSE Loss is: 0.020205605775117874, h Loss is: 25.718849182128906, L1 loss: 1.8024895191192627, Total Loss is: 25.739055633544922\n",
            "MSE Loss is: 0.02036251872777939, h Loss is: 25.98592758178711, L1 loss: 1.802211046218872, Total Loss is: 26.006290435791016\n",
            "MSE Loss is: 0.020290914922952652, h Loss is: 26.254247665405273, L1 loss: 1.8019088506698608, Total Loss is: 26.274538040161133\n",
            "MSE Loss is: 0.02033068984746933, h Loss is: 26.520660400390625, L1 loss: 1.8016417026519775, Total Loss is: 26.540990829467773\n",
            "MSE Loss is: 0.020039502531290054, h Loss is: 26.77634048461914, L1 loss: 1.8013782501220703, Total Loss is: 26.79637908935547\n",
            "MSE Loss is: 0.020012449473142624, h Loss is: 27.029460906982422, L1 loss: 1.8013118505477905, Total Loss is: 27.04947280883789\n",
            "MSE Loss is: 0.02077188529074192, h Loss is: 27.28176498413086, L1 loss: 1.8013027906417847, Total Loss is: 27.302536010742188\n",
            "MSE Loss is: 0.01990639790892601, h Loss is: 27.531023025512695, L1 loss: 1.8012722730636597, Total Loss is: 27.55093002319336\n",
            "MSE Loss is: 0.020944254472851753, h Loss is: 27.790435791015625, L1 loss: 1.801300048828125, Total Loss is: 27.81138038635254\n",
            "MSE Loss is: 0.01957603357732296, h Loss is: 28.05862045288086, L1 loss: 1.8012710809707642, Total Loss is: 28.078195571899414\n",
            "MSE Loss is: 0.019947513937950134, h Loss is: 28.337871551513672, L1 loss: 1.8012984991073608, Total Loss is: 28.357818603515625\n",
            "MSE Loss is: 0.0201396606862545, h Loss is: 28.629671096801758, L1 loss: 1.8012993335723877, Total Loss is: 28.649810791015625\n",
            "MSE Loss is: 0.020322110503911972, h Loss is: 28.929088592529297, L1 loss: 1.8012008666992188, Total Loss is: 28.949411392211914\n",
            "MSE Loss is: 0.0193772092461586, h Loss is: 29.223066329956055, L1 loss: 1.8010376691818237, Total Loss is: 29.242443084716797\n",
            "MSE Loss is: 0.01920553669333458, h Loss is: 29.519176483154297, L1 loss: 1.8008126020431519, Total Loss is: 29.538381576538086\n",
            "MSE Loss is: 0.02033112570643425, h Loss is: 29.813947677612305, L1 loss: 1.8006232976913452, Total Loss is: 29.834278106689453\n",
            "MSE Loss is: 0.019747484475374222, h Loss is: 30.11050033569336, L1 loss: 1.8005383014678955, Total Loss is: 30.130247116088867\n",
            "MSE Loss is: 0.019579365849494934, h Loss is: 30.408061981201172, L1 loss: 1.8004153966903687, Total Loss is: 30.427640914916992\n",
            "MSE Loss is: 0.019172249361872673, h Loss is: 30.702457427978516, L1 loss: 1.8003238439559937, Total Loss is: 30.721630096435547\n",
            "MSE Loss is: 0.01977493427693844, h Loss is: 30.9960880279541, L1 loss: 1.800156831741333, Total Loss is: 31.0158634185791\n",
            "New h_val is : tf.Tensor(3.0756016, shape=(), dtype=float32)\n",
            "Epoch: {} 8\n",
            "MSE Loss is: 0.019388724118471146, h Loss is: 50.54576110839844, L1 loss: 1.7999881505966187, Total Loss is: 50.56515121459961\n",
            "MSE Loss is: 0.01955506205558777, h Loss is: 50.97040557861328, L1 loss: 1.7998192310333252, Total Loss is: 50.989959716796875\n",
            "MSE Loss is: 0.019587181508541107, h Loss is: 51.391395568847656, L1 loss: 1.7997535467147827, Total Loss is: 51.41098403930664\n",
            "MSE Loss is: 0.019142597913742065, h Loss is: 51.812255859375, L1 loss: 1.7997459173202515, Total Loss is: 51.831398010253906\n",
            "MSE Loss is: 0.019805964082479477, h Loss is: 52.24220657348633, L1 loss: 1.7997957468032837, Total Loss is: 52.26201248168945\n",
            "MSE Loss is: 0.01966388151049614, h Loss is: 52.67703628540039, L1 loss: 1.799952507019043, Total Loss is: 52.69670104980469\n",
            "MSE Loss is: 0.019401095807552338, h Loss is: 53.119483947753906, L1 loss: 1.8001402616500854, Total Loss is: 53.138885498046875\n",
            "MSE Loss is: 0.01941865123808384, h Loss is: 53.57406234741211, L1 loss: 1.8003114461898804, Total Loss is: 53.59347915649414\n",
            "MSE Loss is: 0.019260648638010025, h Loss is: 54.0357666015625, L1 loss: 1.800459861755371, Total Loss is: 54.05502700805664\n",
            "MSE Loss is: 0.019462838768959045, h Loss is: 54.49365997314453, L1 loss: 1.8005017042160034, Total Loss is: 54.51312255859375\n",
            "MSE Loss is: 0.019180940464138985, h Loss is: 54.960819244384766, L1 loss: 1.8005096912384033, Total Loss is: 54.97999954223633\n",
            "MSE Loss is: 0.018748749047517776, h Loss is: 55.42906188964844, L1 loss: 1.8004034757614136, Total Loss is: 55.447811126708984\n",
            "MSE Loss is: 0.019952841103076935, h Loss is: 55.8914794921875, L1 loss: 1.8002384901046753, Total Loss is: 55.911434173583984\n",
            "MSE Loss is: 0.01946072280406952, h Loss is: 56.35756301879883, L1 loss: 1.800021767616272, Total Loss is: 56.37702560424805\n",
            "MSE Loss is: 0.019261404871940613, h Loss is: 56.80927658081055, L1 loss: 1.7997934818267822, Total Loss is: 56.82853698730469\n",
            "MSE Loss is: 0.019373364746570587, h Loss is: 57.2608528137207, L1 loss: 1.7996433973312378, Total Loss is: 57.28022766113281\n",
            "MSE Loss is: 0.018301866948604584, h Loss is: 57.708221435546875, L1 loss: 1.799669623374939, Total Loss is: 57.726524353027344\n",
            "MSE Loss is: 0.019497986882925034, h Loss is: 58.15788269042969, L1 loss: 1.799800157546997, Total Loss is: 58.1773796081543\n",
            "MSE Loss is: 0.01893305778503418, h Loss is: 58.604827880859375, L1 loss: 1.7999175786972046, Total Loss is: 58.62376022338867\n",
            "MSE Loss is: 0.01976979523897171, h Loss is: 59.049468994140625, L1 loss: 1.8000812530517578, Total Loss is: 59.06924057006836\n",
            "MSE Loss is: 0.019632458686828613, h Loss is: 59.492584228515625, L1 loss: 1.8003008365631104, Total Loss is: 59.5122184753418\n",
            "MSE Loss is: 0.019908256828784943, h Loss is: 59.93631362915039, L1 loss: 1.8006128072738647, Total Loss is: 59.95622253417969\n",
            "MSE Loss is: 0.018799562007188797, h Loss is: 60.39645767211914, L1 loss: 1.8010129928588867, Total Loss is: 60.41525650024414\n",
            "MSE Loss is: 0.019556164741516113, h Loss is: 60.869407653808594, L1 loss: 1.8013683557510376, Total Loss is: 60.88896560668945\n",
            "MSE Loss is: 0.019133787602186203, h Loss is: 61.355506896972656, L1 loss: 1.8017008304595947, Total Loss is: 61.37464141845703\n",
            "MSE Loss is: 0.019325081259012222, h Loss is: 61.859981536865234, L1 loss: 1.8020013570785522, Total Loss is: 61.87930679321289\n",
            "MSE Loss is: 0.018949365243315697, h Loss is: 62.362056732177734, L1 loss: 1.8021997213363647, Total Loss is: 62.381004333496094\n",
            "MSE Loss is: 0.01874570921063423, h Loss is: 62.88282012939453, L1 loss: 1.8023194074630737, Total Loss is: 62.90156555175781\n",
            "MSE Loss is: 0.018987510353326797, h Loss is: 63.397762298583984, L1 loss: 1.8023430109024048, Total Loss is: 63.416748046875\n",
            "MSE Loss is: 0.01912447065114975, h Loss is: 63.901275634765625, L1 loss: 1.80233633518219, Total Loss is: 63.9203987121582\n",
            "MSE Loss is: 0.01929187774658203, h Loss is: 64.39579772949219, L1 loss: 1.8023849725723267, Total Loss is: 64.41509246826172\n",
            "MSE Loss is: 0.01922580599784851, h Loss is: 64.88735961914062, L1 loss: 1.8024606704711914, Total Loss is: 64.90658569335938\n",
            "MSE Loss is: 0.01929425075650215, h Loss is: 65.36647033691406, L1 loss: 1.802606225013733, Total Loss is: 65.3857650756836\n",
            "MSE Loss is: 0.018975764513015747, h Loss is: 65.81523132324219, L1 loss: 1.8027456998825073, Total Loss is: 65.8342056274414\n",
            "MSE Loss is: 0.01896568574011326, h Loss is: 66.25760650634766, L1 loss: 1.802986741065979, Total Loss is: 66.27657318115234\n",
            "MSE Loss is: 0.01967797614634037, h Loss is: 66.70198822021484, L1 loss: 1.8032581806182861, Total Loss is: 66.72166442871094\n",
            "MSE Loss is: 0.018861711025238037, h Loss is: 67.14363098144531, L1 loss: 1.8034740686416626, Total Loss is: 67.16249084472656\n",
            "MSE Loss is: 0.019897837191820145, h Loss is: 67.61841583251953, L1 loss: 1.8037127256393433, Total Loss is: 67.63831329345703\n",
            "MSE Loss is: 0.0186091810464859, h Loss is: 68.12117004394531, L1 loss: 1.8038628101348877, Total Loss is: 68.13977813720703\n",
            "MSE Loss is: 0.018905367702245712, h Loss is: 68.6541519165039, L1 loss: 1.804024577140808, Total Loss is: 68.67305755615234\n",
            "MSE Loss is: 0.019121874123811722, h Loss is: 69.21424865722656, L1 loss: 1.8041431903839111, Total Loss is: 69.23336791992188\n",
            "MSE Loss is: 0.01929406449198723, h Loss is: 69.78328704833984, L1 loss: 1.804154634475708, Total Loss is: 69.80258178710938\n",
            "MSE Loss is: 0.018327875062823296, h Loss is: 70.3232421875, L1 loss: 1.8040926456451416, Total Loss is: 70.34156799316406\n",
            "MSE Loss is: 0.018192525953054428, h Loss is: 70.85247802734375, L1 loss: 1.804024577140808, Total Loss is: 70.87067413330078\n",
            "MSE Loss is: 0.019263701513409615, h Loss is: 71.36394500732422, L1 loss: 1.8039817810058594, Total Loss is: 71.38320922851562\n",
            "MSE Loss is: 0.018737517297267914, h Loss is: 71.86752319335938, L1 loss: 1.803950548171997, Total Loss is: 71.88626098632812\n",
            "MSE Loss is: 0.018581479787826538, h Loss is: 72.36647033691406, L1 loss: 1.8039196729660034, Total Loss is: 72.38505554199219\n",
            "MSE Loss is: 0.018198756501078606, h Loss is: 72.85427856445312, L1 loss: 1.8039871454238892, Total Loss is: 72.87247467041016\n",
            "MSE Loss is: 0.018805569037795067, h Loss is: 73.34276580810547, L1 loss: 1.8040103912353516, Total Loss is: 73.361572265625\n",
            "New h_val is : tf.Tensor(4.2082767, shape=(), dtype=float32)\n",
            "Epoch: {} 9\n",
            "MSE Loss is: 0.018404100090265274, h Loss is: 113.48933410644531, L1 loss: 1.8040214776992798, Total Loss is: 113.50773620605469\n",
            "MSE Loss is: 0.01857026107609272, h Loss is: 114.18291473388672, L1 loss: 1.8040074110031128, Total Loss is: 114.20148468017578\n",
            "MSE Loss is: 0.01859324797987938, h Loss is: 114.88121032714844, L1 loss: 1.8040119409561157, Total Loss is: 114.8998031616211\n",
            "MSE Loss is: 0.01820143684744835, h Loss is: 115.58920288085938, L1 loss: 1.8040157556533813, Total Loss is: 115.60740661621094\n",
            "MSE Loss is: 0.018806181848049164, h Loss is: 116.32410430908203, L1 loss: 1.8040393590927124, Total Loss is: 116.34291076660156\n",
            "MSE Loss is: 0.01873055472970009, h Loss is: 117.06456756591797, L1 loss: 1.8041660785675049, Total Loss is: 117.08329772949219\n",
            "MSE Loss is: 0.018430989235639572, h Loss is: 117.81414794921875, L1 loss: 1.8043291568756104, Total Loss is: 117.83258056640625\n",
            "MSE Loss is: 0.01842612959444523, h Loss is: 118.57411193847656, L1 loss: 1.804457664489746, Total Loss is: 118.59253692626953\n",
            "MSE Loss is: 0.01830844022333622, h Loss is: 119.32974243164062, L1 loss: 1.804580569267273, Total Loss is: 119.34805297851562\n",
            "MSE Loss is: 0.018483374267816544, h Loss is: 120.05443572998047, L1 loss: 1.8046005964279175, Total Loss is: 120.07292175292969\n",
            "MSE Loss is: 0.01825067028403282, h Loss is: 120.78074645996094, L1 loss: 1.804607629776001, Total Loss is: 120.79899597167969\n",
            "MSE Loss is: 0.017827337607741356, h Loss is: 121.49581909179688, L1 loss: 1.8045456409454346, Total Loss is: 121.5136489868164\n",
            "MSE Loss is: 0.018994420766830444, h Loss is: 122.18741607666016, L1 loss: 1.8045450448989868, Total Loss is: 122.20641326904297\n",
            "MSE Loss is: 0.018528036773204803, h Loss is: 122.88966369628906, L1 loss: 1.8045576810836792, Total Loss is: 122.90819549560547\n",
            "MSE Loss is: 0.018301434814929962, h Loss is: 123.56234741210938, L1 loss: 1.8045767545700073, Total Loss is: 123.58065032958984\n",
            "MSE Loss is: 0.018499519675970078, h Loss is: 124.24404907226562, L1 loss: 1.8046621084213257, Total Loss is: 124.2625503540039\n",
            "MSE Loss is: 0.017460256814956665, h Loss is: 124.92742919921875, L1 loss: 1.8048298358917236, Total Loss is: 124.94489288330078\n",
            "MSE Loss is: 0.01862994395196438, h Loss is: 125.62242126464844, L1 loss: 1.8050791025161743, Total Loss is: 125.64105224609375\n",
            "MSE Loss is: 0.018050096929073334, h Loss is: 126.31248474121094, L1 loss: 1.8052864074707031, Total Loss is: 126.33053588867188\n",
            "MSE Loss is: 0.018828654661774635, h Loss is: 126.99454498291016, L1 loss: 1.8054993152618408, Total Loss is: 127.01337432861328\n",
            "MSE Loss is: 0.018677566200494766, h Loss is: 127.6666259765625, L1 loss: 1.805686593055725, Total Loss is: 127.685302734375\n",
            "MSE Loss is: 0.018944162875413895, h Loss is: 128.3348388671875, L1 loss: 1.8058617115020752, Total Loss is: 128.35379028320312\n",
            "MSE Loss is: 0.01789228990674019, h Loss is: 129.02920532226562, L1 loss: 1.8060835599899292, Total Loss is: 129.04710388183594\n",
            "MSE Loss is: 0.01859653741121292, h Loss is: 129.74339294433594, L1 loss: 1.8062771558761597, Total Loss is: 129.76199340820312\n",
            "MSE Loss is: 0.01821882650256157, h Loss is: 130.47897338867188, L1 loss: 1.8064663410186768, Total Loss is: 130.4971923828125\n",
            "MSE Loss is: 0.018427357077598572, h Loss is: 131.24339294433594, L1 loss: 1.8066692352294922, Total Loss is: 131.26182556152344\n",
            "MSE Loss is: 0.018102403730154037, h Loss is: 131.99366760253906, L1 loss: 1.8067845106124878, Total Loss is: 132.0117645263672\n",
            "MSE Loss is: 0.01788276806473732, h Loss is: 132.77354431152344, L1 loss: 1.806883692741394, Total Loss is: 132.7914276123047\n",
            "MSE Loss is: 0.018096264451742172, h Loss is: 133.5277099609375, L1 loss: 1.806928277015686, Total Loss is: 133.54580688476562\n",
            "MSE Loss is: 0.018236400559544563, h Loss is: 134.24472045898438, L1 loss: 1.8070003986358643, Total Loss is: 134.26295471191406\n",
            "MSE Loss is: 0.01840367540717125, h Loss is: 134.9375457763672, L1 loss: 1.8071253299713135, Total Loss is: 134.95594787597656\n",
            "MSE Loss is: 0.018347153440117836, h Loss is: 135.62777709960938, L1 loss: 1.8072315454483032, Total Loss is: 135.6461181640625\n",
            "MSE Loss is: 0.018446173518896103, h Loss is: 136.29100036621094, L1 loss: 1.8073533773422241, Total Loss is: 136.3094482421875\n",
            "MSE Loss is: 0.018104316666722298, h Loss is: 136.89633178710938, L1 loss: 1.8074123859405518, Total Loss is: 136.9144287109375\n",
            "MSE Loss is: 0.018099358305335045, h Loss is: 137.49742126464844, L1 loss: 1.8075573444366455, Total Loss is: 137.51551818847656\n",
            "MSE Loss is: 0.018776439130306244, h Loss is: 138.11610412597656, L1 loss: 1.8076876401901245, Total Loss is: 138.1348876953125\n",
            "MSE Loss is: 0.0180056169629097, h Loss is: 138.73704528808594, L1 loss: 1.8077408075332642, Total Loss is: 138.7550506591797\n",
            "MSE Loss is: 0.01903374120593071, h Loss is: 139.43528747558594, L1 loss: 1.807843804359436, Total Loss is: 139.45431518554688\n",
            "MSE Loss is: 0.017809439450502396, h Loss is: 140.19078063964844, L1 loss: 1.8079023361206055, Total Loss is: 140.20858764648438\n",
            "MSE Loss is: 0.01805136539041996, h Loss is: 140.99856567382812, L1 loss: 1.8080142736434937, Total Loss is: 141.01661682128906\n",
            "MSE Loss is: 0.018288765102624893, h Loss is: 141.8356170654297, L1 loss: 1.8081356287002563, Total Loss is: 141.85391235351562\n",
            "MSE Loss is: 0.01845560595393181, h Loss is: 142.65594482421875, L1 loss: 1.8081902265548706, Total Loss is: 142.67440795898438\n",
            "MSE Loss is: 0.017467936500906944, h Loss is: 143.3792724609375, L1 loss: 1.8081371784210205, Total Loss is: 143.39674377441406\n",
            "MSE Loss is: 0.017368126660585403, h Loss is: 144.0531768798828, L1 loss: 1.8081350326538086, Total Loss is: 144.07054138183594\n",
            "MSE Loss is: 0.018388770520687103, h Loss is: 144.68101501464844, L1 loss: 1.808161973953247, Total Loss is: 144.69940185546875\n",
            "MSE Loss is: 0.01792156510055065, h Loss is: 145.29891967773438, L1 loss: 1.808197259902954, Total Loss is: 145.3168487548828\n",
            "MSE Loss is: 0.017775047570466995, h Loss is: 145.9286651611328, L1 loss: 1.8082160949707031, Total Loss is: 145.94644165039062\n",
            "MSE Loss is: 0.01742088794708252, h Loss is: 146.5605926513672, L1 loss: 1.8083313703536987, Total Loss is: 146.57801818847656\n",
            "MSE Loss is: 0.01802380383014679, h Loss is: 147.2222137451172, L1 loss: 1.8083981275558472, Total Loss is: 147.240234375\n",
            "New h_val is : tf.Tensor(5.2620583, shape=(), dtype=float32)\n",
            "Epoch: {} 10\n",
            "MSE Loss is: 0.017597604542970657, h Loss is: 216.10293579101562, L1 loss: 1.8084102869033813, Total Loss is: 216.1205291748047\n",
            "MSE Loss is: 0.017777949571609497, h Loss is: 217.04811096191406, L1 loss: 1.8083610534667969, Total Loss is: 217.06588745117188\n",
            "MSE Loss is: 0.017791632562875748, h Loss is: 218.0052490234375, L1 loss: 1.8082538843154907, Total Loss is: 218.02304077148438\n",
            "MSE Loss is: 0.01744154468178749, h Loss is: 218.96249389648438, L1 loss: 1.8081177473068237, Total Loss is: 218.9799346923828\n",
            "MSE Loss is: 0.017998287454247475, h Loss is: 219.93826293945312, L1 loss: 1.808022141456604, Total Loss is: 219.95626831054688\n",
            "MSE Loss is: 0.01797906681895256, h Loss is: 220.87225341796875, L1 loss: 1.808081865310669, Total Loss is: 220.89022827148438\n",
            "MSE Loss is: 0.017656661570072174, h Loss is: 221.78103637695312, L1 loss: 1.8082382678985596, Total Loss is: 221.79869079589844\n",
            "MSE Loss is: 0.017626529559493065, h Loss is: 222.67095947265625, L1 loss: 1.8083988428115845, Total Loss is: 222.68858337402344\n",
            "MSE Loss is: 0.017536897212266922, h Loss is: 223.53233337402344, L1 loss: 1.808588981628418, Total Loss is: 223.54986572265625\n",
            "MSE Loss is: 0.0176987387239933, h Loss is: 224.33035278320312, L1 loss: 1.8086708784103394, Total Loss is: 224.34805297851562\n",
            "MSE Loss is: 0.017513737082481384, h Loss is: 225.14773559570312, L1 loss: 1.8087272644042969, Total Loss is: 225.16525268554688\n",
            "MSE Loss is: 0.01708487793803215, h Loss is: 225.9733123779297, L1 loss: 1.8087173700332642, Total Loss is: 225.9904022216797\n",
            "MSE Loss is: 0.018231332302093506, h Loss is: 226.7816619873047, L1 loss: 1.8087208271026611, Total Loss is: 226.79989624023438\n",
            "MSE Loss is: 0.017778264358639717, h Loss is: 227.6480712890625, L1 loss: 1.80865478515625, Total Loss is: 227.6658477783203\n",
            "MSE Loss is: 0.017533719539642334, h Loss is: 228.47122192382812, L1 loss: 1.8085826635360718, Total Loss is: 228.48875427246094\n",
            "MSE Loss is: 0.017804186791181564, h Loss is: 229.31417846679688, L1 loss: 1.8085764646530151, Total Loss is: 229.3319854736328\n",
            "MSE Loss is: 0.016808681190013885, h Loss is: 230.1471710205078, L1 loss: 1.8086708784103394, Total Loss is: 230.1639862060547\n",
            "MSE Loss is: 0.01794506050646305, h Loss is: 230.9713897705078, L1 loss: 1.8088552951812744, Total Loss is: 230.9893341064453\n",
            "MSE Loss is: 0.01735837012529373, h Loss is: 231.7442626953125, L1 loss: 1.8089942932128906, Total Loss is: 231.76162719726562\n",
            "MSE Loss is: 0.018089376389980316, h Loss is: 232.46566772460938, L1 loss: 1.809128999710083, Total Loss is: 232.4837646484375\n",
            "MSE Loss is: 0.017907846719026566, h Loss is: 233.14935302734375, L1 loss: 1.8092502355575562, Total Loss is: 233.16726684570312\n",
            "MSE Loss is: 0.018180785700678825, h Loss is: 233.82997131347656, L1 loss: 1.8094151020050049, Total Loss is: 233.84814453125\n",
            "MSE Loss is: 0.01717546582221985, h Loss is: 234.57278442382812, L1 loss: 1.8097456693649292, Total Loss is: 234.5899658203125\n",
            "MSE Loss is: 0.017832975834608078, h Loss is: 235.37786865234375, L1 loss: 1.8101240396499634, Total Loss is: 235.3957061767578\n",
            "MSE Loss is: 0.01750234328210354, h Loss is: 236.2514190673828, L1 loss: 1.8105095624923706, Total Loss is: 236.2689208984375\n",
            "MSE Loss is: 0.017721867188811302, h Loss is: 237.1895751953125, L1 loss: 1.8109203577041626, Total Loss is: 237.20729064941406\n",
            "MSE Loss is: 0.017450284212827682, h Loss is: 238.10304260253906, L1 loss: 1.8113106489181519, Total Loss is: 238.12049865722656\n",
            "MSE Loss is: 0.01720508560538292, h Loss is: 239.04981994628906, L1 loss: 1.8116798400878906, Total Loss is: 239.06703186035156\n",
            "MSE Loss is: 0.017406005412340164, h Loss is: 239.91172790527344, L1 loss: 1.8119620084762573, Total Loss is: 239.92913818359375\n",
            "MSE Loss is: 0.017546556890010834, h Loss is: 240.66592407226562, L1 loss: 1.812188744544983, Total Loss is: 240.6834716796875\n",
            "MSE Loss is: 0.017703428864479065, h Loss is: 241.35543823242188, L1 loss: 1.812464952468872, Total Loss is: 241.37313842773438\n",
            "MSE Loss is: 0.017663460224866867, h Loss is: 242.04522705078125, L1 loss: 1.8127574920654297, Total Loss is: 242.06289672851562\n",
            "MSE Loss is: 0.01779310032725334, h Loss is: 242.69102478027344, L1 loss: 1.8130505084991455, Total Loss is: 242.7088165283203\n",
            "MSE Loss is: 0.01743244007229805, h Loss is: 243.25782775878906, L1 loss: 1.813287377357483, Total Loss is: 243.27525329589844\n",
            "MSE Loss is: 0.017426826059818268, h Loss is: 243.85055541992188, L1 loss: 1.8135871887207031, Total Loss is: 243.86798095703125\n",
            "MSE Loss is: 0.018076688051223755, h Loss is: 244.51214599609375, L1 loss: 1.8138954639434814, Total Loss is: 244.5302276611328\n",
            "MSE Loss is: 0.01734093204140663, h Loss is: 245.1969757080078, L1 loss: 1.8141412734985352, Total Loss is: 245.2143096923828\n",
            "MSE Loss is: 0.01835709437727928, h Loss is: 246.0283966064453, L1 loss: 1.814416766166687, Total Loss is: 246.0467529296875\n",
            "MSE Loss is: 0.01718231290578842, h Loss is: 246.94325256347656, L1 loss: 1.8146358728408813, Total Loss is: 246.96043395996094\n",
            "MSE Loss is: 0.017394449561834335, h Loss is: 247.9136962890625, L1 loss: 1.814948320388794, Total Loss is: 247.93109130859375\n",
            "MSE Loss is: 0.01764833927154541, h Loss is: 248.8711395263672, L1 loss: 1.815214991569519, Total Loss is: 248.8887939453125\n",
            "MSE Loss is: 0.017804093658924103, h Loss is: 249.73233032226562, L1 loss: 1.8154022693634033, Total Loss is: 249.75013732910156\n",
            "MSE Loss is: 0.01680833101272583, h Loss is: 250.376220703125, L1 loss: 1.815500259399414, Total Loss is: 250.39303588867188\n",
            "MSE Loss is: 0.016731703653931618, h Loss is: 250.92156982421875, L1 loss: 1.815751075744629, Total Loss is: 250.9383087158203\n",
            "MSE Loss is: 0.017710473388433456, h Loss is: 251.42178344726562, L1 loss: 1.8161907196044922, Total Loss is: 251.4394989013672\n",
            "MSE Loss is: 0.01730136200785637, h Loss is: 251.97003173828125, L1 loss: 1.8166553974151611, Total Loss is: 251.98733520507812\n",
            "MSE Loss is: 0.017157629132270813, h Loss is: 252.6174774169922, L1 loss: 1.8171027898788452, Total Loss is: 252.63462829589844\n",
            "MSE Loss is: 0.01683402806520462, h Loss is: 253.32284545898438, L1 loss: 1.8175934553146362, Total Loss is: 253.3396759033203\n",
            "MSE Loss is: 0.017427317798137665, h Loss is: 254.10763549804688, L1 loss: 1.8180080652236938, Total Loss is: 254.12506103515625\n",
            "New h_val is : tf.Tensor(6.0651894, shape=(), dtype=float32)\n",
            "Epoch: {} 11\n",
            "MSE Loss is: 0.01696890965104103, h Loss is: 354.57257080078125, L1 loss: 1.818384051322937, Total Loss is: 354.58953857421875\n",
            "MSE Loss is: 0.01717895269393921, h Loss is: 355.6087951660156, L1 loss: 1.818769097328186, Total Loss is: 355.6259765625\n",
            "MSE Loss is: 0.0171817634254694, h Loss is: 356.5897521972656, L1 loss: 1.8192033767700195, Total Loss is: 356.60693359375\n",
            "MSE Loss is: 0.016863195225596428, h Loss is: 357.4859313964844, L1 loss: 1.8196567296981812, Total Loss is: 357.5028076171875\n",
            "MSE Loss is: 0.01738133653998375, h Loss is: 358.34271240234375, L1 loss: 1.8201452493667603, Total Loss is: 358.360107421875\n",
            "MSE Loss is: 0.017405137419700623, h Loss is: 359.0768737792969, L1 loss: 1.8206745386123657, Total Loss is: 359.0942687988281\n",
            "MSE Loss is: 0.017072871327400208, h Loss is: 359.77557373046875, L1 loss: 1.821205735206604, Total Loss is: 359.7926330566406\n",
            "MSE Loss is: 0.017019197344779968, h Loss is: 360.4839782714844, L1 loss: 1.8217029571533203, Total Loss is: 360.5010070800781\n",
            "MSE Loss is: 0.016946997493505478, h Loss is: 361.214599609375, L1 loss: 1.8222064971923828, Total Loss is: 361.2315368652344\n",
            "MSE Loss is: 0.017106231302022934, h Loss is: 361.91217041015625, L1 loss: 1.8226324319839478, Total Loss is: 361.9292907714844\n",
            "MSE Loss is: 0.016962727531790733, h Loss is: 362.7021484375, L1 loss: 1.8230926990509033, Total Loss is: 362.7191162109375\n",
            "MSE Loss is: 0.01651790365576744, h Loss is: 363.5357971191406, L1 loss: 1.8236083984375, Total Loss is: 363.55230712890625\n",
            "MSE Loss is: 0.01764780655503273, h Loss is: 364.3221435546875, L1 loss: 1.8241245746612549, Total Loss is: 364.33978271484375\n",
            "MSE Loss is: 0.017205126583576202, h Loss is: 365.18096923828125, L1 loss: 1.8246954679489136, Total Loss is: 365.19818115234375\n",
            "MSE Loss is: 0.016955938190221786, h Loss is: 365.89837646484375, L1 loss: 1.825282096862793, Total Loss is: 365.91534423828125\n",
            "MSE Loss is: 0.01727774553000927, h Loss is: 366.583984375, L1 loss: 1.8259193897247314, Total Loss is: 366.60125732421875\n",
            "MSE Loss is: 0.016332227736711502, h Loss is: 367.2037353515625, L1 loss: 1.8265892267227173, Total Loss is: 367.2200622558594\n",
            "MSE Loss is: 0.017423490062355995, h Loss is: 367.78094482421875, L1 loss: 1.827269196510315, Total Loss is: 367.7983703613281\n",
            "MSE Loss is: 0.016839005053043365, h Loss is: 368.271240234375, L1 loss: 1.827820062637329, Total Loss is: 368.2880859375\n",
            "MSE Loss is: 0.017543312162160873, h Loss is: 368.71661376953125, L1 loss: 1.8282965421676636, Total Loss is: 368.7341613769531\n",
            "MSE Loss is: 0.01731863245368004, h Loss is: 369.17437744140625, L1 loss: 1.828782320022583, Total Loss is: 369.1916809082031\n",
            "MSE Loss is: 0.0176108218729496, h Loss is: 369.71826171875, L1 loss: 1.8293001651763916, Total Loss is: 369.7358703613281\n",
            "MSE Loss is: 0.01664130948483944, h Loss is: 370.421875, L1 loss: 1.829904556274414, Total Loss is: 370.4385070800781\n",
            "MSE Loss is: 0.0172535702586174, h Loss is: 371.2460021972656, L1 loss: 1.8305752277374268, Total Loss is: 371.26324462890625\n",
            "MSE Loss is: 0.016972742974758148, h Loss is: 372.1612548828125, L1 loss: 1.831376314163208, Total Loss is: 372.17822265625\n",
            "MSE Loss is: 0.01719195768237114, h Loss is: 373.1053161621094, L1 loss: 1.8321611881256104, Total Loss is: 373.12249755859375\n",
            "MSE Loss is: 0.01697324961423874, h Loss is: 373.9229736328125, L1 loss: 1.8329471349716187, Total Loss is: 373.93994140625\n",
            "MSE Loss is: 0.016698254272341728, h Loss is: 374.7036437988281, L1 loss: 1.8336261510849, Total Loss is: 374.7203369140625\n",
            "MSE Loss is: 0.016897112131118774, h Loss is: 375.27984619140625, L1 loss: 1.8341636657714844, Total Loss is: 375.2967529296875\n",
            "MSE Loss is: 0.017037689685821533, h Loss is: 375.674072265625, L1 loss: 1.8346073627471924, Total Loss is: 375.69110107421875\n",
            "MSE Loss is: 0.017181914299726486, h Loss is: 376.029541015625, L1 loss: 1.835113763809204, Total Loss is: 376.0467224121094\n",
            "MSE Loss is: 0.01715337112545967, h Loss is: 376.5060119628906, L1 loss: 1.8357059955596924, Total Loss is: 376.5231628417969\n",
            "MSE Loss is: 0.017315510660409927, h Loss is: 377.0133361816406, L1 loss: 1.836288332939148, Total Loss is: 377.0306396484375\n",
            "MSE Loss is: 0.01694127358496189, h Loss is: 377.4773864746094, L1 loss: 1.8368778228759766, Total Loss is: 377.49432373046875\n",
            "MSE Loss is: 0.01692722737789154, h Loss is: 378.01678466796875, L1 loss: 1.8375145196914673, Total Loss is: 378.0337219238281\n",
            "MSE Loss is: 0.01755829155445099, h Loss is: 378.65155029296875, L1 loss: 1.838199496269226, Total Loss is: 378.6690979003906\n",
            "MSE Loss is: 0.016848303377628326, h Loss is: 379.2472839355469, L1 loss: 1.838833212852478, Total Loss is: 379.2641296386719\n",
            "MSE Loss is: 0.017851948738098145, h Loss is: 379.9836120605469, L1 loss: 1.8394907712936401, Total Loss is: 380.00146484375\n",
            "MSE Loss is: 0.016712605953216553, h Loss is: 380.7545166015625, L1 loss: 1.8400768041610718, Total Loss is: 380.771240234375\n",
            "MSE Loss is: 0.016915075480937958, h Loss is: 381.5451965332031, L1 loss: 1.8407353162765503, Total Loss is: 381.5621032714844\n",
            "MSE Loss is: 0.017177997156977654, h Loss is: 382.26861572265625, L1 loss: 1.8413006067276, Total Loss is: 382.2857971191406\n",
            "MSE Loss is: 0.01731831207871437, h Loss is: 382.8438720703125, L1 loss: 1.841787338256836, Total Loss is: 382.8611755371094\n",
            "MSE Loss is: 0.016323257237672806, h Loss is: 383.1410217285156, L1 loss: 1.8422783613204956, Total Loss is: 383.1573486328125\n",
            "MSE Loss is: 0.016265029087662697, h Loss is: 383.3843688964844, L1 loss: 1.8427609205245972, Total Loss is: 383.400634765625\n",
            "MSE Loss is: 0.01721116527915001, h Loss is: 383.6833801269531, L1 loss: 1.8433144092559814, Total Loss is: 383.7005920410156\n",
            "MSE Loss is: 0.016852766275405884, h Loss is: 384.157958984375, L1 loss: 1.8439264297485352, Total Loss is: 384.1748046875\n",
            "MSE Loss is: 0.016703132539987564, h Loss is: 384.832763671875, L1 loss: 1.8445402383804321, Total Loss is: 384.8494567871094\n",
            "MSE Loss is: 0.01641043648123741, h Loss is: 385.5518493652344, L1 loss: 1.8451976776123047, Total Loss is: 385.5682678222656\n",
            "MSE Loss is: 0.01699087955057621, h Loss is: 386.2953796386719, L1 loss: 1.845798373222351, Total Loss is: 386.3123779296875\n",
            "New h_val is : tf.Tensor(6.5471783, shape=(), dtype=float32)\n",
            "Epoch: {} 12\n",
            "MSE Loss is: 0.016499286517500877, h Loss is: 514.6914672851562, L1 loss: 1.846352458000183, Total Loss is: 514.7079467773438\n",
            "MSE Loss is: 0.016746528446674347, h Loss is: 515.3688354492188, L1 loss: 1.8469158411026, Total Loss is: 515.3855590820312\n",
            "MSE Loss is: 0.016738286241889, h Loss is: 515.8949584960938, L1 loss: 1.847517967224121, Total Loss is: 515.9116821289062\n",
            "MSE Loss is: 0.016441641375422478, h Loss is: 516.29931640625, L1 loss: 1.848127007484436, Total Loss is: 516.3157348632812\n",
            "MSE Loss is: 0.016925206407904625, h Loss is: 516.726318359375, L1 loss: 1.8487392663955688, Total Loss is: 516.7432250976562\n",
            "MSE Loss is: 0.016981061547994614, h Loss is: 517.073974609375, L1 loss: 1.8493521213531494, Total Loss is: 517.0909423828125\n",
            "MSE Loss is: 0.016654156148433685, h Loss is: 517.5045166015625, L1 loss: 1.8499523401260376, Total Loss is: 517.5211791992188\n",
            "MSE Loss is: 0.016581453382968903, h Loss is: 518.0414428710938, L1 loss: 1.850523829460144, Total Loss is: 518.0580444335938\n",
            "MSE Loss is: 0.016516653820872307, h Loss is: 518.648681640625, L1 loss: 1.8511451482772827, Total Loss is: 518.6652221679688\n",
            "MSE Loss is: 0.01667547971010208, h Loss is: 519.1693115234375, L1 loss: 1.8517284393310547, Total Loss is: 519.1859741210938\n",
            "MSE Loss is: 0.016564838588237762, h Loss is: 519.7461547851562, L1 loss: 1.8523567914962769, Total Loss is: 519.7626953125\n",
            "MSE Loss is: 0.016105450689792633, h Loss is: 520.2835693359375, L1 loss: 1.8529754877090454, Total Loss is: 520.2996826171875\n",
            "MSE Loss is: 0.017220567911863327, h Loss is: 520.6442260742188, L1 loss: 1.8535772562026978, Total Loss is: 520.6614379882812\n",
            "MSE Loss is: 0.01678658276796341, h Loss is: 521.0946655273438, L1 loss: 1.8542612791061401, Total Loss is: 521.1114501953125\n",
            "MSE Loss is: 0.016539471223950386, h Loss is: 521.3327026367188, L1 loss: 1.854940414428711, Total Loss is: 521.3492431640625\n",
            "MSE Loss is: 0.016887253150343895, h Loss is: 521.5789184570312, L1 loss: 1.8556270599365234, Total Loss is: 521.5958251953125\n",
            "MSE Loss is: 0.015996119007468224, h Loss is: 521.8109130859375, L1 loss: 1.8563129901885986, Total Loss is: 521.826904296875\n",
            "MSE Loss is: 0.017038512974977493, h Loss is: 522.0597534179688, L1 loss: 1.856958270072937, Total Loss is: 522.0767822265625\n",
            "MSE Loss is: 0.016467705368995667, h Loss is: 522.2279052734375, L1 loss: 1.8574516773223877, Total Loss is: 522.244384765625\n",
            "MSE Loss is: 0.017159510403871536, h Loss is: 522.3633422851562, L1 loss: 1.8578732013702393, Total Loss is: 522.3804931640625\n",
            "MSE Loss is: 0.016880100592970848, h Loss is: 522.537353515625, L1 loss: 1.8583555221557617, Total Loss is: 522.5542602539062\n",
            "MSE Loss is: 0.017203371971845627, h Loss is: 522.8352661132812, L1 loss: 1.8589270114898682, Total Loss is: 522.8524780273438\n",
            "MSE Loss is: 0.01626005209982395, h Loss is: 523.3177490234375, L1 loss: 1.859585165977478, Total Loss is: 523.333984375\n",
            "MSE Loss is: 0.016832325607538223, h Loss is: 523.905029296875, L1 loss: 1.8603229522705078, Total Loss is: 523.921875\n",
            "MSE Loss is: 0.01660008355975151, h Loss is: 524.556640625, L1 loss: 1.8610657453536987, Total Loss is: 524.5732421875\n",
            "MSE Loss is: 0.016815058887004852, h Loss is: 525.181640625, L1 loss: 1.8617511987686157, Total Loss is: 525.1984252929688\n",
            "MSE Loss is: 0.016633523628115654, h Loss is: 525.6055297851562, L1 loss: 1.8623889684677124, Total Loss is: 525.6221923828125\n",
            "MSE Loss is: 0.016327422112226486, h Loss is: 525.9942626953125, L1 loss: 1.8628952503204346, Total Loss is: 526.0106201171875\n",
            "MSE Loss is: 0.016535043716430664, h Loss is: 526.140380859375, L1 loss: 1.8632720708847046, Total Loss is: 526.1569213867188\n",
            "MSE Loss is: 0.01668001338839531, h Loss is: 526.1237182617188, L1 loss: 1.86358642578125, Total Loss is: 526.140380859375\n",
            "MSE Loss is: 0.016807103529572487, h Loss is: 526.174072265625, L1 loss: 1.8639906644821167, Total Loss is: 526.1908569335938\n",
            "MSE Loss is: 0.01678803190588951, h Loss is: 526.5098876953125, L1 loss: 1.8645209074020386, Total Loss is: 526.5266723632812\n",
            "MSE Loss is: 0.016975777223706245, h Loss is: 526.8942260742188, L1 loss: 1.8650562763214111, Total Loss is: 526.9111938476562\n",
            "MSE Loss is: 0.016597816720604897, h Loss is: 527.15625, L1 loss: 1.865610122680664, Total Loss is: 527.1728515625\n",
            "MSE Loss is: 0.016568457707762718, h Loss is: 527.4158935546875, L1 loss: 1.8661911487579346, Total Loss is: 527.4324340820312\n",
            "MSE Loss is: 0.01719326339662075, h Loss is: 527.7115478515625, L1 loss: 1.86678147315979, Total Loss is: 527.728759765625\n",
            "MSE Loss is: 0.016501346603035927, h Loss is: 527.863037109375, L1 loss: 1.8672751188278198, Total Loss is: 527.8795166015625\n",
            "MSE Loss is: 0.017487509176135063, h Loss is: 528.2041015625, L1 loss: 1.8677681684494019, Total Loss is: 528.2216186523438\n",
            "MSE Loss is: 0.016376890242099762, h Loss is: 528.6395874023438, L1 loss: 1.8682022094726562, Total Loss is: 528.6559448242188\n",
            "MSE Loss is: 0.01657440885901451, h Loss is: 529.196533203125, L1 loss: 1.8687232732772827, Total Loss is: 529.213134765625\n",
            "MSE Loss is: 0.01684543304145336, h Loss is: 529.7222900390625, L1 loss: 1.8691848516464233, Total Loss is: 529.7391357421875\n",
            "MSE Loss is: 0.016970358788967133, h Loss is: 530.076416015625, L1 loss: 1.869597315788269, Total Loss is: 530.0933837890625\n",
            "MSE Loss is: 0.015990206971764565, h Loss is: 530.058349609375, L1 loss: 1.8699816465377808, Total Loss is: 530.0743408203125\n",
            "MSE Loss is: 0.01593398116528988, h Loss is: 529.9578857421875, L1 loss: 1.8703702688217163, Total Loss is: 529.9738159179688\n",
            "MSE Loss is: 0.016852956265211105, h Loss is: 529.9383544921875, L1 loss: 1.8708324432373047, Total Loss is: 529.9552001953125\n",
            "MSE Loss is: 0.016537673771381378, h Loss is: 530.16162109375, L1 loss: 1.871346354484558, Total Loss is: 530.1781616210938\n",
            "MSE Loss is: 0.016377918422222137, h Loss is: 530.6493530273438, L1 loss: 1.8718379735946655, Total Loss is: 530.6657104492188\n",
            "MSE Loss is: 0.016120659187436104, h Loss is: 531.1406860351562, L1 loss: 1.8723537921905518, Total Loss is: 531.1567993164062\n",
            "MSE Loss is: 0.016682026907801628, h Loss is: 531.6219482421875, L1 loss: 1.8727928400039673, Total Loss is: 531.6386108398438\n",
            "New h_val is : tf.Tensor(6.741269, shape=(), dtype=float32)\n",
            "Epoch: {} 13\n",
            "MSE Loss is: 0.016164712607860565, h Loss is: 680.9103393554688, L1 loss: 1.873185396194458, Total Loss is: 680.926513671875\n",
            "MSE Loss is: 0.01644851639866829, h Loss is: 681.1438598632812, L1 loss: 1.873605728149414, Total Loss is: 681.1602783203125\n",
            "MSE Loss is: 0.016426701098680496, h Loss is: 681.2476196289062, L1 loss: 1.8740886449813843, Total Loss is: 681.2640380859375\n",
            "MSE Loss is: 0.016145974397659302, h Loss is: 681.2596435546875, L1 loss: 1.8746010065078735, Total Loss is: 681.2758178710938\n",
            "MSE Loss is: 0.016605187207460403, h Loss is: 681.3626708984375, L1 loss: 1.875133752822876, Total Loss is: 681.3792724609375\n",
            "MSE Loss is: 0.016681792214512825, h Loss is: 681.3558349609375, L1 loss: 1.8756674528121948, Total Loss is: 681.3724975585938\n",
            "MSE Loss is: 0.016364462673664093, h Loss is: 681.4478759765625, L1 loss: 1.8761786222457886, Total Loss is: 681.4642333984375\n",
            "MSE Loss is: 0.01627550646662712, h Loss is: 681.6415405273438, L1 loss: 1.8766393661499023, Total Loss is: 681.6578369140625\n",
            "MSE Loss is: 0.0162111297249794, h Loss is: 681.8817749023438, L1 loss: 1.8771418333053589, Total Loss is: 681.8980102539062\n",
            "MSE Loss is: 0.016374899074435234, h Loss is: 681.9598388671875, L1 loss: 1.8775981664657593, Total Loss is: 681.9761962890625\n",
            "MSE Loss is: 0.016289561986923218, h Loss is: 682.1073608398438, L1 loss: 1.8781136274337769, Total Loss is: 682.1236572265625\n",
            "MSE Loss is: 0.01581493578851223, h Loss is: 682.2197875976562, L1 loss: 1.8786381483078003, Total Loss is: 682.235595703125\n",
            "MSE Loss is: 0.0169210322201252, h Loss is: 682.1129150390625, L1 loss: 1.8791621923446655, Total Loss is: 682.1298217773438\n",
            "MSE Loss is: 0.016489097848534584, h Loss is: 682.2055053710938, L1 loss: 1.8797835111618042, Total Loss is: 682.2219848632812\n",
            "MSE Loss is: 0.016247175633907318, h Loss is: 682.0331420898438, L1 loss: 1.8803852796554565, Total Loss is: 682.0493774414062\n",
            "MSE Loss is: 0.016604438424110413, h Loss is: 681.8878784179688, L1 loss: 1.8810112476348877, Total Loss is: 681.9044799804688\n",
            "MSE Loss is: 0.015768995508551598, h Loss is: 681.7232666015625, L1 loss: 1.8817051649093628, Total Loss is: 681.739013671875\n",
            "MSE Loss is: 0.01676509901881218, h Loss is: 681.5635375976562, L1 loss: 1.882303237915039, Total Loss is: 681.580322265625\n",
            "MSE Loss is: 0.016210440546274185, h Loss is: 681.2656860351562, L1 loss: 1.882696509361267, Total Loss is: 681.2819213867188\n",
            "MSE Loss is: 0.0168939046561718, h Loss is: 680.924072265625, L1 loss: 1.8830063343048096, Total Loss is: 680.9409790039062\n",
            "MSE Loss is: 0.016561755910515785, h Loss is: 680.6807250976562, L1 loss: 1.8833668231964111, Total Loss is: 680.697265625\n",
            "MSE Loss is: 0.016921931877732277, h Loss is: 680.6581420898438, L1 loss: 1.883858561515808, Total Loss is: 680.675048828125\n",
            "MSE Loss is: 0.016000811010599136, h Loss is: 680.8890380859375, L1 loss: 1.8845303058624268, Total Loss is: 680.905029296875\n",
            "MSE Loss is: 0.016534069553017616, h Loss is: 681.2285766601562, L1 loss: 1.8853130340576172, Total Loss is: 681.2451171875\n",
            "MSE Loss is: 0.01634199731051922, h Loss is: 681.6032104492188, L1 loss: 1.8860679864883423, Total Loss is: 681.6195678710938\n",
            "MSE Loss is: 0.01655721850693226, h Loss is: 681.8751831054688, L1 loss: 1.8867454528808594, Total Loss is: 681.8917236328125\n",
            "MSE Loss is: 0.01639832928776741, h Loss is: 681.8613891601562, L1 loss: 1.8872966766357422, Total Loss is: 681.8778076171875\n",
            "MSE Loss is: 0.016066759824752808, h Loss is: 681.8278198242188, L1 loss: 1.8877075910568237, Total Loss is: 681.8438720703125\n",
            "MSE Loss is: 0.01628917083144188, h Loss is: 681.5285034179688, L1 loss: 1.8879836797714233, Total Loss is: 681.5447998046875\n",
            "MSE Loss is: 0.01643546298146248, h Loss is: 681.1096801757812, L1 loss: 1.8882389068603516, Total Loss is: 681.1260986328125\n",
            "MSE Loss is: 0.016538744792342186, h Loss is: 680.8854370117188, L1 loss: 1.888653039932251, Total Loss is: 680.9019775390625\n",
            "MSE Loss is: 0.016530780121684074, h Loss is: 681.1091918945312, L1 loss: 1.8892322778701782, Total Loss is: 681.125732421875\n",
            "MSE Loss is: 0.016741707921028137, h Loss is: 681.32861328125, L1 loss: 1.8898251056671143, Total Loss is: 681.3453369140625\n",
            "MSE Loss is: 0.01636103168129921, h Loss is: 681.2685546875, L1 loss: 1.890390396118164, Total Loss is: 681.284912109375\n",
            "MSE Loss is: 0.016324009746313095, h Loss is: 681.0873413085938, L1 loss: 1.890959620475769, Total Loss is: 681.1036376953125\n",
            "MSE Loss is: 0.01694396696984768, h Loss is: 680.9160766601562, L1 loss: 1.8914680480957031, Total Loss is: 680.9330444335938\n",
            "MSE Loss is: 0.016264483332633972, h Loss is: 680.5712280273438, L1 loss: 1.8918125629425049, Total Loss is: 680.5874633789062\n",
            "MSE Loss is: 0.017230039462447166, h Loss is: 680.584716796875, L1 loss: 1.892187476158142, Total Loss is: 680.6019287109375\n",
            "MSE Loss is: 0.01614169031381607, h Loss is: 680.8424072265625, L1 loss: 1.8925880193710327, Total Loss is: 680.8585205078125\n",
            "MSE Loss is: 0.016345856711268425, h Loss is: 681.3466186523438, L1 loss: 1.8931277990341187, Total Loss is: 681.3629760742188\n",
            "MSE Loss is: 0.016619162634015083, h Loss is: 681.7739868164062, L1 loss: 1.8936843872070312, Total Loss is: 681.7905883789062\n",
            "MSE Loss is: 0.016725700348615646, h Loss is: 681.8670043945312, L1 loss: 1.894182562828064, Total Loss is: 681.8837280273438\n",
            "MSE Loss is: 0.015768583863973618, h Loss is: 681.3641357421875, L1 loss: 1.8945810794830322, Total Loss is: 681.3798828125\n",
            "MSE Loss is: 0.015704240649938583, h Loss is: 680.7019653320312, L1 loss: 1.894974946975708, Total Loss is: 680.7176513671875\n",
            "MSE Loss is: 0.016605304554104805, h Loss is: 680.204833984375, L1 loss: 1.8953980207443237, Total Loss is: 680.221435546875\n",
            "MSE Loss is: 0.016324393451213837, h Loss is: 680.147705078125, L1 loss: 1.8958858251571655, Total Loss is: 680.1640014648438\n",
            "MSE Loss is: 0.016148224472999573, h Loss is: 680.5568237304688, L1 loss: 1.8963654041290283, Total Loss is: 680.572998046875\n",
            "MSE Loss is: 0.015925999730825424, h Loss is: 680.9878540039062, L1 loss: 1.896905779838562, Total Loss is: 681.0037841796875\n",
            "MSE Loss is: 0.016465725377202034, h Loss is: 681.3628540039062, L1 loss: 1.897351861000061, Total Loss is: 681.3793334960938\n",
            "New h_val is : tf.Tensor(6.7446003, shape=(), dtype=float32)\n",
            "Epoch: {} 14\n",
            "MSE Loss is: 0.01592923514544964, h Loss is: 845.4478759765625, L1 loss: 1.8977645635604858, Total Loss is: 845.4638061523438\n",
            "MSE Loss is: 0.01624479703605175, h Loss is: 845.242431640625, L1 loss: 1.8982042074203491, Total Loss is: 845.2586669921875\n",
            "MSE Loss is: 0.016212468966841698, h Loss is: 844.8267211914062, L1 loss: 1.8986819982528687, Total Loss is: 844.8429565429688\n",
            "MSE Loss is: 0.01594323106110096, h Loss is: 844.3089599609375, L1 loss: 1.8991730213165283, Total Loss is: 844.3248901367188\n",
            "MSE Loss is: 0.016383446753025055, h Loss is: 843.9862670898438, L1 loss: 1.8997215032577515, Total Loss is: 844.0026245117188\n",
            "MSE Loss is: 0.01647147536277771, h Loss is: 843.5977783203125, L1 loss: 1.9003117084503174, Total Loss is: 843.6142578125\n",
            "MSE Loss is: 0.01616840809583664, h Loss is: 843.4259033203125, L1 loss: 1.9009073972702026, Total Loss is: 843.4420776367188\n",
            "MSE Loss is: 0.01606661081314087, h Loss is: 843.4191284179688, L1 loss: 1.9014228582382202, Total Loss is: 843.4351806640625\n",
            "MSE Loss is: 0.015998192131519318, h Loss is: 843.4486083984375, L1 loss: 1.9019533395767212, Total Loss is: 843.464599609375\n",
            "MSE Loss is: 0.016170457005500793, h Loss is: 843.1931762695312, L1 loss: 1.9023990631103516, Total Loss is: 843.2093505859375\n",
            "MSE Loss is: 0.016102734953165054, h Loss is: 842.9548950195312, L1 loss: 1.9028927087783813, Total Loss is: 842.9710083007812\n",
            "MSE Loss is: 0.01561671681702137, h Loss is: 842.616455078125, L1 loss: 1.9034080505371094, Total Loss is: 842.632080078125\n",
            "MSE Loss is: 0.016712898388504982, h Loss is: 841.9669799804688, L1 loss: 1.9039814472198486, Total Loss is: 841.9837036132812\n",
            "MSE Loss is: 0.016281457617878914, h Loss is: 841.6557006835938, L1 loss: 1.904670000076294, Total Loss is: 841.6719970703125\n",
            "MSE Loss is: 0.016046589240431786, h Loss is: 841.0647583007812, L1 loss: 1.9053022861480713, Total Loss is: 841.080810546875\n",
            "MSE Loss is: 0.016402756795287132, h Loss is: 840.5843505859375, L1 loss: 1.905892252922058, Total Loss is: 840.6007690429688\n",
            "MSE Loss is: 0.015618212521076202, h Loss is: 840.1285400390625, L1 loss: 1.9064579010009766, Total Loss is: 840.1441650390625\n",
            "MSE Loss is: 0.016572609543800354, h Loss is: 839.6873779296875, L1 loss: 1.9069252014160156, Total Loss is: 839.7039794921875\n",
            "MSE Loss is: 0.016034536063671112, h Loss is: 839.02294921875, L1 loss: 1.9072083234786987, Total Loss is: 839.0390014648438\n",
            "MSE Loss is: 0.016712429001927376, h Loss is: 838.2591552734375, L1 loss: 1.9074528217315674, Total Loss is: 838.27587890625\n",
            "MSE Loss is: 0.016335714608430862, h Loss is: 837.6162109375, L1 loss: 1.9077919721603394, Total Loss is: 837.632568359375\n",
            "MSE Loss is: 0.016728613525629044, h Loss is: 837.2627563476562, L1 loss: 1.9082905054092407, Total Loss is: 837.2794799804688\n",
            "MSE Loss is: 0.01582719199359417, h Loss is: 837.2198486328125, L1 loss: 1.908964991569519, Total Loss is: 837.2356567382812\n",
            "MSE Loss is: 0.016325727105140686, h Loss is: 837.2979736328125, L1 loss: 1.9097031354904175, Total Loss is: 837.3142700195312\n",
            "MSE Loss is: 0.016166768968105316, h Loss is: 837.4165649414062, L1 loss: 1.910342812538147, Total Loss is: 837.4327392578125\n",
            "MSE Loss is: 0.01638377457857132, h Loss is: 837.4034423828125, L1 loss: 1.9108555316925049, Total Loss is: 837.4197998046875\n",
            "MSE Loss is: 0.016238799318671227, h Loss is: 837.0690307617188, L1 loss: 1.911227822303772, Total Loss is: 837.0852661132812\n",
            "MSE Loss is: 0.01588771492242813, h Loss is: 836.7611694335938, L1 loss: 1.9114996194839478, Total Loss is: 836.7770385742188\n",
            "MSE Loss is: 0.01612643525004387, h Loss is: 836.15380859375, L1 loss: 1.9117008447647095, Total Loss is: 836.169921875\n",
            "MSE Loss is: 0.01627027429640293, h Loss is: 835.4229125976562, L1 loss: 1.9119504690170288, Total Loss is: 835.439208984375\n",
            "MSE Loss is: 0.01634754240512848, h Loss is: 834.952392578125, L1 loss: 1.9124011993408203, Total Loss is: 834.96875\n",
            "MSE Loss is: 0.01635109633207321, h Loss is: 835.0396728515625, L1 loss: 1.9130058288574219, Total Loss is: 835.0560302734375\n",
            "MSE Loss is: 0.016583964228630066, h Loss is: 835.02001953125, L1 loss: 1.913560152053833, Total Loss is: 835.03662109375\n",
            "MSE Loss is: 0.016196830198168755, h Loss is: 834.5679931640625, L1 loss: 1.914006233215332, Total Loss is: 834.5841674804688\n",
            "MSE Loss is: 0.016159672290086746, h Loss is: 833.9512939453125, L1 loss: 1.914406657218933, Total Loss is: 833.9674682617188\n",
            "MSE Loss is: 0.016773449257016182, h Loss is: 833.4351806640625, L1 loss: 1.9147357940673828, Total Loss is: 833.4519653320312\n",
            "MSE Loss is: 0.01610761508345604, h Loss is: 832.8095703125, L1 loss: 1.914936900138855, Total Loss is: 832.82568359375\n",
            "MSE Loss is: 0.017051134258508682, h Loss is: 832.748046875, L1 loss: 1.9152535200119019, Total Loss is: 832.7650756835938\n",
            "MSE Loss is: 0.01597505994141102, h Loss is: 833.0159912109375, L1 loss: 1.9156688451766968, Total Loss is: 833.031982421875\n",
            "MSE Loss is: 0.016195472329854965, h Loss is: 833.525634765625, L1 loss: 1.9162510633468628, Total Loss is: 833.5418090820312\n",
            "MSE Loss is: 0.016468113288283348, h Loss is: 833.7529907226562, L1 loss: 1.9168188571929932, Total Loss is: 833.7694702148438\n",
            "MSE Loss is: 0.016556570306420326, h Loss is: 833.3909912109375, L1 loss: 1.9172550439834595, Total Loss is: 833.4075317382812\n",
            "MSE Loss is: 0.0156202157959342, h Loss is: 832.2459106445312, L1 loss: 1.9175119400024414, Total Loss is: 832.2615356445312\n",
            "MSE Loss is: 0.015547782182693481, h Loss is: 831.024658203125, L1 loss: 1.9177402257919312, Total Loss is: 831.0402221679688\n",
            "MSE Loss is: 0.0164390429854393, h Loss is: 830.2596435546875, L1 loss: 1.9180233478546143, Total Loss is: 830.2760620117188\n",
            "MSE Loss is: 0.01618174836039543, h Loss is: 830.277099609375, L1 loss: 1.9184372425079346, Total Loss is: 830.2932739257812\n",
            "MSE Loss is: 0.015990400686860085, h Loss is: 830.9481811523438, L1 loss: 1.9189023971557617, Total Loss is: 830.9641723632812\n",
            "MSE Loss is: 0.01579812541604042, h Loss is: 831.482421875, L1 loss: 1.9194687604904175, Total Loss is: 831.4982299804688\n",
            "MSE Loss is: 0.016315514221787453, h Loss is: 831.6868286132812, L1 loss: 1.919921875, Total Loss is: 831.703125\n",
            "New h_val is : tf.Tensor(6.6399174, shape=(), dtype=float32)\n",
            "Epoch: {} 15\n",
            "MSE Loss is: 0.015766065567731857, h Loss is: 1006.1453857421875, L1 loss: 1.9203087091445923, Total Loss is: 1006.1611328125\n",
            "MSE Loss is: 0.01610773615539074, h Loss is: 1005.2457885742188, L1 loss: 1.9206775426864624, Total Loss is: 1005.2619018554688\n",
            "MSE Loss is: 0.01606731489300728, h Loss is: 1004.2069091796875, L1 loss: 1.9210518598556519, Total Loss is: 1004.2229614257812\n",
            "MSE Loss is: 0.015805289149284363, h Loss is: 1003.2994995117188, L1 loss: 1.9214380979537964, Total Loss is: 1003.3153076171875\n",
            "MSE Loss is: 0.01622890681028366, h Loss is: 1002.9141235351562, L1 loss: 1.9219120740890503, Total Loss is: 1002.9303588867188\n",
            "MSE Loss is: 0.016325263306498528, h Loss is: 1002.5767822265625, L1 loss: 1.9224655628204346, Total Loss is: 1002.5930786132812\n",
            "MSE Loss is: 0.016034964472055435, h Loss is: 1002.4708251953125, L1 loss: 1.9230495691299438, Total Loss is: 1002.4868774414062\n",
            "MSE Loss is: 0.01592395454645157, h Loss is: 1002.36474609375, L1 loss: 1.9235479831695557, Total Loss is: 1002.3806762695312\n",
            "MSE Loss is: 0.015849104151129723, h Loss is: 1002.052978515625, L1 loss: 1.924051284790039, Total Loss is: 1002.06884765625\n",
            "MSE Loss is: 0.016032611951231956, h Loss is: 1001.2081298828125, L1 loss: 1.9244511127471924, Total Loss is: 1001.2241821289062\n",
            "MSE Loss is: 0.015977881848812103, h Loss is: 1000.3823852539062, L1 loss: 1.924896240234375, Total Loss is: 1000.3983764648438\n",
            "MSE Loss is: 0.015480877831578255, h Loss is: 999.5843505859375, L1 loss: 1.9253685474395752, Total Loss is: 999.599853515625\n",
            "MSE Loss is: 0.01657041534781456, h Loss is: 998.609130859375, L1 loss: 1.9259092807769775, Total Loss is: 998.6256713867188\n",
            "MSE Loss is: 0.016136297956109047, h Loss is: 998.2682495117188, L1 loss: 1.9265663623809814, Total Loss is: 998.2843627929688\n",
            "MSE Loss is: 0.015911009162664413, h Loss is: 997.6326293945312, L1 loss: 1.9271477460861206, Total Loss is: 997.6485595703125\n",
            "MSE Loss is: 0.016261346638202667, h Loss is: 997.053955078125, L1 loss: 1.9276660680770874, Total Loss is: 997.0701904296875\n",
            "MSE Loss is: 0.015516787767410278, h Loss is: 996.3412475585938, L1 loss: 1.9281574487686157, Total Loss is: 996.3567504882812\n",
            "MSE Loss is: 0.016438394784927368, h Loss is: 995.4866333007812, L1 loss: 1.9285529851913452, Total Loss is: 995.5030517578125\n",
            "MSE Loss is: 0.015915878117084503, h Loss is: 994.260986328125, L1 loss: 1.9287807941436768, Total Loss is: 994.2769165039062\n",
            "MSE Loss is: 0.016589675098657608, h Loss is: 992.95849609375, L1 loss: 1.9289989471435547, Total Loss is: 992.97509765625\n",
            "MSE Loss is: 0.01617848314344883, h Loss is: 991.970703125, L1 loss: 1.9293292760849, Total Loss is: 991.9868774414062\n",
            "MSE Loss is: 0.016592983156442642, h Loss is: 991.5123901367188, L1 loss: 1.9298219680786133, Total Loss is: 991.5289916992188\n",
            "MSE Loss is: 0.015710653737187386, h Loss is: 991.5050659179688, L1 loss: 1.9304689168930054, Total Loss is: 991.520751953125\n",
            "MSE Loss is: 0.01618155464529991, h Loss is: 991.5958251953125, L1 loss: 1.9311332702636719, Total Loss is: 991.6119995117188\n",
            "MSE Loss is: 0.016050156205892563, h Loss is: 991.610595703125, L1 loss: 1.9316519498825073, Total Loss is: 991.6266479492188\n",
            "MSE Loss is: 0.01626596227288246, h Loss is: 991.323486328125, L1 loss: 1.9320499897003174, Total Loss is: 991.3397827148438\n",
            "MSE Loss is: 0.016131553798913956, h Loss is: 990.5968017578125, L1 loss: 1.9323608875274658, Total Loss is: 990.6129150390625\n",
            "MSE Loss is: 0.01576753333210945, h Loss is: 989.95458984375, L1 loss: 1.932633638381958, Total Loss is: 989.9703369140625\n",
            "MSE Loss is: 0.016020115464925766, h Loss is: 989.0455932617188, L1 loss: 1.9328701496124268, Total Loss is: 989.0615844726562\n",
            "MSE Loss is: 0.01616010256111622, h Loss is: 988.0989379882812, L1 loss: 1.9331735372543335, Total Loss is: 988.1151123046875\n",
            "MSE Loss is: 0.016214216127991676, h Loss is: 987.5487060546875, L1 loss: 1.9337036609649658, Total Loss is: 987.56494140625\n",
            "MSE Loss is: 0.01622544787824154, h Loss is: 987.6851806640625, L1 loss: 1.934377908706665, Total Loss is: 987.701416015625\n",
            "MSE Loss is: 0.016478154808282852, h Loss is: 987.5640258789062, L1 loss: 1.9348983764648438, Total Loss is: 987.5805053710938\n",
            "MSE Loss is: 0.016082746908068657, h Loss is: 986.79150390625, L1 loss: 1.935238242149353, Total Loss is: 986.8075561523438\n",
            "MSE Loss is: 0.016046758741140366, h Loss is: 985.7791748046875, L1 loss: 1.935522437095642, Total Loss is: 985.7952270507812\n",
            "MSE Loss is: 0.01665644720196724, h Loss is: 984.9800415039062, L1 loss: 1.9358152151107788, Total Loss is: 984.9967041015625\n",
            "MSE Loss is: 0.016004759818315506, h Loss is: 984.1558227539062, L1 loss: 1.9360523223876953, Total Loss is: 984.1718139648438\n",
            "MSE Loss is: 0.016925815492868423, h Loss is: 984.1221313476562, L1 loss: 1.9365142583847046, Total Loss is: 984.1390380859375\n",
            "MSE Loss is: 0.0158565454185009, h Loss is: 984.485595703125, L1 loss: 1.9371074438095093, Total Loss is: 984.50146484375\n",
            "MSE Loss is: 0.01609533280134201, h Loss is: 985.0438232421875, L1 loss: 1.9378429651260376, Total Loss is: 985.0599365234375\n",
            "MSE Loss is: 0.0163666270673275, h Loss is: 985.0753784179688, L1 loss: 1.9384288787841797, Total Loss is: 985.0917358398438\n",
            "MSE Loss is: 0.016437921673059464, h Loss is: 984.2681884765625, L1 loss: 1.938747763633728, Total Loss is: 984.2846069335938\n",
            "MSE Loss is: 0.01552126556634903, h Loss is: 982.55859375, L1 loss: 1.9388151168823242, Total Loss is: 982.5740966796875\n",
            "MSE Loss is: 0.015441320836544037, h Loss is: 980.9608154296875, L1 loss: 1.938891053199768, Total Loss is: 980.9762573242188\n",
            "MSE Loss is: 0.0163275059312582, h Loss is: 980.1784057617188, L1 loss: 1.9391685724258423, Total Loss is: 980.1947631835938\n",
            "MSE Loss is: 0.016084767878055573, h Loss is: 980.4881591796875, L1 loss: 1.9397343397140503, Total Loss is: 980.5042724609375\n",
            "MSE Loss is: 0.0158831849694252, h Loss is: 981.503173828125, L1 loss: 1.9404163360595703, Total Loss is: 981.51904296875\n",
            "MSE Loss is: 0.01571730524301529, h Loss is: 982.0432739257812, L1 loss: 1.9411213397979736, Total Loss is: 982.0590209960938\n",
            "MSE Loss is: 0.016209563240408897, h Loss is: 981.8905029296875, L1 loss: 1.9415905475616455, Total Loss is: 981.90673828125\n",
            "New h_val is : tf.Tensor(6.484686, shape=(), dtype=float32)\n",
            "Epoch: {} 16\n",
            "MSE Loss is: 0.015655100345611572, h Loss is: 1164.3359375, L1 loss: 1.94189453125, Total Loss is: 1164.3515625\n",
            "MSE Loss is: 0.016017558053135872, h Loss is: 1162.837158203125, L1 loss: 1.9421627521514893, Total Loss is: 1162.8531494140625\n",
            "MSE Loss is: 0.01597021147608757, h Loss is: 1161.5301513671875, L1 loss: 1.9424861669540405, Total Loss is: 1161.546142578125\n",
            "MSE Loss is: 0.015712685883045197, h Loss is: 1160.7109375, L1 loss: 1.9429035186767578, Total Loss is: 1160.7266845703125\n",
            "MSE Loss is: 0.01612340658903122, h Loss is: 1160.6595458984375, L1 loss: 1.943493127822876, Total Loss is: 1160.6756591796875\n",
            "MSE Loss is: 0.016225259751081467, h Loss is: 1160.50244140625, L1 loss: 1.9441627264022827, Total Loss is: 1160.5186767578125\n",
            "MSE Loss is: 0.015944454818964005, h Loss is: 1160.29150390625, L1 loss: 1.9448350667953491, Total Loss is: 1160.3074951171875\n",
            "MSE Loss is: 0.01582665927708149, h Loss is: 1159.7572021484375, L1 loss: 1.9453643560409546, Total Loss is: 1159.7730712890625\n",
            "MSE Loss is: 0.015744812786579132, h Loss is: 1158.83837890625, L1 loss: 1.9458494186401367, Total Loss is: 1158.8541259765625\n",
            "MSE Loss is: 0.015943020582199097, h Loss is: 1157.40087890625, L1 loss: 1.9462156295776367, Total Loss is: 1157.4168701171875\n",
            "MSE Loss is: 0.015895429998636246, h Loss is: 1156.3040771484375, L1 loss: 1.946681261062622, Total Loss is: 1156.3199462890625\n",
            "MSE Loss is: 0.015387197956442833, h Loss is: 1155.554931640625, L1 loss: 1.947230577468872, Total Loss is: 1155.5703125\n",
            "MSE Loss is: 0.016472268849611282, h Loss is: 1154.7208251953125, L1 loss: 1.9478511810302734, Total Loss is: 1154.7373046875\n",
            "MSE Loss is: 0.01603626273572445, h Loss is: 1154.5826416015625, L1 loss: 1.948616862297058, Total Loss is: 1154.5986328125\n",
            "MSE Loss is: 0.015821252018213272, h Loss is: 1153.80419921875, L1 loss: 1.949233889579773, Total Loss is: 1153.820068359375\n",
            "MSE Loss is: 0.01616325043141842, h Loss is: 1152.8138427734375, L1 loss: 1.949733018875122, Total Loss is: 1152.8299560546875\n",
            "MSE Loss is: 0.01544630154967308, h Loss is: 1151.5338134765625, L1 loss: 1.9501800537109375, Total Loss is: 1151.54931640625\n",
            "MSE Loss is: 0.01634407788515091, h Loss is: 1150.1688232421875, L1 loss: 1.9505480527877808, Total Loss is: 1150.1851806640625\n",
            "MSE Loss is: 0.015837710350751877, h Loss is: 1148.580810546875, L1 loss: 1.9507664442062378, Total Loss is: 1148.5966796875\n",
            "MSE Loss is: 0.01650914177298546, h Loss is: 1147.16796875, L1 loss: 1.951022744178772, Total Loss is: 1147.1844482421875\n",
            "MSE Loss is: 0.016069035977125168, h Loss is: 1146.29541015625, L1 loss: 1.9514429569244385, Total Loss is: 1146.3115234375\n",
            "MSE Loss is: 0.016495320945978165, h Loss is: 1146.0225830078125, L1 loss: 1.952033281326294, Total Loss is: 1146.0390625\n",
            "MSE Loss is: 0.01563349738717079, h Loss is: 1146.0604248046875, L1 loss: 1.9527370929718018, Total Loss is: 1146.0760498046875\n",
            "MSE Loss is: 0.016083599999547005, h Loss is: 1145.950439453125, L1 loss: 1.9533913135528564, Total Loss is: 1145.966552734375\n",
            "MSE Loss is: 0.015973100438714027, h Loss is: 1145.5906982421875, L1 loss: 1.9538425207138062, Total Loss is: 1145.606689453125\n",
            "MSE Loss is: 0.016182592138648033, h Loss is: 1144.9027099609375, L1 loss: 1.954146385192871, Total Loss is: 1144.9189453125\n",
            "MSE Loss is: 0.016059786081314087, h Loss is: 1143.9071044921875, L1 loss: 1.954388976097107, Total Loss is: 1143.9232177734375\n",
            "MSE Loss is: 0.015687089413404465, h Loss is: 1143.27099609375, L1 loss: 1.9546623229980469, Total Loss is: 1143.2867431640625\n",
            "MSE Loss is: 0.015949588268995285, h Loss is: 1142.451904296875, L1 loss: 1.9549320936203003, Total Loss is: 1142.4678955078125\n",
            "MSE Loss is: 0.01608682982623577, h Loss is: 1141.5555419921875, L1 loss: 1.9552547931671143, Total Loss is: 1141.5716552734375\n",
            "MSE Loss is: 0.01612180843949318, h Loss is: 1140.9781494140625, L1 loss: 1.9557520151138306, Total Loss is: 1140.9942626953125\n",
            "MSE Loss is: 0.016138192266225815, h Loss is: 1141.0343017578125, L1 loss: 1.9563273191452026, Total Loss is: 1141.0504150390625\n",
            "MSE Loss is: 0.016406910493969917, h Loss is: 1140.613037109375, L1 loss: 1.956702470779419, Total Loss is: 1140.62939453125\n",
            "MSE Loss is: 0.016004694625735283, h Loss is: 1139.4132080078125, L1 loss: 1.9569027423858643, Total Loss is: 1139.42919921875\n",
            "MSE Loss is: 0.01596781611442566, h Loss is: 1138.0994873046875, L1 loss: 1.9571136236190796, Total Loss is: 1138.115478515625\n",
            "MSE Loss is: 0.016576595604419708, h Loss is: 1137.294189453125, L1 loss: 1.9574066400527954, Total Loss is: 1137.310791015625\n",
            "MSE Loss is: 0.015936264768242836, h Loss is: 1136.59716796875, L1 loss: 1.9576835632324219, Total Loss is: 1136.6131591796875\n",
            "MSE Loss is: 0.016835983842611313, h Loss is: 1136.8226318359375, L1 loss: 1.958189845085144, Total Loss is: 1136.8394775390625\n",
            "MSE Loss is: 0.01577356457710266, h Loss is: 1137.321044921875, L1 loss: 1.9587726593017578, Total Loss is: 1137.3367919921875\n",
            "MSE Loss is: 0.016028650104999542, h Loss is: 1137.8067626953125, L1 loss: 1.9595069885253906, Total Loss is: 1137.82275390625\n",
            "MSE Loss is: 0.01629685051739216, h Loss is: 1137.4718017578125, L1 loss: 1.9599884748458862, Total Loss is: 1137.4881591796875\n",
            "MSE Loss is: 0.016350582242012024, h Loss is: 1136.1563720703125, L1 loss: 1.96014404296875, Total Loss is: 1136.1727294921875\n",
            "MSE Loss is: 0.015458380803465843, h Loss is: 1134.010009765625, L1 loss: 1.9600781202316284, Total Loss is: 1134.0255126953125\n",
            "MSE Loss is: 0.01536780595779419, h Loss is: 1132.3333740234375, L1 loss: 1.960123896598816, Total Loss is: 1132.3487548828125\n",
            "MSE Loss is: 0.016250696033239365, h Loss is: 1131.859619140625, L1 loss: 1.9605220556259155, Total Loss is: 1131.8758544921875\n",
            "MSE Loss is: 0.016018103808164597, h Loss is: 1132.654541015625, L1 loss: 1.9612960815429688, Total Loss is: 1132.6705322265625\n",
            "MSE Loss is: 0.015810277312994003, h Loss is: 1133.97705078125, L1 loss: 1.9621294736862183, Total Loss is: 1133.992919921875\n",
            "MSE Loss is: 0.015665823593735695, h Loss is: 1134.30615234375, L1 loss: 1.9628181457519531, Total Loss is: 1134.32177734375\n",
            "MSE Loss is: 0.01613118126988411, h Loss is: 1133.591552734375, L1 loss: 1.9631454944610596, Total Loss is: 1133.607666015625\n",
            "New h_val is : tf.Tensor(6.315609, shape=(), dtype=float32)\n",
            "Epoch: {} 17\n",
            "MSE Loss is: 0.01557871326804161, h Loss is: 1323.33642578125, L1 loss: 1.9632705450057983, Total Loss is: 1323.35205078125\n",
            "MSE Loss is: 0.015957513824105263, h Loss is: 1321.526611328125, L1 loss: 1.9634666442871094, Total Loss is: 1321.5426025390625\n",
            "MSE Loss is: 0.015905294567346573, h Loss is: 1320.439208984375, L1 loss: 1.9638617038726807, Total Loss is: 1320.455078125\n",
            "MSE Loss is: 0.015648316591978073, h Loss is: 1320.144287109375, L1 loss: 1.9644415378570557, Total Loss is: 1320.159912109375\n",
            "MSE Loss is: 0.016050174832344055, h Loss is: 1320.575927734375, L1 loss: 1.9652026891708374, Total Loss is: 1320.5919189453125\n",
            "MSE Loss is: 0.016156703233718872, h Loss is: 1320.398681640625, L1 loss: 1.9659265279769897, Total Loss is: 1320.414794921875\n",
            "MSE Loss is: 0.01588304340839386, h Loss is: 1319.70263671875, L1 loss: 1.9665707349777222, Total Loss is: 1319.718505859375\n",
            "MSE Loss is: 0.015759028494358063, h Loss is: 1318.46240234375, L1 loss: 1.9670352935791016, Total Loss is: 1318.4781494140625\n",
            "MSE Loss is: 0.015670202672481537, h Loss is: 1316.9837646484375, L1 loss: 1.9674290418624878, Total Loss is: 1316.9993896484375\n",
            "MSE Loss is: 0.01588628441095352, h Loss is: 1315.34716796875, L1 loss: 1.9677547216415405, Total Loss is: 1315.363037109375\n",
            "MSE Loss is: 0.015840798616409302, h Loss is: 1314.5311279296875, L1 loss: 1.9683382511138916, Total Loss is: 1314.5469970703125\n",
            "MSE Loss is: 0.015321949496865273, h Loss is: 1314.2449951171875, L1 loss: 1.9691303968429565, Total Loss is: 1314.2603759765625\n",
            "MSE Loss is: 0.016403116285800934, h Loss is: 1313.606201171875, L1 loss: 1.9700268507003784, Total Loss is: 1313.62255859375\n",
            "MSE Loss is: 0.01596863940358162, h Loss is: 1313.3946533203125, L1 loss: 1.9710149765014648, Total Loss is: 1313.41064453125\n",
            "MSE Loss is: 0.01576201617717743, h Loss is: 1312.056640625, L1 loss: 1.9716814756393433, Total Loss is: 1312.0723876953125\n",
            "MSE Loss is: 0.01609359309077263, h Loss is: 1310.3984375, L1 loss: 1.9721225500106812, Total Loss is: 1310.41455078125\n",
            "MSE Loss is: 0.015395613387227058, h Loss is: 1308.6456298828125, L1 loss: 1.9725335836410522, Total Loss is: 1308.6610107421875\n",
            "MSE Loss is: 0.01627880334854126, h Loss is: 1307.1973876953125, L1 loss: 1.9729126691818237, Total Loss is: 1307.213623046875\n",
            "MSE Loss is: 0.01578642800450325, h Loss is: 1305.77978515625, L1 loss: 1.9732431173324585, Total Loss is: 1305.7955322265625\n",
            "MSE Loss is: 0.016453905031085014, h Loss is: 1304.6119384765625, L1 loss: 1.973662257194519, Total Loss is: 1304.62841796875\n",
            "MSE Loss is: 0.01599167287349701, h Loss is: 1303.8736572265625, L1 loss: 1.9742149114608765, Total Loss is: 1303.8896484375\n",
            "MSE Loss is: 0.01642528548836708, h Loss is: 1303.493896484375, L1 loss: 1.9748525619506836, Total Loss is: 1303.5103759765625\n",
            "MSE Loss is: 0.015582378022372723, h Loss is: 1303.1973876953125, L1 loss: 1.9755992889404297, Total Loss is: 1303.2130126953125\n",
            "MSE Loss is: 0.01601560413837433, h Loss is: 1302.681396484375, L1 loss: 1.9762295484542847, Total Loss is: 1302.6973876953125\n",
            "MSE Loss is: 0.015921030193567276, h Loss is: 1302.083740234375, L1 loss: 1.9766441583633423, Total Loss is: 1302.099609375\n",
            "MSE Loss is: 0.016123982146382332, h Loss is: 1301.4158935546875, L1 loss: 1.9769617319107056, Total Loss is: 1301.4320068359375\n",
            "MSE Loss is: 0.01601131074130535, h Loss is: 1300.6407470703125, L1 loss: 1.9772720336914062, Total Loss is: 1300.65673828125\n",
            "MSE Loss is: 0.01563197560608387, h Loss is: 1300.3157958984375, L1 loss: 1.977746605873108, Total Loss is: 1300.3314208984375\n",
            "MSE Loss is: 0.015901029109954834, h Loss is: 1299.5679931640625, L1 loss: 1.97820246219635, Total Loss is: 1299.5838623046875\n",
            "MSE Loss is: 0.01603848859667778, h Loss is: 1298.432861328125, L1 loss: 1.9786207675933838, Total Loss is: 1298.4488525390625\n",
            "MSE Loss is: 0.016058195382356644, h Loss is: 1297.4942626953125, L1 loss: 1.979182481765747, Total Loss is: 1297.5103759765625\n",
            "MSE Loss is: 0.01607636921107769, h Loss is: 1297.3291015625, L1 loss: 1.9797931909561157, Total Loss is: 1297.34521484375\n",
            "MSE Loss is: 0.016357574611902237, h Loss is: 1296.7679443359375, L1 loss: 1.9801616668701172, Total Loss is: 1296.7843017578125\n",
            "MSE Loss is: 0.0159517303109169, h Loss is: 1295.52099609375, L1 loss: 1.9803520441055298, Total Loss is: 1295.5369873046875\n",
            "MSE Loss is: 0.015912506729364395, h Loss is: 1294.3189697265625, L1 loss: 1.9806365966796875, Total Loss is: 1294.3348388671875\n",
            "MSE Loss is: 0.016521397978067398, h Loss is: 1293.745361328125, L1 loss: 1.9810596704483032, Total Loss is: 1293.7618408203125\n",
            "MSE Loss is: 0.015888947993516922, h Loss is: 1293.130126953125, L1 loss: 1.9814924001693726, Total Loss is: 1293.14599609375\n",
            "MSE Loss is: 0.016771692782640457, h Loss is: 1293.362548828125, L1 loss: 1.9821850061416626, Total Loss is: 1293.3792724609375\n",
            "MSE Loss is: 0.015715816989541054, h Loss is: 1293.724853515625, L1 loss: 1.9829071760177612, Total Loss is: 1293.7406005859375\n",
            "MSE Loss is: 0.01598324626684189, h Loss is: 1294.03857421875, L1 loss: 1.9836891889572144, Total Loss is: 1294.0545654296875\n",
            "MSE Loss is: 0.016247624531388283, h Loss is: 1293.4996337890625, L1 loss: 1.9841406345367432, Total Loss is: 1293.515869140625\n",
            "MSE Loss is: 0.016286522150039673, h Loss is: 1292.0244140625, L1 loss: 1.9842827320098877, Total Loss is: 1292.0406494140625\n",
            "MSE Loss is: 0.015418103896081448, h Loss is: 1289.804443359375, L1 loss: 1.9842445850372314, Total Loss is: 1289.81982421875\n",
            "MSE Loss is: 0.015313880518078804, h Loss is: 1288.251220703125, L1 loss: 1.9843800067901611, Total Loss is: 1288.2664794921875\n",
            "MSE Loss is: 0.016197146847844124, h Loss is: 1288.0560302734375, L1 loss: 1.9849258661270142, Total Loss is: 1288.072265625\n",
            "MSE Loss is: 0.015973690897226334, h Loss is: 1289.117431640625, L1 loss: 1.9858407974243164, Total Loss is: 1289.1334228515625\n",
            "MSE Loss is: 0.01575976237654686, h Loss is: 1290.4901123046875, L1 loss: 1.9867054224014282, Total Loss is: 1290.505859375\n",
            "MSE Loss is: 0.015629103407263756, h Loss is: 1290.490966796875, L1 loss: 1.987334132194519, Total Loss is: 1290.506591796875\n",
            "MSE Loss is: 0.0160712581127882, h Loss is: 1289.357421875, L1 loss: 1.9875128269195557, Total Loss is: 1289.37353515625\n",
            "New h_val is : tf.Tensor(6.153858, shape=(), dtype=float32)\n",
            "Epoch: {} 18\n",
            "MSE Loss is: 0.015525966882705688, h Loss is: 1487.2479248046875, L1 loss: 1.987548828125, Total Loss is: 1487.263427734375\n",
            "MSE Loss is: 0.01591615006327629, h Loss is: 1485.5753173828125, L1 loss: 1.987831711769104, Total Loss is: 1485.5911865234375\n",
            "MSE Loss is: 0.015859778970479965, h Loss is: 1485.0015869140625, L1 loss: 1.988402247428894, Total Loss is: 1485.0174560546875\n",
            "MSE Loss is: 0.015601377002894878, h Loss is: 1485.2098388671875, L1 loss: 1.9891632795333862, Total Loss is: 1485.2254638671875\n",
            "MSE Loss is: 0.015998471528291702, h Loss is: 1485.8262939453125, L1 loss: 1.990045189857483, Total Loss is: 1485.84228515625\n",
            "MSE Loss is: 0.016109395772218704, h Loss is: 1485.2520751953125, L1 loss: 1.9907954931259155, Total Loss is: 1485.2681884765625\n",
            "MSE Loss is: 0.01583801582455635, h Loss is: 1483.8931884765625, L1 loss: 1.9913688898086548, Total Loss is: 1483.9090576171875\n",
            "MSE Loss is: 0.015710704028606415, h Loss is: 1482.134521484375, L1 loss: 1.9917166233062744, Total Loss is: 1482.1502685546875\n",
            "MSE Loss is: 0.015616754069924355, h Loss is: 1480.566162109375, L1 loss: 1.9921327829360962, Total Loss is: 1480.581787109375\n",
            "MSE Loss is: 0.015848930925130844, h Loss is: 1479.2269287109375, L1 loss: 1.9926553964614868, Total Loss is: 1479.2427978515625\n",
            "MSE Loss is: 0.01580251380801201, h Loss is: 1478.9449462890625, L1 loss: 1.993503212928772, Total Loss is: 1478.960693359375\n",
            "MSE Loss is: 0.015275374054908752, h Loss is: 1478.9931640625, L1 loss: 1.9944508075714111, Total Loss is: 1479.0084228515625\n",
            "MSE Loss is: 0.016355453059077263, h Loss is: 1478.136474609375, L1 loss: 1.995309829711914, Total Loss is: 1478.15283203125\n",
            "MSE Loss is: 0.015920722857117653, h Loss is: 1477.4573974609375, L1 loss: 1.9961460828781128, Total Loss is: 1477.4732666015625\n",
            "MSE Loss is: 0.015721337869763374, h Loss is: 1475.44677734375, L1 loss: 1.9966312646865845, Total Loss is: 1475.4625244140625\n",
            "MSE Loss is: 0.016044169664382935, h Loss is: 1473.400634765625, L1 loss: 1.9969723224639893, Total Loss is: 1473.4166259765625\n",
            "MSE Loss is: 0.015358993783593178, h Loss is: 1471.697265625, L1 loss: 1.9973804950714111, Total Loss is: 1471.712646484375\n",
            "MSE Loss is: 0.01623377576470375, h Loss is: 1470.6292724609375, L1 loss: 1.9978876113891602, Total Loss is: 1470.6455078125\n",
            "MSE Loss is: 0.015751462429761887, h Loss is: 1469.51953125, L1 loss: 1.9983917474746704, Total Loss is: 1469.5352783203125\n",
            "MSE Loss is: 0.016415704041719437, h Loss is: 1468.343994140625, L1 loss: 1.9988950490951538, Total Loss is: 1468.3603515625\n",
            "MSE Loss is: 0.015938108786940575, h Loss is: 1467.298828125, L1 loss: 1.999444603919983, Total Loss is: 1467.3148193359375\n",
            "MSE Loss is: 0.016374986618757248, h Loss is: 1466.482666015625, L1 loss: 1.999985933303833, Total Loss is: 1466.4990234375\n",
            "MSE Loss is: 0.0155467689037323, h Loss is: 1465.86669921875, L1 loss: 2.0005877017974854, Total Loss is: 1465.8822021484375\n",
            "MSE Loss is: 0.015967942774295807, h Loss is: 1465.31005859375, L1 loss: 2.0011143684387207, Total Loss is: 1465.3260498046875\n",
            "MSE Loss is: 0.015886301174759865, h Loss is: 1464.9925537109375, L1 loss: 2.0015358924865723, Total Loss is: 1465.0084228515625\n",
            "MSE Loss is: 0.016082745045423508, h Loss is: 1464.7130126953125, L1 loss: 2.001938581466675, Total Loss is: 1464.7291259765625\n",
            "MSE Loss is: 0.01597699709236622, h Loss is: 1464.1732177734375, L1 loss: 2.0024051666259766, Total Loss is: 1464.189208984375\n",
            "MSE Loss is: 0.015593206509947777, h Loss is: 1463.86572265625, L1 loss: 2.002913236618042, Total Loss is: 1463.88134765625\n",
            "MSE Loss is: 0.015866901725530624, h Loss is: 1462.7880859375, L1 loss: 2.003286838531494, Total Loss is: 1462.803955078125\n",
            "MSE Loss is: 0.016007238999009132, h Loss is: 1461.1864013671875, L1 loss: 2.0035560131073, Total Loss is: 1461.202392578125\n",
            "MSE Loss is: 0.016013702377676964, h Loss is: 1460.001708984375, L1 loss: 2.003990650177002, Total Loss is: 1460.0177001953125\n",
            "MSE Loss is: 0.016032781451940536, h Loss is: 1460.026123046875, L1 loss: 2.0045318603515625, Total Loss is: 1460.0421142578125\n",
            "MSE Loss is: 0.016325203701853752, h Loss is: 1459.7755126953125, L1 loss: 2.004925489425659, Total Loss is: 1459.7918701171875\n",
            "MSE Loss is: 0.015915466472506523, h Loss is: 1458.7135009765625, L1 loss: 2.0052335262298584, Total Loss is: 1458.7293701171875\n",
            "MSE Loss is: 0.015873240306973457, h Loss is: 1457.52587890625, L1 loss: 2.005615711212158, Total Loss is: 1457.541748046875\n",
            "MSE Loss is: 0.016482505947351456, h Loss is: 1456.872802734375, L1 loss: 2.006057024002075, Total Loss is: 1456.8892822265625\n",
            "MSE Loss is: 0.015857547521591187, h Loss is: 1456.025146484375, L1 loss: 2.006413221359253, Total Loss is: 1456.041015625\n",
            "MSE Loss is: 0.016725877299904823, h Loss is: 1456.1827392578125, L1 loss: 2.006965398788452, Total Loss is: 1456.199462890625\n",
            "MSE Loss is: 0.01567472144961357, h Loss is: 1456.61279296875, L1 loss: 2.0075442790985107, Total Loss is: 1456.62841796875\n",
            "MSE Loss is: 0.01595286652445793, h Loss is: 1457.1396484375, L1 loss: 2.0081980228424072, Total Loss is: 1457.1556396484375\n",
            "MSE Loss is: 0.016214720904827118, h Loss is: 1456.727294921875, L1 loss: 2.00862455368042, Total Loss is: 1456.7435302734375\n",
            "MSE Loss is: 0.01624046079814434, h Loss is: 1455.200927734375, L1 loss: 2.0088038444519043, Total Loss is: 1455.2171630859375\n",
            "MSE Loss is: 0.01539081335067749, h Loss is: 1452.7659912109375, L1 loss: 2.0088231563568115, Total Loss is: 1452.7813720703125\n",
            "MSE Loss is: 0.01527479663491249, h Loss is: 1451.07958984375, L1 loss: 2.0090034008026123, Total Loss is: 1451.0948486328125\n",
            "MSE Loss is: 0.016162201762199402, h Loss is: 1450.9794921875, L1 loss: 2.009504795074463, Total Loss is: 1450.99560546875\n",
            "MSE Loss is: 0.015944428741931915, h Loss is: 1452.3245849609375, L1 loss: 2.0102853775024414, Total Loss is: 1452.340576171875\n",
            "MSE Loss is: 0.015724031254649162, h Loss is: 1453.953369140625, L1 loss: 2.010988712310791, Total Loss is: 1453.9691162109375\n",
            "MSE Loss is: 0.015603053383529186, h Loss is: 1453.895263671875, L1 loss: 2.011530637741089, Total Loss is: 1453.910888671875\n",
            "MSE Loss is: 0.016026882454752922, h Loss is: 1452.551513671875, L1 loss: 2.0116794109344482, Total Loss is: 1452.5675048828125\n",
            "New h_val is : tf.Tensor(6.0091667, shape=(), dtype=float32)\n",
            "Epoch: {} 19\n",
            "MSE Loss is: 0.015488353557884693, h Loss is: 1659.950927734375, L1 loss: 2.0117557048797607, Total Loss is: 1659.9664306640625\n",
            "MSE Loss is: 0.015886111184954643, h Loss is: 1658.34521484375, L1 loss: 2.0120232105255127, Total Loss is: 1658.361083984375\n",
            "MSE Loss is: 0.0158278439193964, h Loss is: 1658.1103515625, L1 loss: 2.0126006603240967, Total Loss is: 1658.126220703125\n",
            "MSE Loss is: 0.015567818656563759, h Loss is: 1658.6605224609375, L1 loss: 2.013341188430786, Total Loss is: 1658.6761474609375\n",
            "MSE Loss is: 0.015960920602083206, h Loss is: 1659.4058837890625, L1 loss: 2.014148235321045, Total Loss is: 1659.421875\n",
            "MSE Loss is: 0.01607613079249859, h Loss is: 1658.5316162109375, L1 loss: 2.0147738456726074, Total Loss is: 1658.5477294921875\n",
            "MSE Loss is: 0.015804678201675415, h Loss is: 1656.728515625, L1 loss: 2.0152130126953125, Total Loss is: 1656.7442626953125\n",
            "MSE Loss is: 0.015676263719797134, h Loss is: 1654.6998291015625, L1 loss: 2.0154612064361572, Total Loss is: 1654.7154541015625\n",
            "MSE Loss is: 0.015577579848468304, h Loss is: 1653.20947265625, L1 loss: 2.01585054397583, Total Loss is: 1653.22509765625\n",
            "MSE Loss is: 0.01582321897149086, h Loss is: 1652.16552734375, L1 loss: 2.016399383544922, Total Loss is: 1652.181396484375\n",
            "MSE Loss is: 0.015775874257087708, h Loss is: 1652.26611328125, L1 loss: 2.017277956008911, Total Loss is: 1652.2818603515625\n",
            "MSE Loss is: 0.015241589397192001, h Loss is: 1652.436767578125, L1 loss: 2.0181949138641357, Total Loss is: 1652.4520263671875\n",
            "MSE Loss is: 0.016322188079357147, h Loss is: 1651.2281494140625, L1 loss: 2.0189430713653564, Total Loss is: 1651.2445068359375\n",
            "MSE Loss is: 0.015885721892118454, h Loss is: 1650.144775390625, L1 loss: 2.019617795944214, Total Loss is: 1650.16064453125\n",
            "MSE Loss is: 0.015693502500653267, h Loss is: 1647.6976318359375, L1 loss: 2.019949197769165, Total Loss is: 1647.71337890625\n",
            "MSE Loss is: 0.01600908488035202, h Loss is: 1645.56103515625, L1 loss: 2.020209312438965, Total Loss is: 1645.5770263671875\n",
            "MSE Loss is: 0.015331422910094261, h Loss is: 1644.10009765625, L1 loss: 2.0206258296966553, Total Loss is: 1644.115478515625\n",
            "MSE Loss is: 0.01620214246213436, h Loss is: 1643.3912353515625, L1 loss: 2.021176815032959, Total Loss is: 1643.407470703125\n",
            "MSE Loss is: 0.015727534890174866, h Loss is: 1642.3575439453125, L1 loss: 2.0216894149780273, Total Loss is: 1642.373291015625\n",
            "MSE Loss is: 0.016389867290854454, h Loss is: 1640.8931884765625, L1 loss: 2.022115468978882, Total Loss is: 1640.9095458984375\n",
            "MSE Loss is: 0.015900852158665657, h Loss is: 1639.4088134765625, L1 loss: 2.022507429122925, Total Loss is: 1639.4246826171875\n",
            "MSE Loss is: 0.016338128596544266, h Loss is: 1638.2879638671875, L1 loss: 2.022869348526001, Total Loss is: 1638.3043212890625\n",
            "MSE Loss is: 0.015521768480539322, h Loss is: 1637.683349609375, L1 loss: 2.0233426094055176, Total Loss is: 1637.6988525390625\n",
            "MSE Loss is: 0.01593451201915741, h Loss is: 1637.4200439453125, L1 loss: 2.023813486099243, Total Loss is: 1637.43603515625\n",
            "MSE Loss is: 0.015862654894590378, h Loss is: 1637.517822265625, L1 loss: 2.0242371559143066, Total Loss is: 1637.53369140625\n",
            "MSE Loss is: 0.016053328290581703, h Loss is: 1637.4923095703125, L1 loss: 2.024651050567627, Total Loss is: 1637.5084228515625\n",
            "MSE Loss is: 0.0159526988863945, h Loss is: 1636.8885498046875, L1 loss: 2.0250813961029053, Total Loss is: 1636.904541015625\n",
            "MSE Loss is: 0.015564394183456898, h Loss is: 1636.3526611328125, L1 loss: 2.0254883766174316, Total Loss is: 1636.3682861328125\n",
            "MSE Loss is: 0.015841912478208542, h Loss is: 1634.9168701171875, L1 loss: 2.0257110595703125, Total Loss is: 1634.9327392578125\n",
            "MSE Loss is: 0.015986893326044083, h Loss is: 1633.083740234375, L1 loss: 2.025831937789917, Total Loss is: 1633.0997314453125\n",
            "MSE Loss is: 0.01598101295530796, h Loss is: 1632.0172119140625, L1 loss: 2.0261716842651367, Total Loss is: 1632.033203125\n",
            "MSE Loss is: 0.01600266434252262, h Loss is: 1632.511474609375, L1 loss: 2.026676893234253, Total Loss is: 1632.5274658203125\n",
            "MSE Loss is: 0.016304703429341316, h Loss is: 1632.588134765625, L1 loss: 2.0270652770996094, Total Loss is: 1632.6044921875\n",
            "MSE Loss is: 0.01589035987854004, h Loss is: 1631.4666748046875, L1 loss: 2.027355194091797, Total Loss is: 1631.4825439453125\n",
            "MSE Loss is: 0.015846192836761475, h Loss is: 1629.9803466796875, L1 loss: 2.0276849269866943, Total Loss is: 1629.9962158203125\n",
            "MSE Loss is: 0.016454314813017845, h Loss is: 1629.102294921875, L1 loss: 2.0280263423919678, Total Loss is: 1629.1187744140625\n",
            "MSE Loss is: 0.015836669132113457, h Loss is: 1628.1400146484375, L1 loss: 2.0282585620880127, Total Loss is: 1628.1558837890625\n",
            "MSE Loss is: 0.01669260300695896, h Loss is: 1628.5516357421875, L1 loss: 2.0287094116210938, Total Loss is: 1628.568359375\n",
            "MSE Loss is: 0.01564423181116581, h Loss is: 1629.378662109375, L1 loss: 2.029222249984741, Total Loss is: 1629.394287109375\n",
            "MSE Loss is: 0.015933342278003693, h Loss is: 1630.2506103515625, L1 loss: 2.0298328399658203, Total Loss is: 1630.2666015625\n",
            "MSE Loss is: 0.016192834824323654, h Loss is: 1629.830322265625, L1 loss: 2.030217409133911, Total Loss is: 1629.8465576171875\n",
            "MSE Loss is: 0.01620631292462349, h Loss is: 1627.9693603515625, L1 loss: 2.0303335189819336, Total Loss is: 1627.985595703125\n",
            "MSE Loss is: 0.015371670015156269, h Loss is: 1625.0985107421875, L1 loss: 2.030268907546997, Total Loss is: 1625.1138916015625\n",
            "MSE Loss is: 0.015247311443090439, h Loss is: 1623.2989501953125, L1 loss: 2.0303735733032227, Total Loss is: 1623.314208984375\n",
            "MSE Loss is: 0.016139358282089233, h Loss is: 1623.5455322265625, L1 loss: 2.0308151245117188, Total Loss is: 1623.5616455078125\n",
            "MSE Loss is: 0.015924712643027306, h Loss is: 1625.4937744140625, L1 loss: 2.031545639038086, Total Loss is: 1625.5096435546875\n",
            "MSE Loss is: 0.015700021758675575, h Loss is: 1627.557373046875, L1 loss: 2.032184362411499, Total Loss is: 1627.5731201171875\n",
            "MSE Loss is: 0.015584985725581646, h Loss is: 1627.375244140625, L1 loss: 2.0326523780822754, Total Loss is: 1627.390869140625\n",
            "MSE Loss is: 0.01599305309355259, h Loss is: 1625.6097412109375, L1 loss: 2.032712459564209, Total Loss is: 1625.625732421875\n",
            "New h_val is : tf.Tensor(5.881791, shape=(), dtype=float32)\n",
            "saving model to: /content//CausalNN_model_final_1711387663.h5\n",
            "53/53 [==============================] - 0s 2ms/step\n",
            "The conv layer 1 weights before training : [[ 0.01729116 -0.03241783  0.20739552  0.21393666  0.05369946  0.34967968\n",
            "  -0.14644605 -0.3412853  -0.11558445 -0.22822307  0.21469983  0.0880048\n",
            "  -0.1930318   0.18624505 -0.17902574 -0.3153036  -0.22727013 -0.03919902\n",
            "   0.06509387 -0.07369781 -0.10506889 -0.03960952  0.17076865  0.15640828]]\n",
            "The conv layer 1 weights after training : [[ 0.07100528 -0.06001503  0.14216025 -0.47514835  0.4286465   0.00572329\n",
            "   0.15681776 -0.27346534  0.22036177 -0.01523273  0.1850599  -0.14684404\n",
            "   0.498847   -0.02045046 -0.02911717 -0.07069717  0.51738787  0.04611331\n",
            "   0.16096537 -0.01701978  0.00910253  0.10065496  0.0321525   0.00219245]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_nn, mat_nn = cnnmodel.fit(x=syn_data_2d_nn, y=data_y_syn_nn, maxiter=20, batch_size=2048, save_dir='/content/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llrgyqlk5-_7",
        "outputId": "dccb5dbd-b55a-4182-de8e-82a991f89dca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: {} 0\n",
            "MSE Loss is: 33.54899597167969, h Loss is: 1.003495640361507e-06, L1 loss: 1.4565690755844116, Total Loss is: 33.54899597167969\n",
            "MSE Loss is: 32.97697830200195, h Loss is: 1.230557700182544e-06, L1 loss: 1.4465690851211548, Total Loss is: 32.97697830200195\n",
            "MSE Loss is: 32.64115905761719, h Loss is: 1.5379196156573016e-06, L1 loss: 1.4370572566986084, Total Loss is: 32.64115905761719\n",
            "MSE Loss is: 30.624813079833984, h Loss is: 1.9636372599052265e-06, L1 loss: 1.4292304515838623, Total Loss is: 30.624814987182617\n",
            "MSE Loss is: 30.067358016967773, h Loss is: 2.5542703951941803e-06, L1 loss: 1.42755126953125, Total Loss is: 30.067359924316406\n",
            "MSE Loss is: 29.546630859375, h Loss is: 3.3767832974263e-06, L1 loss: 1.4413032531738281, Total Loss is: 29.546634674072266\n",
            "MSE Loss is: 29.134113311767578, h Loss is: 4.523697498370893e-06, L1 loss: 1.4561477899551392, Total Loss is: 29.134117126464844\n",
            "MSE Loss is: 28.35997200012207, h Loss is: 6.114937150414335e-06, L1 loss: 1.472538948059082, Total Loss is: 28.35997772216797\n",
            "MSE Loss is: 27.363582611083984, h Loss is: 8.318569598486647e-06, L1 loss: 1.4938217401504517, Total Loss is: 27.363590240478516\n",
            "MSE Loss is: 26.7799072265625, h Loss is: 1.1373231245670468e-05, L1 loss: 1.5181556940078735, Total Loss is: 26.779918670654297\n",
            "MSE Loss is: 25.33921241760254, h Loss is: 1.559186785016209e-05, L1 loss: 1.5416619777679443, Total Loss is: 25.3392276763916\n",
            "MSE Loss is: 24.04656219482422, h Loss is: 2.1412708520074375e-05, L1 loss: 1.5652042627334595, Total Loss is: 24.04658317565918\n",
            "MSE Loss is: 24.722505569458008, h Loss is: 2.937000863312278e-05, L1 loss: 1.5905135869979858, Total Loss is: 24.7225341796875\n",
            "MSE Loss is: 22.74445152282715, h Loss is: 4.0190025174524635e-05, L1 loss: 1.6174334287643433, Total Loss is: 22.744491577148438\n",
            "MSE Loss is: 21.56648826599121, h Loss is: 5.466485890792683e-05, L1 loss: 1.6438236236572266, Total Loss is: 21.566543579101562\n",
            "MSE Loss is: 20.923982620239258, h Loss is: 7.361210737144575e-05, L1 loss: 1.671242594718933, Total Loss is: 20.924057006835938\n",
            "MSE Loss is: 19.72345733642578, h Loss is: 9.766564471647143e-05, L1 loss: 1.699057936668396, Total Loss is: 19.723554611206055\n",
            "MSE Loss is: 19.30878448486328, h Loss is: 0.00012805774167645723, L1 loss: 1.7251907587051392, Total Loss is: 19.30891227722168\n",
            "MSE Loss is: 18.274147033691406, h Loss is: 0.00016554046305827796, L1 loss: 1.7511249780654907, Total Loss is: 18.27431297302246\n",
            "MSE Loss is: 18.313705444335938, h Loss is: 0.0002107254695147276, L1 loss: 1.7777715921401978, Total Loss is: 18.313915252685547\n",
            "MSE Loss is: 17.395572662353516, h Loss is: 0.0002643410407472402, L1 loss: 1.806106448173523, Total Loss is: 17.395837783813477\n",
            "MSE Loss is: 17.785537719726562, h Loss is: 0.0003267030406277627, L1 loss: 1.8345168828964233, Total Loss is: 17.785863876342773\n",
            "MSE Loss is: 16.491928100585938, h Loss is: 0.00039800081867724657, L1 loss: 1.8610105514526367, Total Loss is: 16.492326736450195\n",
            "MSE Loss is: 16.434555053710938, h Loss is: 0.00047821577754803, L1 loss: 1.8866745233535767, Total Loss is: 16.435033798217773\n",
            "MSE Loss is: 16.31289291381836, h Loss is: 0.0005669451202265918, L1 loss: 1.9106571674346924, Total Loss is: 16.313459396362305\n",
            "MSE Loss is: 16.521699905395508, h Loss is: 0.0006640024948865175, L1 loss: 1.933813452720642, Total Loss is: 16.522363662719727\n",
            "MSE Loss is: 16.401611328125, h Loss is: 0.0007694539381191134, L1 loss: 1.9552615880966187, Total Loss is: 16.402379989624023\n",
            "MSE Loss is: 15.609683990478516, h Loss is: 0.0008819722570478916, L1 loss: 1.9726842641830444, Total Loss is: 15.610566139221191\n",
            "MSE Loss is: 16.028545379638672, h Loss is: 0.0009975387947633862, L1 loss: 1.9879127740859985, Total Loss is: 16.029542922973633\n",
            "MSE Loss is: 15.938919067382812, h Loss is: 0.001116632018238306, L1 loss: 1.9999679327011108, Total Loss is: 15.940035820007324\n",
            "MSE Loss is: 15.826302528381348, h Loss is: 0.0012412748765200377, L1 loss: 2.009275436401367, Total Loss is: 15.827544212341309\n",
            "MSE Loss is: 15.296969413757324, h Loss is: 0.001370438258163631, L1 loss: 2.0177762508392334, Total Loss is: 15.29833984375\n",
            "MSE Loss is: 15.398715019226074, h Loss is: 0.0015041360165923834, L1 loss: 2.0241048336029053, Total Loss is: 15.400218963623047\n",
            "MSE Loss is: 15.017107963562012, h Loss is: 0.0016414589481428266, L1 loss: 2.0295398235321045, Total Loss is: 15.018749237060547\n",
            "MSE Loss is: 15.015012741088867, h Loss is: 0.0017833838937804103, L1 loss: 2.0350539684295654, Total Loss is: 15.016796112060547\n",
            "MSE Loss is: 15.241247177124023, h Loss is: 0.0019285858143121004, L1 loss: 2.040522336959839, Total Loss is: 15.243175506591797\n",
            "MSE Loss is: 14.836297988891602, h Loss is: 0.0020805462263524532, L1 loss: 2.045442581176758, Total Loss is: 14.83837890625\n",
            "MSE Loss is: 15.257730484008789, h Loss is: 0.0022413323167711496, L1 loss: 2.0502171516418457, Total Loss is: 15.259971618652344\n",
            "MSE Loss is: 14.27730941772461, h Loss is: 0.0024109489750117064, L1 loss: 2.055022954940796, Total Loss is: 14.279720306396484\n",
            "MSE Loss is: 14.40285587310791, h Loss is: 0.0025912809651345015, L1 loss: 2.0607149600982666, Total Loss is: 14.405447006225586\n",
            "MSE Loss is: 14.511759757995605, h Loss is: 0.0027862258721143007, L1 loss: 2.0667080879211426, Total Loss is: 14.514546394348145\n",
            "MSE Loss is: 14.680413246154785, h Loss is: 0.002998349256813526, L1 loss: 2.0729219913482666, Total Loss is: 14.683411598205566\n",
            "MSE Loss is: 13.93089485168457, h Loss is: 0.0032265991903841496, L1 loss: 2.082226514816284, Total Loss is: 13.934121131896973\n",
            "MSE Loss is: 13.927988052368164, h Loss is: 0.003473745658993721, L1 loss: 2.093262195587158, Total Loss is: 13.931461334228516\n",
            "MSE Loss is: 14.465532302856445, h Loss is: 0.003745265305042267, L1 loss: 2.1046109199523926, Total Loss is: 14.469277381896973\n",
            "MSE Loss is: 13.838021278381348, h Loss is: 0.004043321590870619, L1 loss: 2.116374969482422, Total Loss is: 13.84206485748291\n",
            "MSE Loss is: 13.810821533203125, h Loss is: 0.0043685524724423885, L1 loss: 2.129431962966919, Total Loss is: 13.815190315246582\n",
            "MSE Loss is: 13.743840217590332, h Loss is: 0.004730495624244213, L1 loss: 2.1423325538635254, Total Loss is: 13.748570442199707\n",
            "MSE Loss is: 13.864577293395996, h Loss is: 0.0051259915344417095, L1 loss: 2.1551263332366943, Total Loss is: 13.86970329284668\n",
            "New h_val is : tf.Tensor(0.105623245, shape=(), dtype=float32)\n",
            "Epoch: {} 1\n",
            "MSE Loss is: 13.876363754272461, h Loss is: 0.016677025705575943, L1 loss: 2.167372703552246, Total Loss is: 13.893040657043457\n",
            "MSE Loss is: 13.97006607055664, h Loss is: 0.01760375313460827, L1 loss: 2.1791348457336426, Total Loss is: 13.987669944763184\n",
            "MSE Loss is: 13.968762397766113, h Loss is: 0.018592558801174164, L1 loss: 2.1903388500213623, Total Loss is: 13.98735523223877\n",
            "MSE Loss is: 13.283164978027344, h Loss is: 0.01964740827679634, L1 loss: 2.200685977935791, Total Loss is: 13.302812576293945\n",
            "MSE Loss is: 13.622374534606934, h Loss is: 0.020768439397215843, L1 loss: 2.21026611328125, Total Loss is: 13.643142700195312\n",
            "MSE Loss is: 13.516677856445312, h Loss is: 0.021940909326076508, L1 loss: 2.218975067138672, Total Loss is: 13.538619041442871\n",
            "MSE Loss is: 13.621152877807617, h Loss is: 0.0231742225587368, L1 loss: 2.22651743888855, Total Loss is: 13.644327163696289\n",
            "MSE Loss is: 13.505935668945312, h Loss is: 0.024450112134218216, L1 loss: 2.2329471111297607, Total Loss is: 13.530385971069336\n",
            "MSE Loss is: 13.387716293334961, h Loss is: 0.025780200958251953, L1 loss: 2.238266706466675, Total Loss is: 13.413496017456055\n",
            "MSE Loss is: 13.46668815612793, h Loss is: 0.027156192809343338, L1 loss: 2.242368459701538, Total Loss is: 13.493844032287598\n",
            "MSE Loss is: 13.348064422607422, h Loss is: 0.028596002608537674, L1 loss: 2.247772216796875, Total Loss is: 13.376660346984863\n",
            "MSE Loss is: 13.106620788574219, h Loss is: 0.030068455263972282, L1 loss: 2.2526748180389404, Total Loss is: 13.136689186096191\n",
            "MSE Loss is: 13.985196113586426, h Loss is: 0.03156949207186699, L1 loss: 2.2570478916168213, Total Loss is: 14.016765594482422\n",
            "MSE Loss is: 13.277210235595703, h Loss is: 0.03311329334974289, L1 loss: 2.260763645172119, Total Loss is: 13.310323715209961\n",
            "MSE Loss is: 13.066526412963867, h Loss is: 0.034702591598033905, L1 loss: 2.2638227939605713, Total Loss is: 13.101228713989258\n",
            "MSE Loss is: 13.117875099182129, h Loss is: 0.036338239908218384, L1 loss: 2.2665092945098877, Total Loss is: 13.154212951660156\n",
            "MSE Loss is: 13.132038116455078, h Loss is: 0.038081228733062744, L1 loss: 2.2688190937042236, Total Loss is: 13.170119285583496\n",
            "MSE Loss is: 13.515140533447266, h Loss is: 0.03984261304140091, L1 loss: 2.271075487136841, Total Loss is: 13.554983139038086\n",
            "MSE Loss is: 13.040811538696289, h Loss is: 0.041617438197135925, L1 loss: 2.273003101348877, Total Loss is: 13.082428932189941\n",
            "MSE Loss is: 13.652192115783691, h Loss is: 0.0434461385011673, L1 loss: 2.2749245166778564, Total Loss is: 13.695638656616211\n",
            "MSE Loss is: 13.275016784667969, h Loss is: 0.04534047842025757, L1 loss: 2.2770214080810547, Total Loss is: 13.320357322692871\n",
            "MSE Loss is: 13.672834396362305, h Loss is: 0.04729429632425308, L1 loss: 2.2789697647094727, Total Loss is: 13.720129013061523\n",
            "MSE Loss is: 13.120119094848633, h Loss is: 0.04933292046189308, L1 loss: 2.281172513961792, Total Loss is: 13.169451713562012\n",
            "MSE Loss is: 13.292015075683594, h Loss is: 0.051451653242111206, L1 loss: 2.2842719554901123, Total Loss is: 13.343466758728027\n",
            "MSE Loss is: 13.209712982177734, h Loss is: 0.05362262949347496, L1 loss: 2.2872116565704346, Total Loss is: 13.263335227966309\n",
            "MSE Loss is: 13.338513374328613, h Loss is: 0.055816613137722015, L1 loss: 2.290040969848633, Total Loss is: 13.394330024719238\n",
            "MSE Loss is: 13.40424919128418, h Loss is: 0.05809389799833298, L1 loss: 2.2929813861846924, Total Loss is: 13.462343215942383\n",
            "MSE Loss is: 12.796573638916016, h Loss is: 0.060404688119888306, L1 loss: 2.2960381507873535, Total Loss is: 12.856978416442871\n",
            "MSE Loss is: 12.964330673217773, h Loss is: 0.06268317252397537, L1 loss: 2.299314022064209, Total Loss is: 13.027013778686523\n",
            "MSE Loss is: 13.161890029907227, h Loss is: 0.06498313695192337, L1 loss: 2.3025283813476562, Total Loss is: 13.226873397827148\n",
            "MSE Loss is: 13.15089225769043, h Loss is: 0.06740298122167587, L1 loss: 2.305330991744995, Total Loss is: 13.218295097351074\n",
            "MSE Loss is: 13.029657363891602, h Loss is: 0.06995144486427307, L1 loss: 2.307770252227783, Total Loss is: 13.099608421325684\n",
            "MSE Loss is: 13.074877738952637, h Loss is: 0.07256908714771271, L1 loss: 2.30973219871521, Total Loss is: 13.147446632385254\n",
            "MSE Loss is: 12.819366455078125, h Loss is: 0.0752972811460495, L1 loss: 2.3115193843841553, Total Loss is: 12.89466381072998\n",
            "MSE Loss is: 12.909930229187012, h Loss is: 0.0780835896730423, L1 loss: 2.313347101211548, Total Loss is: 12.988014221191406\n",
            "MSE Loss is: 13.340068817138672, h Loss is: 0.08088318258523941, L1 loss: 2.3150928020477295, Total Loss is: 13.420951843261719\n",
            "MSE Loss is: 12.960890769958496, h Loss is: 0.0837240070104599, L1 loss: 2.3160054683685303, Total Loss is: 13.044614791870117\n",
            "MSE Loss is: 13.5966157913208, h Loss is: 0.08657778799533844, L1 loss: 2.3165836334228516, Total Loss is: 13.68319320678711\n",
            "MSE Loss is: 12.656363487243652, h Loss is: 0.08946045488119125, L1 loss: 2.3169472217559814, Total Loss is: 12.745823860168457\n",
            "MSE Loss is: 12.753122329711914, h Loss is: 0.09252242743968964, L1 loss: 2.318012237548828, Total Loss is: 12.8456449508667\n",
            "MSE Loss is: 12.984819412231445, h Loss is: 0.0956747829914093, L1 loss: 2.319396734237671, Total Loss is: 13.080493927001953\n",
            "MSE Loss is: 13.17459487915039, h Loss is: 0.09895524382591248, L1 loss: 2.32072114944458, Total Loss is: 13.273550033569336\n",
            "MSE Loss is: 12.437021255493164, h Loss is: 0.10226631164550781, L1 loss: 2.3218791484832764, Total Loss is: 12.539287567138672\n",
            "MSE Loss is: 12.53957748413086, h Loss is: 0.1056467592716217, L1 loss: 2.323052167892456, Total Loss is: 12.645224571228027\n",
            "MSE Loss is: 13.019876480102539, h Loss is: 0.10913555324077606, L1 loss: 2.324117660522461, Total Loss is: 13.129012107849121\n",
            "MSE Loss is: 12.581485748291016, h Loss is: 0.1127341240644455, L1 loss: 2.324998617172241, Total Loss is: 12.694219589233398\n",
            "MSE Loss is: 12.630008697509766, h Loss is: 0.11643773317337036, L1 loss: 2.325838327407837, Total Loss is: 12.74644660949707\n",
            "MSE Loss is: 12.59823226928711, h Loss is: 0.12039698660373688, L1 loss: 2.326563596725464, Total Loss is: 12.718628883361816\n",
            "MSE Loss is: 12.749913215637207, h Loss is: 0.12450140714645386, L1 loss: 2.3272366523742676, Total Loss is: 12.874414443969727\n",
            "New h_val is : tf.Tensor(0.41282272, shape=(), dtype=float32)\n",
            "Epoch: {} 2\n",
            "MSE Loss is: 12.752664566040039, h Loss is: 0.3245621919631958, L1 loss: 2.3278579711914062, Total Loss is: 13.077226638793945\n",
            "MSE Loss is: 12.832448959350586, h Loss is: 0.33326035737991333, L1 loss: 2.3284058570861816, Total Loss is: 13.165709495544434\n",
            "MSE Loss is: 12.862468719482422, h Loss is: 0.3420476019382477, L1 loss: 2.3290107250213623, Total Loss is: 13.204516410827637\n",
            "MSE Loss is: 12.21632194519043, h Loss is: 0.3510901629924774, L1 loss: 2.3295466899871826, Total Loss is: 12.567412376403809\n",
            "MSE Loss is: 12.585822105407715, h Loss is: 0.3603978753089905, L1 loss: 2.3301339149475098, Total Loss is: 12.946220397949219\n",
            "MSE Loss is: 12.452613830566406, h Loss is: 0.3697267472743988, L1 loss: 2.3304975032806396, Total Loss is: 12.822340965270996\n",
            "MSE Loss is: 12.547430038452148, h Loss is: 0.37916332483291626, L1 loss: 2.3307175636291504, Total Loss is: 12.926593780517578\n",
            "MSE Loss is: 12.482293128967285, h Loss is: 0.38879328966140747, L1 loss: 2.3313100337982178, Total Loss is: 12.871086120605469\n",
            "MSE Loss is: 12.333354949951172, h Loss is: 0.39863407611846924, L1 loss: 2.332181453704834, Total Loss is: 12.731988906860352\n",
            "MSE Loss is: 12.429327011108398, h Loss is: 0.4086150527000427, L1 loss: 2.3335983753204346, Total Loss is: 12.837942123413086\n",
            "MSE Loss is: 12.426263809204102, h Loss is: 0.4189675748348236, L1 loss: 2.3348898887634277, Total Loss is: 12.845231056213379\n",
            "MSE Loss is: 12.200279235839844, h Loss is: 0.42934638261795044, L1 loss: 2.336033582687378, Total Loss is: 12.62962532043457\n",
            "MSE Loss is: 12.964309692382812, h Loss is: 0.43969613313674927, L1 loss: 2.3370602130889893, Total Loss is: 13.404006004333496\n",
            "MSE Loss is: 12.34305191040039, h Loss is: 0.45026010274887085, L1 loss: 2.3378334045410156, Total Loss is: 12.793312072753906\n",
            "MSE Loss is: 12.169820785522461, h Loss is: 0.46119600534439087, L1 loss: 2.3386147022247314, Total Loss is: 12.631016731262207\n",
            "MSE Loss is: 12.281808853149414, h Loss is: 0.4725407660007477, L1 loss: 2.3393349647521973, Total Loss is: 12.754349708557129\n",
            "MSE Loss is: 12.186168670654297, h Loss is: 0.4847269058227539, L1 loss: 2.339850425720215, Total Loss is: 12.67089557647705\n",
            "MSE Loss is: 12.673009872436523, h Loss is: 0.49708688259124756, L1 loss: 2.3404972553253174, Total Loss is: 13.170096397399902\n",
            "MSE Loss is: 12.154430389404297, h Loss is: 0.509453296661377, L1 loss: 2.3405826091766357, Total Loss is: 12.663883209228516\n",
            "MSE Loss is: 12.724170684814453, h Loss is: 0.5221018195152283, L1 loss: 2.341517210006714, Total Loss is: 13.246272087097168\n",
            "MSE Loss is: 12.367884635925293, h Loss is: 0.5352596044540405, L1 loss: 2.3426969051361084, Total Loss is: 12.903143882751465\n",
            "MSE Loss is: 12.749198913574219, h Loss is: 0.5488039255142212, L1 loss: 2.343703269958496, Total Loss is: 13.298003196716309\n",
            "MSE Loss is: 12.184944152832031, h Loss is: 0.5629116296768188, L1 loss: 2.3443593978881836, Total Loss is: 12.747856140136719\n",
            "MSE Loss is: 12.439072608947754, h Loss is: 0.5775080919265747, L1 loss: 2.3452327251434326, Total Loss is: 13.016580581665039\n",
            "MSE Loss is: 12.31312370300293, h Loss is: 0.5922443270683289, L1 loss: 2.345707416534424, Total Loss is: 12.905367851257324\n",
            "MSE Loss is: 12.450399398803711, h Loss is: 0.6069541573524475, L1 loss: 2.345940351486206, Total Loss is: 13.057353973388672\n",
            "MSE Loss is: 12.55137825012207, h Loss is: 0.6220929622650146, L1 loss: 2.3457953929901123, Total Loss is: 13.173471450805664\n",
            "MSE Loss is: 11.938468933105469, h Loss is: 0.6374800205230713, L1 loss: 2.34553861618042, Total Loss is: 12.575948715209961\n",
            "MSE Loss is: 12.074146270751953, h Loss is: 0.6527989506721497, L1 loss: 2.345397710800171, Total Loss is: 12.726944923400879\n",
            "MSE Loss is: 12.281164169311523, h Loss is: 0.6685470938682556, L1 loss: 2.345210552215576, Total Loss is: 12.949710845947266\n",
            "MSE Loss is: 12.30276870727539, h Loss is: 0.6852660179138184, L1 loss: 2.345158576965332, Total Loss is: 12.988035202026367\n",
            "MSE Loss is: 12.179882049560547, h Loss is: 0.7028751373291016, L1 loss: 2.3456177711486816, Total Loss is: 12.882757186889648\n",
            "MSE Loss is: 12.235925674438477, h Loss is: 0.7207846641540527, L1 loss: 2.346205949783325, Total Loss is: 12.956710815429688\n",
            "MSE Loss is: 11.946649551391602, h Loss is: 0.7389636039733887, L1 loss: 2.3470218181610107, Total Loss is: 12.685613632202148\n",
            "MSE Loss is: 12.030189514160156, h Loss is: 0.7572206258773804, L1 loss: 2.3478667736053467, Total Loss is: 12.787409782409668\n",
            "MSE Loss is: 12.458457946777344, h Loss is: 0.7755942344665527, L1 loss: 2.3489272594451904, Total Loss is: 13.234052658081055\n",
            "MSE Loss is: 12.08076000213623, h Loss is: 0.7946321964263916, L1 loss: 2.3493354320526123, Total Loss is: 12.875391960144043\n",
            "MSE Loss is: 12.76854419708252, h Loss is: 0.814303994178772, L1 loss: 2.349630355834961, Total Loss is: 13.58284854888916\n",
            "MSE Loss is: 11.84721565246582, h Loss is: 0.8348932266235352, L1 loss: 2.3495678901672363, Total Loss is: 12.682108879089355\n",
            "MSE Loss is: 11.92066764831543, h Loss is: 0.8566409349441528, L1 loss: 2.3497769832611084, Total Loss is: 12.777308464050293\n",
            "MSE Loss is: 12.131521224975586, h Loss is: 0.8787945508956909, L1 loss: 2.349968671798706, Total Loss is: 13.010315895080566\n",
            "MSE Loss is: 12.339567184448242, h Loss is: 0.9011338949203491, L1 loss: 2.350205421447754, Total Loss is: 13.240700721740723\n",
            "MSE Loss is: 11.611835479736328, h Loss is: 0.9232791662216187, L1 loss: 2.3507537841796875, Total Loss is: 12.535114288330078\n",
            "MSE Loss is: 11.681864738464355, h Loss is: 0.9456782341003418, L1 loss: 2.3512909412384033, Total Loss is: 12.627542495727539\n",
            "MSE Loss is: 12.123514175415039, h Loss is: 0.9688209891319275, L1 loss: 2.3516926765441895, Total Loss is: 13.092334747314453\n",
            "MSE Loss is: 11.72929573059082, h Loss is: 0.9930979013442993, L1 loss: 2.3519139289855957, Total Loss is: 12.722393989562988\n",
            "MSE Loss is: 11.788019180297852, h Loss is: 1.0183889865875244, L1 loss: 2.3520753383636475, Total Loss is: 12.806407928466797\n",
            "MSE Loss is: 11.761757850646973, h Loss is: 1.0446892976760864, L1 loss: 2.351952075958252, Total Loss is: 12.80644702911377\n",
            "MSE Loss is: 11.938232421875, h Loss is: 1.071343183517456, L1 loss: 2.351769208908081, Total Loss is: 13.009575843811035\n",
            "New h_val is : tf.Tensor(0.99311256, shape=(), dtype=float32)\n",
            "Epoch: {} 3\n",
            "MSE Loss is: 11.923602104187012, h Loss is: 2.3450255393981934, L1 loss: 2.3516736030578613, Total Loss is: 14.268627166748047\n",
            "MSE Loss is: 11.988656044006348, h Loss is: 2.3936960697174072, L1 loss: 2.351708173751831, Total Loss is: 14.382351875305176\n",
            "MSE Loss is: 11.988866806030273, h Loss is: 2.443206548690796, L1 loss: 2.3517918586730957, Total Loss is: 14.432073593139648\n",
            "MSE Loss is: 11.400215148925781, h Loss is: 2.4947242736816406, L1 loss: 2.351957082748413, Total Loss is: 13.894939422607422\n",
            "MSE Loss is: 11.750932693481445, h Loss is: 2.548198699951172, L1 loss: 2.351936101913452, Total Loss is: 14.299131393432617\n",
            "MSE Loss is: 11.607744216918945, h Loss is: 2.6026484966278076, L1 loss: 2.3518917560577393, Total Loss is: 14.210392951965332\n",
            "MSE Loss is: 11.706750869750977, h Loss is: 2.657679557800293, L1 loss: 2.3517181873321533, Total Loss is: 14.36443042755127\n",
            "MSE Loss is: 11.589937210083008, h Loss is: 2.7134151458740234, L1 loss: 2.3519530296325684, Total Loss is: 14.303352355957031\n",
            "MSE Loss is: 11.500102996826172, h Loss is: 2.7694437503814697, L1 loss: 2.3524789810180664, Total Loss is: 14.269546508789062\n",
            "MSE Loss is: 11.537232398986816, h Loss is: 2.825730085372925, L1 loss: 2.35284686088562, Total Loss is: 14.36296272277832\n",
            "MSE Loss is: 11.604104995727539, h Loss is: 2.8836112022399902, L1 loss: 2.352851629257202, Total Loss is: 14.487716674804688\n",
            "MSE Loss is: 11.357176780700684, h Loss is: 2.942254066467285, L1 loss: 2.3526597023010254, Total Loss is: 14.299430847167969\n",
            "MSE Loss is: 12.084610939025879, h Loss is: 3.0015172958374023, L1 loss: 2.3527894020080566, Total Loss is: 15.086128234863281\n",
            "MSE Loss is: 11.516048431396484, h Loss is: 3.0624144077301025, L1 loss: 2.3524627685546875, Total Loss is: 14.578462600708008\n",
            "MSE Loss is: 11.363048553466797, h Loss is: 3.124582290649414, L1 loss: 2.3518881797790527, Total Loss is: 14.487630844116211\n",
            "MSE Loss is: 11.457550048828125, h Loss is: 3.1875133514404297, L1 loss: 2.351130962371826, Total Loss is: 14.645063400268555\n",
            "MSE Loss is: 11.342867851257324, h Loss is: 3.2518539428710938, L1 loss: 2.3498876094818115, Total Loss is: 14.594721794128418\n",
            "MSE Loss is: 11.895129203796387, h Loss is: 3.316859483718872, L1 loss: 2.34907603263855, Total Loss is: 15.21198844909668\n",
            "MSE Loss is: 11.303476333618164, h Loss is: 3.3826613426208496, L1 loss: 2.3476734161376953, Total Loss is: 14.686138153076172\n",
            "MSE Loss is: 11.872804641723633, h Loss is: 3.4499759674072266, L1 loss: 2.346055507659912, Total Loss is: 15.32278060913086\n",
            "MSE Loss is: 11.478446960449219, h Loss is: 3.519947052001953, L1 loss: 2.344391345977783, Total Loss is: 14.998394012451172\n",
            "MSE Loss is: 11.889747619628906, h Loss is: 3.591555595397949, L1 loss: 2.3426578044891357, Total Loss is: 15.481303215026855\n",
            "MSE Loss is: 11.333023071289062, h Loss is: 3.6641573905944824, L1 loss: 2.3402509689331055, Total Loss is: 14.997180938720703\n",
            "MSE Loss is: 11.620585441589355, h Loss is: 3.737844467163086, L1 loss: 2.33843731880188, Total Loss is: 15.358429908752441\n",
            "MSE Loss is: 11.482552528381348, h Loss is: 3.811631202697754, L1 loss: 2.3364315032958984, Total Loss is: 15.294183731079102\n",
            "MSE Loss is: 11.647846221923828, h Loss is: 3.886435031890869, L1 loss: 2.33435320854187, Total Loss is: 15.534280776977539\n",
            "MSE Loss is: 11.762922286987305, h Loss is: 3.96317720413208, L1 loss: 2.331801176071167, Total Loss is: 15.726099014282227\n",
            "MSE Loss is: 11.151374816894531, h Loss is: 4.041720390319824, L1 loss: 2.3293864727020264, Total Loss is: 15.193095207214355\n",
            "MSE Loss is: 11.272693634033203, h Loss is: 4.12035608291626, L1 loss: 2.327667713165283, Total Loss is: 15.393049240112305\n",
            "MSE Loss is: 11.451658248901367, h Loss is: 4.1997504234313965, L1 loss: 2.3261184692382812, Total Loss is: 15.651409149169922\n",
            "MSE Loss is: 11.512101173400879, h Loss is: 4.280569076538086, L1 loss: 2.324488878250122, Total Loss is: 15.792670249938965\n",
            "MSE Loss is: 11.358582496643066, h Loss is: 4.363232135772705, L1 loss: 2.3229825496673584, Total Loss is: 15.72181510925293\n",
            "MSE Loss is: 11.443997383117676, h Loss is: 4.447600364685059, L1 loss: 2.3211257457733154, Total Loss is: 15.891597747802734\n",
            "MSE Loss is: 11.124371528625488, h Loss is: 4.532866477966309, L1 loss: 2.3192832469940186, Total Loss is: 15.657238006591797\n",
            "MSE Loss is: 11.20610237121582, h Loss is: 4.6191816329956055, L1 loss: 2.3173465728759766, Total Loss is: 15.825284004211426\n",
            "MSE Loss is: 11.642755508422852, h Loss is: 4.706690788269043, L1 loss: 2.315769910812378, Total Loss is: 16.349445343017578\n",
            "MSE Loss is: 11.290059089660645, h Loss is: 4.796328067779541, L1 loss: 2.31337308883667, Total Loss is: 16.086387634277344\n",
            "MSE Loss is: 11.976522445678711, h Loss is: 4.887359619140625, L1 loss: 2.3111026287078857, Total Loss is: 16.863882064819336\n",
            "MSE Loss is: 11.104342460632324, h Loss is: 4.979948997497559, L1 loss: 2.3104207515716553, Total Loss is: 16.084291458129883\n",
            "MSE Loss is: 11.162817001342773, h Loss is: 5.073123455047607, L1 loss: 2.310321807861328, Total Loss is: 16.23594093322754\n",
            "MSE Loss is: 11.3526611328125, h Loss is: 5.167121887207031, L1 loss: 2.310274362564087, Total Loss is: 16.51978302001953\n",
            "MSE Loss is: 11.576473236083984, h Loss is: 5.261434555053711, L1 loss: 2.310171127319336, Total Loss is: 16.837907791137695\n",
            "MSE Loss is: 10.881057739257812, h Loss is: 5.3563456535339355, L1 loss: 2.310120105743408, Total Loss is: 16.237403869628906\n",
            "MSE Loss is: 10.927797317504883, h Loss is: 5.451836109161377, L1 loss: 2.3099255561828613, Total Loss is: 16.3796329498291\n",
            "MSE Loss is: 11.324195861816406, h Loss is: 5.548018455505371, L1 loss: 2.309603691101074, Total Loss is: 16.872215270996094\n",
            "MSE Loss is: 10.98059368133545, h Loss is: 5.645241737365723, L1 loss: 2.310025215148926, Total Loss is: 16.625835418701172\n",
            "MSE Loss is: 11.045180320739746, h Loss is: 5.743034362792969, L1 loss: 2.310303211212158, Total Loss is: 16.78821563720703\n",
            "MSE Loss is: 11.051336288452148, h Loss is: 5.8414154052734375, L1 loss: 2.310300827026367, Total Loss is: 16.892751693725586\n",
            "MSE Loss is: 11.234652519226074, h Loss is: 5.940193176269531, L1 loss: 2.3102238178253174, Total Loss is: 17.174846649169922\n",
            "New h_val is : tf.Tensor(2.023078, shape=(), dtype=float32)\n",
            "Epoch: {} 4\n",
            "MSE Loss is: 11.20367431640625, h Loss is: 11.73156452178955, L1 loss: 2.310312509536743, Total Loss is: 22.935237884521484\n",
            "MSE Loss is: 11.286430358886719, h Loss is: 11.900094032287598, L1 loss: 2.31076979637146, Total Loss is: 23.1865234375\n",
            "MSE Loss is: 11.253852844238281, h Loss is: 12.071285247802734, L1 loss: 2.311319589614868, Total Loss is: 23.325138092041016\n",
            "MSE Loss is: 10.724874496459961, h Loss is: 12.24554443359375, L1 loss: 2.312199831008911, Total Loss is: 22.97041893005371\n",
            "MSE Loss is: 11.024824142456055, h Loss is: 12.419862747192383, L1 loss: 2.3130016326904297, Total Loss is: 23.444686889648438\n",
            "MSE Loss is: 10.887465476989746, h Loss is: 12.592033386230469, L1 loss: 2.313777446746826, Total Loss is: 23.47949981689453\n",
            "MSE Loss is: 11.021539688110352, h Loss is: 12.760773658752441, L1 loss: 2.3142030239105225, Total Loss is: 23.78231430053711\n",
            "MSE Loss is: 10.868158340454102, h Loss is: 12.926997184753418, L1 loss: 2.314997911453247, Total Loss is: 23.795154571533203\n",
            "MSE Loss is: 10.842955589294434, h Loss is: 13.09024429321289, L1 loss: 2.315978527069092, Total Loss is: 23.93320083618164\n",
            "MSE Loss is: 10.838982582092285, h Loss is: 13.250290870666504, L1 loss: 2.3164620399475098, Total Loss is: 24.08927345275879\n",
            "MSE Loss is: 10.951761245727539, h Loss is: 13.410745620727539, L1 loss: 2.316518545150757, Total Loss is: 24.362506866455078\n",
            "MSE Loss is: 10.678024291992188, h Loss is: 13.568413734436035, L1 loss: 2.3168609142303467, Total Loss is: 24.246437072753906\n",
            "MSE Loss is: 11.429769515991211, h Loss is: 13.724193572998047, L1 loss: 2.3176028728485107, Total Loss is: 25.153963088989258\n",
            "MSE Loss is: 10.887960433959961, h Loss is: 13.882887840270996, L1 loss: 2.317905902862549, Total Loss is: 24.77084732055664\n",
            "MSE Loss is: 10.750146865844727, h Loss is: 14.04174518585205, L1 loss: 2.318132162094116, Total Loss is: 24.791893005371094\n",
            "MSE Loss is: 10.83584213256836, h Loss is: 14.200865745544434, L1 loss: 2.318443536758423, Total Loss is: 25.03670883178711\n",
            "MSE Loss is: 10.77279281616211, h Loss is: 14.35988998413086, L1 loss: 2.3186707496643066, Total Loss is: 25.13268280029297\n",
            "MSE Loss is: 11.318388938903809, h Loss is: 14.521284103393555, L1 loss: 2.3195598125457764, Total Loss is: 25.839672088623047\n",
            "MSE Loss is: 10.697097778320312, h Loss is: 14.684769630432129, L1 loss: 2.31988787651062, Total Loss is: 25.381866455078125\n",
            "MSE Loss is: 11.273846626281738, h Loss is: 14.848236083984375, L1 loss: 2.320338726043701, Total Loss is: 26.122081756591797\n",
            "MSE Loss is: 10.846956253051758, h Loss is: 15.013561248779297, L1 loss: 2.3210902214050293, Total Loss is: 25.860517501831055\n",
            "MSE Loss is: 11.275352478027344, h Loss is: 15.178696632385254, L1 loss: 2.3220980167388916, Total Loss is: 26.45404815673828\n",
            "MSE Loss is: 10.754786491394043, h Loss is: 15.340702056884766, L1 loss: 2.3228187561035156, Total Loss is: 26.095489501953125\n",
            "MSE Loss is: 11.013982772827148, h Loss is: 15.501412391662598, L1 loss: 2.3240420818328857, Total Loss is: 26.515396118164062\n",
            "MSE Loss is: 10.92894172668457, h Loss is: 15.65598201751709, L1 loss: 2.325377941131592, Total Loss is: 26.584922790527344\n",
            "MSE Loss is: 11.123617172241211, h Loss is: 15.806957244873047, L1 loss: 2.326586961746216, Total Loss is: 26.930574417114258\n",
            "MSE Loss is: 11.247194290161133, h Loss is: 15.95419692993164, L1 loss: 2.327772378921509, Total Loss is: 27.201391220092773\n",
            "MSE Loss is: 10.62521743774414, h Loss is: 16.099605560302734, L1 loss: 2.329028367996216, Total Loss is: 26.724822998046875\n",
            "MSE Loss is: 10.756532669067383, h Loss is: 16.24203109741211, L1 loss: 2.3305511474609375, Total Loss is: 26.998563766479492\n",
            "MSE Loss is: 10.912649154663086, h Loss is: 16.383121490478516, L1 loss: 2.3320934772491455, Total Loss is: 27.2957706451416\n",
            "MSE Loss is: 10.997224807739258, h Loss is: 16.52267074584961, L1 loss: 2.333461046218872, Total Loss is: 27.519895553588867\n",
            "MSE Loss is: 10.822301864624023, h Loss is: 16.661104202270508, L1 loss: 2.3349080085754395, Total Loss is: 27.48340606689453\n",
            "MSE Loss is: 10.935108184814453, h Loss is: 16.801332473754883, L1 loss: 2.3369691371917725, Total Loss is: 27.736440658569336\n",
            "MSE Loss is: 10.601774215698242, h Loss is: 16.940818786621094, L1 loss: 2.339061737060547, Total Loss is: 27.542593002319336\n",
            "MSE Loss is: 10.724230766296387, h Loss is: 17.080141067504883, L1 loss: 2.3406944274902344, Total Loss is: 27.804370880126953\n",
            "MSE Loss is: 11.13125228881836, h Loss is: 17.21839714050293, L1 loss: 2.3429903984069824, Total Loss is: 28.34964942932129\n",
            "MSE Loss is: 10.826150894165039, h Loss is: 17.357126235961914, L1 loss: 2.3442180156707764, Total Loss is: 28.183277130126953\n",
            "MSE Loss is: 11.463556289672852, h Loss is: 17.49571990966797, L1 loss: 2.3457131385803223, Total Loss is: 28.95927619934082\n",
            "MSE Loss is: 10.663625717163086, h Loss is: 17.632144927978516, L1 loss: 2.346729278564453, Total Loss is: 28.2957706451416\n",
            "MSE Loss is: 10.715167999267578, h Loss is: 17.762399673461914, L1 loss: 2.348690986633301, Total Loss is: 28.477567672729492\n",
            "MSE Loss is: 10.894735336303711, h Loss is: 17.89020538330078, L1 loss: 2.350675344467163, Total Loss is: 28.784940719604492\n",
            "MSE Loss is: 11.135215759277344, h Loss is: 18.011688232421875, L1 loss: 2.3524487018585205, Total Loss is: 29.14690399169922\n",
            "MSE Loss is: 10.469976425170898, h Loss is: 18.130765914916992, L1 loss: 2.3546881675720215, Total Loss is: 28.60074234008789\n",
            "MSE Loss is: 10.50390338897705, h Loss is: 18.246288299560547, L1 loss: 2.356818914413452, Total Loss is: 28.75019073486328\n",
            "MSE Loss is: 10.881649017333984, h Loss is: 18.359661102294922, L1 loss: 2.3586881160736084, Total Loss is: 29.241310119628906\n",
            "MSE Loss is: 10.580050468444824, h Loss is: 18.471498489379883, L1 loss: 2.3607983589172363, Total Loss is: 29.05154800415039\n",
            "MSE Loss is: 10.631288528442383, h Loss is: 18.580625534057617, L1 loss: 2.3628931045532227, Total Loss is: 29.2119140625\n",
            "MSE Loss is: 10.675628662109375, h Loss is: 18.688478469848633, L1 loss: 2.3642337322235107, Total Loss is: 29.364107131958008\n",
            "MSE Loss is: 10.845819473266602, h Loss is: 18.792957305908203, L1 loss: 2.366337537765503, Total Loss is: 29.638776779174805\n",
            "New h_val is : tf.Tensor(2.9458904, shape=(), dtype=float32)\n",
            "Epoch: {} 5\n",
            "MSE Loss is: 10.812538146972656, h Loss is: 32.172996520996094, L1 loss: 2.368820905685425, Total Loss is: 42.98553466796875\n",
            "MSE Loss is: 10.938521385192871, h Loss is: 32.32957077026367, L1 loss: 2.3719749450683594, Total Loss is: 43.26809310913086\n",
            "MSE Loss is: 10.874418258666992, h Loss is: 32.49015426635742, L1 loss: 2.375209093093872, Total Loss is: 43.36457061767578\n",
            "MSE Loss is: 10.373687744140625, h Loss is: 32.65353775024414, L1 loss: 2.378692388534546, Total Loss is: 43.027225494384766\n",
            "MSE Loss is: 10.623920440673828, h Loss is: 32.806663513183594, L1 loss: 2.3821890354156494, Total Loss is: 43.43058395385742\n",
            "MSE Loss is: 10.527223587036133, h Loss is: 32.94319152832031, L1 loss: 2.385101795196533, Total Loss is: 43.47041320800781\n",
            "MSE Loss is: 10.67277717590332, h Loss is: 33.06252670288086, L1 loss: 2.387321949005127, Total Loss is: 43.73530578613281\n",
            "MSE Loss is: 10.523880004882812, h Loss is: 33.17052459716797, L1 loss: 2.3902335166931152, Total Loss is: 43.69440460205078\n",
            "MSE Loss is: 10.532283782958984, h Loss is: 33.26945495605469, L1 loss: 2.3929927349090576, Total Loss is: 43.80173873901367\n",
            "MSE Loss is: 10.520942687988281, h Loss is: 33.36029052734375, L1 loss: 2.39526629447937, Total Loss is: 43.88123321533203\n",
            "MSE Loss is: 10.643548011779785, h Loss is: 33.452110290527344, L1 loss: 2.3970179557800293, Total Loss is: 44.09565734863281\n",
            "MSE Loss is: 10.342826843261719, h Loss is: 33.540321350097656, L1 loss: 2.398716688156128, Total Loss is: 43.883148193359375\n",
            "MSE Loss is: 11.151429176330566, h Loss is: 33.6292724609375, L1 loss: 2.4002883434295654, Total Loss is: 44.78070068359375\n",
            "MSE Loss is: 10.600103378295898, h Loss is: 33.72942352294922, L1 loss: 2.402092456817627, Total Loss is: 44.32952880859375\n",
            "MSE Loss is: 10.468812942504883, h Loss is: 33.82872772216797, L1 loss: 2.404057264328003, Total Loss is: 44.29753875732422\n",
            "MSE Loss is: 10.559839248657227, h Loss is: 33.926273345947266, L1 loss: 2.406306028366089, Total Loss is: 44.486114501953125\n",
            "MSE Loss is: 10.540925025939941, h Loss is: 34.018856048583984, L1 loss: 2.408287763595581, Total Loss is: 44.55978012084961\n",
            "MSE Loss is: 11.061880111694336, h Loss is: 34.11166000366211, L1 loss: 2.411060333251953, Total Loss is: 45.17353820800781\n",
            "MSE Loss is: 10.449197769165039, h Loss is: 34.20286178588867, L1 loss: 2.4129223823547363, Total Loss is: 44.652061462402344\n",
            "MSE Loss is: 11.034673690795898, h Loss is: 34.287254333496094, L1 loss: 2.4144704341888428, Total Loss is: 45.321929931640625\n",
            "MSE Loss is: 10.595813751220703, h Loss is: 34.36865234375, L1 loss: 2.4163177013397217, Total Loss is: 44.9644660949707\n",
            "MSE Loss is: 11.012884140014648, h Loss is: 34.44573974609375, L1 loss: 2.4179134368896484, Total Loss is: 45.45862579345703\n",
            "MSE Loss is: 10.529611587524414, h Loss is: 34.512969970703125, L1 loss: 2.4181060791015625, Total Loss is: 45.042579650878906\n",
            "MSE Loss is: 10.755680084228516, h Loss is: 34.57799530029297, L1 loss: 2.4194657802581787, Total Loss is: 45.333675384521484\n",
            "MSE Loss is: 10.721016883850098, h Loss is: 34.63063049316406, L1 loss: 2.4212594032287598, Total Loss is: 45.351646423339844\n",
            "MSE Loss is: 10.923896789550781, h Loss is: 34.678592681884766, L1 loss: 2.4238121509552, Total Loss is: 45.60248947143555\n",
            "MSE Loss is: 11.046367645263672, h Loss is: 34.723148345947266, L1 loss: 2.426356077194214, Total Loss is: 45.76951599121094\n",
            "MSE Loss is: 10.408731460571289, h Loss is: 34.768585205078125, L1 loss: 2.428828477859497, Total Loss is: 45.17731475830078\n",
            "MSE Loss is: 10.557966232299805, h Loss is: 34.81462478637695, L1 loss: 2.4317715167999268, Total Loss is: 45.372589111328125\n",
            "MSE Loss is: 10.713106155395508, h Loss is: 34.85936737060547, L1 loss: 2.434849977493286, Total Loss is: 45.572471618652344\n",
            "MSE Loss is: 10.80627155303955, h Loss is: 34.90028762817383, L1 loss: 2.4376094341278076, Total Loss is: 45.70655822753906\n",
            "MSE Loss is: 10.607534408569336, h Loss is: 34.93828582763672, L1 loss: 2.4399361610412598, Total Loss is: 45.54582214355469\n",
            "MSE Loss is: 10.743476867675781, h Loss is: 34.97964859008789, L1 loss: 2.441190719604492, Total Loss is: 45.72312545776367\n",
            "MSE Loss is: 10.403762817382812, h Loss is: 35.018821716308594, L1 loss: 2.442371368408203, Total Loss is: 45.422584533691406\n",
            "MSE Loss is: 10.571578979492188, h Loss is: 35.053775787353516, L1 loss: 2.4434359073638916, Total Loss is: 45.6253547668457\n",
            "MSE Loss is: 10.939037322998047, h Loss is: 35.083473205566406, L1 loss: 2.445202112197876, Total Loss is: 46.02251052856445\n",
            "MSE Loss is: 10.671614646911621, h Loss is: 35.115108489990234, L1 loss: 2.44636607170105, Total Loss is: 45.78672409057617\n",
            "MSE Loss is: 11.255120277404785, h Loss is: 35.14866638183594, L1 loss: 2.4481751918792725, Total Loss is: 46.403785705566406\n",
            "MSE Loss is: 10.506237030029297, h Loss is: 35.17620086669922, L1 loss: 2.449930429458618, Total Loss is: 45.682437896728516\n",
            "MSE Loss is: 10.553292274475098, h Loss is: 35.19194412231445, L1 loss: 2.452496290206909, Total Loss is: 45.745235443115234\n",
            "MSE Loss is: 10.739542007446289, h Loss is: 35.20713424682617, L1 loss: 2.454907178878784, Total Loss is: 45.946678161621094\n",
            "MSE Loss is: 10.98107624053955, h Loss is: 35.213722229003906, L1 loss: 2.456942558288574, Total Loss is: 46.19479751586914\n",
            "MSE Loss is: 10.332855224609375, h Loss is: 35.22148895263672, L1 loss: 2.458549976348877, Total Loss is: 45.554344177246094\n",
            "MSE Loss is: 10.345626831054688, h Loss is: 35.2271728515625, L1 loss: 2.4597160816192627, Total Loss is: 45.57279968261719\n",
            "MSE Loss is: 10.718807220458984, h Loss is: 35.23514175415039, L1 loss: 2.460648775100708, Total Loss is: 45.953948974609375\n",
            "MSE Loss is: 10.45510482788086, h Loss is: 35.24471664428711, L1 loss: 2.4623053073883057, Total Loss is: 45.69982147216797\n",
            "MSE Loss is: 10.482417106628418, h Loss is: 35.25084686279297, L1 loss: 2.464181900024414, Total Loss is: 45.7332649230957\n",
            "MSE Loss is: 10.545450210571289, h Loss is: 35.25692367553711, L1 loss: 2.4651200771331787, Total Loss is: 45.80237579345703\n",
            "MSE Loss is: 10.69818115234375, h Loss is: 35.25735855102539, L1 loss: 2.4666059017181396, Total Loss is: 45.95553970336914\n",
            "New h_val is : tf.Tensor(3.1783276, shape=(), dtype=float32)\n",
            "Epoch: {} 6\n",
            "MSE Loss is: 10.670955657958984, h Loss is: 52.25760269165039, L1 loss: 2.4687976837158203, Total Loss is: 62.928558349609375\n",
            "MSE Loss is: 10.831327438354492, h Loss is: 52.253665924072266, L1 loss: 2.4716451168060303, Total Loss is: 63.084991455078125\n",
            "MSE Loss is: 10.750771522521973, h Loss is: 52.25489807128906, L1 loss: 2.474330425262451, Total Loss is: 63.00566864013672\n",
            "MSE Loss is: 10.26329231262207, h Loss is: 52.260860443115234, L1 loss: 2.4771461486816406, Total Loss is: 62.52415466308594\n",
            "MSE Loss is: 10.472493171691895, h Loss is: 52.25261688232422, L1 loss: 2.480074167251587, Total Loss is: 62.7251091003418\n",
            "MSE Loss is: 10.402074813842773, h Loss is: 52.22206115722656, L1 loss: 2.4825425148010254, Total Loss is: 62.62413787841797\n",
            "MSE Loss is: 10.557393074035645, h Loss is: 52.17071533203125, L1 loss: 2.484344005584717, Total Loss is: 62.72810745239258\n",
            "MSE Loss is: 10.412910461425781, h Loss is: 52.11014175415039, L1 loss: 2.486536741256714, Total Loss is: 62.52305221557617\n",
            "MSE Loss is: 10.428115844726562, h Loss is: 52.04822540283203, L1 loss: 2.488706350326538, Total Loss is: 62.476341247558594\n",
            "MSE Loss is: 10.428461074829102, h Loss is: 51.986656188964844, L1 loss: 2.4907047748565674, Total Loss is: 62.41511535644531\n",
            "MSE Loss is: 10.547998428344727, h Loss is: 51.9362678527832, L1 loss: 2.492103338241577, Total Loss is: 62.48426818847656\n",
            "MSE Loss is: 10.228626251220703, h Loss is: 51.88666915893555, L1 loss: 2.4935109615325928, Total Loss is: 62.11529541015625\n",
            "MSE Loss is: 11.063547134399414, h Loss is: 51.84288787841797, L1 loss: 2.494971513748169, Total Loss is: 62.90643310546875\n",
            "MSE Loss is: 10.503286361694336, h Loss is: 51.81765365600586, L1 loss: 2.496635913848877, Total Loss is: 62.32093811035156\n",
            "MSE Loss is: 10.37533950805664, h Loss is: 51.78804397583008, L1 loss: 2.498276948928833, Total Loss is: 62.16338348388672\n",
            "MSE Loss is: 10.471726417541504, h Loss is: 51.75155258178711, L1 loss: 2.499934673309326, Total Loss is: 62.2232780456543\n",
            "MSE Loss is: 10.466718673706055, h Loss is: 51.705482482910156, L1 loss: 2.5013680458068848, Total Loss is: 62.172203063964844\n",
            "MSE Loss is: 10.980037689208984, h Loss is: 51.65766906738281, L1 loss: 2.503408193588257, Total Loss is: 62.6377067565918\n",
            "MSE Loss is: 10.372786521911621, h Loss is: 51.60928726196289, L1 loss: 2.50460147857666, Total Loss is: 61.98207473754883\n",
            "MSE Loss is: 10.973249435424805, h Loss is: 51.55647277832031, L1 loss: 2.506014347076416, Total Loss is: 62.52972412109375\n",
            "MSE Loss is: 10.520221710205078, h Loss is: 51.507057189941406, L1 loss: 2.507309913635254, Total Loss is: 62.027278900146484\n",
            "MSE Loss is: 10.91857624053955, h Loss is: 51.46110534667969, L1 loss: 2.5085644721984863, Total Loss is: 62.37968063354492\n",
            "MSE Loss is: 10.462730407714844, h Loss is: 51.410804748535156, L1 loss: 2.5089094638824463, Total Loss is: 61.87353515625\n",
            "MSE Loss is: 10.679762840270996, h Loss is: 51.36781311035156, L1 loss: 2.5100739002227783, Total Loss is: 62.047576904296875\n",
            "MSE Loss is: 10.660297393798828, h Loss is: 51.313392639160156, L1 loss: 2.511186361312866, Total Loss is: 61.973690032958984\n",
            "MSE Loss is: 10.864102363586426, h Loss is: 51.25670623779297, L1 loss: 2.512479782104492, Total Loss is: 62.12080764770508\n",
            "MSE Loss is: 10.978964805603027, h Loss is: 51.199058532714844, L1 loss: 2.513746500015259, Total Loss is: 62.17802429199219\n",
            "MSE Loss is: 10.334836959838867, h Loss is: 51.14431381225586, L1 loss: 2.5150344371795654, Total Loss is: 61.479148864746094\n",
            "MSE Loss is: 10.493816375732422, h Loss is: 51.09122085571289, L1 loss: 2.5169851779937744, Total Loss is: 61.58503723144531\n",
            "MSE Loss is: 10.645331382751465, h Loss is: 51.03522872924805, L1 loss: 2.519066572189331, Total Loss is: 61.68056106567383\n",
            "MSE Loss is: 10.742280006408691, h Loss is: 50.973854064941406, L1 loss: 2.520911931991577, Total Loss is: 61.71613311767578\n",
            "MSE Loss is: 10.52151107788086, h Loss is: 50.910438537597656, L1 loss: 2.5225369930267334, Total Loss is: 61.431949615478516\n",
            "MSE Loss is: 10.685016632080078, h Loss is: 50.85528564453125, L1 loss: 2.522902011871338, Total Loss is: 61.54030227661133\n",
            "MSE Loss is: 10.338357925415039, h Loss is: 50.801631927490234, L1 loss: 2.52315354347229, Total Loss is: 61.139991760253906\n",
            "MSE Loss is: 10.531763076782227, h Loss is: 50.746795654296875, L1 loss: 2.52321720123291, Total Loss is: 61.27855682373047\n",
            "MSE Loss is: 10.86809253692627, h Loss is: 50.68948745727539, L1 loss: 2.5238516330718994, Total Loss is: 61.557579040527344\n",
            "MSE Loss is: 10.619820594787598, h Loss is: 50.640380859375, L1 loss: 2.5238900184631348, Total Loss is: 61.26020050048828\n",
            "MSE Loss is: 11.167587280273438, h Loss is: 50.59808349609375, L1 loss: 2.524667978286743, Total Loss is: 61.76567077636719\n",
            "MSE Loss is: 10.456490516662598, h Loss is: 50.54945755004883, L1 loss: 2.5258233547210693, Total Loss is: 61.00594711303711\n",
            "MSE Loss is: 10.49769401550293, h Loss is: 50.48698806762695, L1 loss: 2.527642011642456, Total Loss is: 60.98468017578125\n",
            "MSE Loss is: 10.686527252197266, h Loss is: 50.42626953125, L1 loss: 2.5291554927825928, Total Loss is: 61.112796783447266\n",
            "MSE Loss is: 10.922832489013672, h Loss is: 50.35452651977539, L1 loss: 2.5304114818573, Total Loss is: 61.27735900878906\n",
            "MSE Loss is: 10.280817031860352, h Loss is: 50.287818908691406, L1 loss: 2.5312557220458984, Total Loss is: 60.568634033203125\n",
            "MSE Loss is: 10.276135444641113, h Loss is: 50.22136306762695, L1 loss: 2.531824827194214, Total Loss is: 60.49749755859375\n",
            "MSE Loss is: 10.6537446975708, h Loss is: 50.162147521972656, L1 loss: 2.5326356887817383, Total Loss is: 60.81589126586914\n",
            "MSE Loss is: 10.415451049804688, h Loss is: 50.10801696777344, L1 loss: 2.5347161293029785, Total Loss is: 60.523468017578125\n",
            "MSE Loss is: 10.431142807006836, h Loss is: 50.05118942260742, L1 loss: 2.537259578704834, Total Loss is: 60.482330322265625\n",
            "MSE Loss is: 10.485190391540527, h Loss is: 49.99774932861328, L1 loss: 2.5384581089019775, Total Loss is: 60.482940673828125\n",
            "MSE Loss is: 10.629076957702637, h Loss is: 49.93831253051758, L1 loss: 2.540330648422241, Total Loss is: 60.56739044189453\n",
            "New h_val is : tf.Tensor(3.0522375, shape=(), dtype=float32)\n",
            "Epoch: {} 7\n",
            "MSE Loss is: 10.609977722167969, h Loss is: 67.12509155273438, L1 loss: 2.542630910873413, Total Loss is: 77.73506927490234\n",
            "MSE Loss is: 10.794490814208984, h Loss is: 67.04899597167969, L1 loss: 2.5453383922576904, Total Loss is: 77.84349060058594\n",
            "MSE Loss is: 10.708950996398926, h Loss is: 66.98268127441406, L1 loss: 2.5475733280181885, Total Loss is: 77.69163513183594\n",
            "MSE Loss is: 10.221174240112305, h Loss is: 66.92715454101562, L1 loss: 2.5491695404052734, Total Loss is: 77.14833068847656\n",
            "MSE Loss is: 10.403055191040039, h Loss is: 66.85702514648438, L1 loss: 2.5507466793060303, Total Loss is: 77.26007843017578\n",
            "MSE Loss is: 10.34410285949707, h Loss is: 66.76192474365234, L1 loss: 2.551745891571045, Total Loss is: 77.10602569580078\n",
            "MSE Loss is: 10.516029357910156, h Loss is: 66.64584350585938, L1 loss: 2.5523171424865723, Total Loss is: 77.16187286376953\n",
            "MSE Loss is: 10.372331619262695, h Loss is: 66.52346801757812, L1 loss: 2.5535085201263428, Total Loss is: 76.89579772949219\n",
            "MSE Loss is: 10.38441276550293, h Loss is: 66.40621185302734, L1 loss: 2.554919481277466, Total Loss is: 76.7906265258789\n",
            "MSE Loss is: 10.398504257202148, h Loss is: 66.29405975341797, L1 loss: 2.556244373321533, Total Loss is: 76.69256591796875\n",
            "MSE Loss is: 10.510965347290039, h Loss is: 66.20011901855469, L1 loss: 2.557016134262085, Total Loss is: 76.7110824584961\n",
            "MSE Loss is: 10.182975769042969, h Loss is: 66.10742950439453, L1 loss: 2.5581154823303223, Total Loss is: 76.2904052734375\n",
            "MSE Loss is: 11.026329040527344, h Loss is: 66.02059936523438, L1 loss: 2.559335470199585, Total Loss is: 77.04692840576172\n",
            "MSE Loss is: 10.46501350402832, h Loss is: 65.95561981201172, L1 loss: 2.560227155685425, Total Loss is: 76.4206314086914\n",
            "MSE Loss is: 10.33862018585205, h Loss is: 65.88236236572266, L1 loss: 2.561089038848877, Total Loss is: 76.22098541259766\n",
            "MSE Loss is: 10.427846908569336, h Loss is: 65.79813385009766, L1 loss: 2.5625159740448, Total Loss is: 76.22598266601562\n",
            "MSE Loss is: 10.440439224243164, h Loss is: 65.70278930664062, L1 loss: 2.5637431144714355, Total Loss is: 76.14322662353516\n",
            "MSE Loss is: 10.942056655883789, h Loss is: 65.60594940185547, L1 loss: 2.565657138824463, Total Loss is: 76.54800415039062\n",
            "MSE Loss is: 10.34505844116211, h Loss is: 65.5130844116211, L1 loss: 2.567009449005127, Total Loss is: 75.85813903808594\n",
            "MSE Loss is: 10.957286834716797, h Loss is: 65.42057037353516, L1 loss: 2.568682909011841, Total Loss is: 76.37785339355469\n",
            "MSE Loss is: 10.493846893310547, h Loss is: 65.33926391601562, L1 loss: 2.570131778717041, Total Loss is: 75.83311462402344\n",
            "MSE Loss is: 10.879059791564941, h Loss is: 65.26897430419922, L1 loss: 2.5713164806365967, Total Loss is: 76.14803314208984\n",
            "MSE Loss is: 10.434326171875, h Loss is: 65.19866180419922, L1 loss: 2.571298837661743, Total Loss is: 75.63298797607422\n",
            "MSE Loss is: 10.650331497192383, h Loss is: 65.14063262939453, L1 loss: 2.5721287727355957, Total Loss is: 75.79096221923828\n",
            "MSE Loss is: 10.637933731079102, h Loss is: 65.0683822631836, L1 loss: 2.5730607509613037, Total Loss is: 75.70631408691406\n",
            "MSE Loss is: 10.841079711914062, h Loss is: 64.99266815185547, L1 loss: 2.5739686489105225, Total Loss is: 75.83374786376953\n",
            "MSE Loss is: 10.948864936828613, h Loss is: 64.9155502319336, L1 loss: 2.574671506881714, Total Loss is: 75.86441802978516\n",
            "MSE Loss is: 10.309532165527344, h Loss is: 64.84148406982422, L1 loss: 2.5757317543029785, Total Loss is: 75.15101623535156\n",
            "MSE Loss is: 10.465502738952637, h Loss is: 64.76839447021484, L1 loss: 2.577505350112915, Total Loss is: 75.23389434814453\n",
            "MSE Loss is: 10.61214828491211, h Loss is: 64.6914291381836, L1 loss: 2.579425096511841, Total Loss is: 75.30357360839844\n",
            "MSE Loss is: 10.714261054992676, h Loss is: 64.60953521728516, L1 loss: 2.581333637237549, Total Loss is: 75.32379913330078\n",
            "MSE Loss is: 10.48190689086914, h Loss is: 64.52725982666016, L1 loss: 2.5828256607055664, Total Loss is: 75.00917053222656\n",
            "MSE Loss is: 10.662824630737305, h Loss is: 64.45870208740234, L1 loss: 2.5828258991241455, Total Loss is: 75.12152862548828\n",
            "MSE Loss is: 10.310832977294922, h Loss is: 64.39519500732422, L1 loss: 2.582702398300171, Total Loss is: 74.70602416992188\n",
            "MSE Loss is: 10.516731262207031, h Loss is: 64.33272552490234, L1 loss: 2.5825154781341553, Total Loss is: 74.84945678710938\n",
            "MSE Loss is: 10.833373069763184, h Loss is: 64.2680892944336, L1 loss: 2.5827808380126953, Total Loss is: 75.1014633178711\n",
            "MSE Loss is: 10.595803260803223, h Loss is: 64.21428680419922, L1 loss: 2.5821945667266846, Total Loss is: 74.81008911132812\n",
            "MSE Loss is: 11.123258590698242, h Loss is: 64.16837310791016, L1 loss: 2.582521438598633, Total Loss is: 75.29163360595703\n",
            "MSE Loss is: 10.439760208129883, h Loss is: 64.11237335205078, L1 loss: 2.583148241043091, Total Loss is: 74.55213165283203\n",
            "MSE Loss is: 10.471515655517578, h Loss is: 64.03783416748047, L1 loss: 2.584690809249878, Total Loss is: 74.50935363769531\n",
            "MSE Loss is: 10.659223556518555, h Loss is: 63.96475601196289, L1 loss: 2.586170196533203, Total Loss is: 74.62397766113281\n",
            "MSE Loss is: 10.892295837402344, h Loss is: 63.87751007080078, L1 loss: 2.5872156620025635, Total Loss is: 74.76980590820312\n",
            "MSE Loss is: 10.255496978759766, h Loss is: 63.798736572265625, L1 loss: 2.5878829956054688, Total Loss is: 74.05422973632812\n",
            "MSE Loss is: 10.239767074584961, h Loss is: 63.72194290161133, L1 loss: 2.5884175300598145, Total Loss is: 73.96170806884766\n",
            "MSE Loss is: 10.620828628540039, h Loss is: 63.657649993896484, L1 loss: 2.589076519012451, Total Loss is: 74.27848052978516\n",
            "MSE Loss is: 10.399555206298828, h Loss is: 63.602745056152344, L1 loss: 2.591102361679077, Total Loss is: 74.00230407714844\n",
            "MSE Loss is: 10.408236503601074, h Loss is: 63.5458869934082, L1 loss: 2.5934677124023438, Total Loss is: 73.9541244506836\n",
            "MSE Loss is: 10.451156616210938, h Loss is: 63.49456787109375, L1 loss: 2.594325304031372, Total Loss is: 73.94572448730469\n",
            "MSE Loss is: 10.592568397521973, h Loss is: 63.43528366088867, L1 loss: 2.595918655395508, Total Loss is: 74.0278549194336\n",
            "New h_val is : tf.Tensor(2.899354, shape=(), dtype=float32)\n",
            "Epoch: {} 8\n",
            "MSE Loss is: 10.577608108520508, h Loss is: 80.49207305908203, L1 loss: 2.59786057472229, Total Loss is: 91.0696792602539\n",
            "MSE Loss is: 10.777349472045898, h Loss is: 80.420654296875, L1 loss: 2.599928379058838, Total Loss is: 91.19800567626953\n",
            "MSE Loss is: 10.688295364379883, h Loss is: 80.35993957519531, L1 loss: 2.6016006469726562, Total Loss is: 91.04823303222656\n",
            "MSE Loss is: 10.197858810424805, h Loss is: 80.31331634521484, L1 loss: 2.6024575233459473, Total Loss is: 90.51117706298828\n",
            "MSE Loss is: 10.365560531616211, h Loss is: 80.24840545654297, L1 loss: 2.6030900478363037, Total Loss is: 90.61396789550781\n",
            "MSE Loss is: 10.315353393554688, h Loss is: 80.15243530273438, L1 loss: 2.603299856185913, Total Loss is: 90.46778869628906\n",
            "MSE Loss is: 10.496837615966797, h Loss is: 80.03302764892578, L1 loss: 2.603809356689453, Total Loss is: 90.52986145019531\n",
            "MSE Loss is: 10.352230072021484, h Loss is: 79.90841674804688, L1 loss: 2.6051719188690186, Total Loss is: 90.26065063476562\n",
            "MSE Loss is: 10.361160278320312, h Loss is: 79.79461669921875, L1 loss: 2.606633424758911, Total Loss is: 90.15577697753906\n",
            "MSE Loss is: 10.386655807495117, h Loss is: 79.68953704833984, L1 loss: 2.6076207160949707, Total Loss is: 90.0761947631836\n",
            "MSE Loss is: 10.493523597717285, h Loss is: 79.60847473144531, L1 loss: 2.6080105304718018, Total Loss is: 90.10199737548828\n",
            "MSE Loss is: 10.161290168762207, h Loss is: 79.52684020996094, L1 loss: 2.6089046001434326, Total Loss is: 89.6881332397461\n",
            "MSE Loss is: 11.006823539733887, h Loss is: 79.44833374023438, L1 loss: 2.6097424030303955, Total Loss is: 90.45515441894531\n",
            "MSE Loss is: 10.445953369140625, h Loss is: 79.39278411865234, L1 loss: 2.610269546508789, Total Loss is: 89.83873748779297\n",
            "MSE Loss is: 10.320332527160645, h Loss is: 79.32243347167969, L1 loss: 2.6110782623291016, Total Loss is: 89.64276885986328\n",
            "MSE Loss is: 10.401032447814941, h Loss is: 79.23518371582031, L1 loss: 2.6126163005828857, Total Loss is: 89.63621520996094\n",
            "MSE Loss is: 10.429435729980469, h Loss is: 79.13419342041016, L1 loss: 2.6141223907470703, Total Loss is: 89.56362915039062\n",
            "MSE Loss is: 10.919792175292969, h Loss is: 79.03148651123047, L1 loss: 2.6160430908203125, Total Loss is: 89.95127868652344\n",
            "MSE Loss is: 10.333620071411133, h Loss is: 78.93719482421875, L1 loss: 2.6173088550567627, Total Loss is: 89.27081298828125\n",
            "MSE Loss is: 10.953268051147461, h Loss is: 78.84801483154297, L1 loss: 2.6186683177948, Total Loss is: 89.80128479003906\n",
            "MSE Loss is: 10.482519149780273, h Loss is: 78.77803802490234, L1 loss: 2.619997262954712, Total Loss is: 89.26055908203125\n",
            "MSE Loss is: 10.859193801879883, h Loss is: 78.72657012939453, L1 loss: 2.6205952167510986, Total Loss is: 89.58576202392578\n",
            "MSE Loss is: 10.420734405517578, h Loss is: 78.67781066894531, L1 loss: 2.6198832988739014, Total Loss is: 89.09854125976562\n",
            "MSE Loss is: 10.636214256286621, h Loss is: 78.6436538696289, L1 loss: 2.6202008724212646, Total Loss is: 89.27986907958984\n",
            "MSE Loss is: 10.628202438354492, h Loss is: 78.58955383300781, L1 loss: 2.6203925609588623, Total Loss is: 89.21775817871094\n",
            "MSE Loss is: 10.830110549926758, h Loss is: 78.52821350097656, L1 loss: 2.6205832958221436, Total Loss is: 89.35832214355469\n",
            "MSE Loss is: 10.932883262634277, h Loss is: 78.46289825439453, L1 loss: 2.621004819869995, Total Loss is: 89.39578247070312\n",
            "MSE Loss is: 10.298734664916992, h Loss is: 78.3995590209961, L1 loss: 2.621739149093628, Total Loss is: 88.69829559326172\n",
            "MSE Loss is: 10.450201034545898, h Loss is: 78.3359375, L1 loss: 2.6229801177978516, Total Loss is: 88.78614044189453\n",
            "MSE Loss is: 10.595422744750977, h Loss is: 78.26765441894531, L1 loss: 2.624511241912842, Total Loss is: 88.86307525634766\n",
            "MSE Loss is: 10.70134162902832, h Loss is: 78.19477844238281, L1 loss: 2.626317262649536, Total Loss is: 88.8961181640625\n",
            "MSE Loss is: 10.461353302001953, h Loss is: 78.12320709228516, L1 loss: 2.6276466846466064, Total Loss is: 88.58456420898438\n",
            "MSE Loss is: 10.653557777404785, h Loss is: 78.07083129882812, L1 loss: 2.6277711391448975, Total Loss is: 88.7243881225586\n",
            "MSE Loss is: 10.297723770141602, h Loss is: 78.02607727050781, L1 loss: 2.627671480178833, Total Loss is: 88.32379913330078\n",
            "MSE Loss is: 10.512908935546875, h Loss is: 77.98258209228516, L1 loss: 2.627377510070801, Total Loss is: 88.49549102783203\n",
            "MSE Loss is: 10.814666748046875, h Loss is: 77.93464660644531, L1 loss: 2.6270368099212646, Total Loss is: 88.74931335449219\n",
            "MSE Loss is: 10.583089828491211, h Loss is: 77.8982162475586, L1 loss: 2.6257474422454834, Total Loss is: 88.48130798339844\n",
            "MSE Loss is: 11.097681999206543, h Loss is: 77.86833190917969, L1 loss: 2.625455379486084, Total Loss is: 88.96601104736328\n",
            "MSE Loss is: 10.43338394165039, h Loss is: 77.82251739501953, L1 loss: 2.6254000663757324, Total Loss is: 88.25590515136719\n",
            "MSE Loss is: 10.457496643066406, h Loss is: 77.75247192382812, L1 loss: 2.626361131668091, Total Loss is: 88.20996856689453\n",
            "MSE Loss is: 10.644607543945312, h Loss is: 77.68376922607422, L1 loss: 2.6276283264160156, Total Loss is: 88.32837677001953\n",
            "MSE Loss is: 10.874828338623047, h Loss is: 77.59833526611328, L1 loss: 2.628436326980591, Total Loss is: 88.47315979003906\n",
            "MSE Loss is: 10.241641998291016, h Loss is: 77.52680206298828, L1 loss: 2.629185438156128, Total Loss is: 87.76844787597656\n",
            "MSE Loss is: 10.217963218688965, h Loss is: 77.45991516113281, L1 loss: 2.6299147605895996, Total Loss is: 87.6778793334961\n",
            "MSE Loss is: 10.602052688598633, h Loss is: 77.41185760498047, L1 loss: 2.6310949325561523, Total Loss is: 88.01390838623047\n",
            "MSE Loss is: 10.394353866577148, h Loss is: 77.37759399414062, L1 loss: 2.6336276531219482, Total Loss is: 87.7719497680664\n",
            "MSE Loss is: 10.395313262939453, h Loss is: 77.3404312133789, L1 loss: 2.6360573768615723, Total Loss is: 87.73574829101562\n",
            "MSE Loss is: 10.43156623840332, h Loss is: 77.30889892578125, L1 loss: 2.637225389480591, Total Loss is: 87.74046325683594\n",
            "MSE Loss is: 10.57159423828125, h Loss is: 77.26470947265625, L1 loss: 2.6387765407562256, Total Loss is: 87.8363037109375\n",
            "New h_val is : tf.Tensor(2.791708, shape=(), dtype=float32)\n",
            "Epoch: {} 9\n",
            "MSE Loss is: 10.55798053741455, h Loss is: 94.6721420288086, L1 loss: 2.6402175426483154, Total Loss is: 105.2301254272461\n",
            "MSE Loss is: 10.76812744140625, h Loss is: 94.61793518066406, L1 loss: 2.6419074535369873, Total Loss is: 105.38606262207031\n",
            "MSE Loss is: 10.67705249786377, h Loss is: 94.57475280761719, L1 loss: 2.643247365951538, Total Loss is: 105.2518081665039\n",
            "MSE Loss is: 10.183890342712402, h Loss is: 94.54900360107422, L1 loss: 2.6437759399414062, Total Loss is: 104.73289489746094\n",
            "MSE Loss is: 10.344256401062012, h Loss is: 94.5011215209961, L1 loss: 2.644150495529175, Total Loss is: 104.84537506103516\n",
            "MSE Loss is: 10.300134658813477, h Loss is: 94.41582489013672, L1 loss: 2.644578695297241, Total Loss is: 104.71595764160156\n",
            "MSE Loss is: 10.487627029418945, h Loss is: 94.30489349365234, L1 loss: 2.6449379920959473, Total Loss is: 104.79251861572266\n",
            "MSE Loss is: 10.34122085571289, h Loss is: 94.19078063964844, L1 loss: 2.6467602252960205, Total Loss is: 104.53199768066406\n",
            "MSE Loss is: 10.348775863647461, h Loss is: 94.09410858154297, L1 loss: 2.6481552124023438, Total Loss is: 104.44288635253906\n",
            "MSE Loss is: 10.382085800170898, h Loss is: 94.00946807861328, L1 loss: 2.6488473415374756, Total Loss is: 104.39155578613281\n",
            "MSE Loss is: 10.484466552734375, h Loss is: 93.953857421875, L1 loss: 2.6491901874542236, Total Loss is: 104.43832397460938\n",
            "MSE Loss is: 10.150114059448242, h Loss is: 93.89286041259766, L1 loss: 2.6495845317840576, Total Loss is: 104.04297637939453\n",
            "MSE Loss is: 10.995622634887695, h Loss is: 93.82908630371094, L1 loss: 2.6499485969543457, Total Loss is: 104.82470703125\n",
            "MSE Loss is: 10.435577392578125, h Loss is: 93.78795623779297, L1 loss: 2.6500041484832764, Total Loss is: 104.2235336303711\n",
            "MSE Loss is: 10.31064224243164, h Loss is: 93.72398376464844, L1 loss: 2.6506154537200928, Total Loss is: 104.03462219238281\n",
            "MSE Loss is: 10.384208679199219, h Loss is: 93.63661193847656, L1 loss: 2.651942253112793, Total Loss is: 104.02082061767578\n",
            "MSE Loss is: 10.424843788146973, h Loss is: 93.53425598144531, L1 loss: 2.6532371044158936, Total Loss is: 103.95909881591797\n",
            "MSE Loss is: 10.905756950378418, h Loss is: 93.43292236328125, L1 loss: 2.65499210357666, Total Loss is: 104.33867645263672\n",
            "MSE Loss is: 10.328537940979004, h Loss is: 93.34689331054688, L1 loss: 2.656141996383667, Total Loss is: 103.67543029785156\n",
            "MSE Loss is: 10.953441619873047, h Loss is: 93.27197265625, L1 loss: 2.6572937965393066, Total Loss is: 104.22541809082031\n",
            "MSE Loss is: 10.477520942687988, h Loss is: 93.22557067871094, L1 loss: 2.6584784984588623, Total Loss is: 103.70309448242188\n",
            "MSE Loss is: 10.848649978637695, h Loss is: 93.2038345336914, L1 loss: 2.6587159633636475, Total Loss is: 104.05248260498047\n",
            "MSE Loss is: 10.414030075073242, h Loss is: 93.18466186523438, L1 loss: 2.657698154449463, Total Loss is: 103.59869384765625\n",
            "MSE Loss is: 10.629060745239258, h Loss is: 93.17912292480469, L1 loss: 2.657487630844116, Total Loss is: 103.80818176269531\n",
            "MSE Loss is: 10.623929977416992, h Loss is: 93.14417266845703, L1 loss: 2.6572887897491455, Total Loss is: 103.76810455322266\n",
            "MSE Loss is: 10.824766159057617, h Loss is: 93.09614562988281, L1 loss: 2.657292127609253, Total Loss is: 103.92091369628906\n",
            "MSE Loss is: 10.923809051513672, h Loss is: 93.0416030883789, L1 loss: 2.6575868129730225, Total Loss is: 103.96540832519531\n",
            "MSE Loss is: 10.293468475341797, h Loss is: 92.99027252197266, L1 loss: 2.658081293106079, Total Loss is: 103.28373718261719\n",
            "MSE Loss is: 10.44145393371582, h Loss is: 92.94036865234375, L1 loss: 2.6590497493743896, Total Loss is: 103.38182067871094\n",
            "MSE Loss is: 10.586444854736328, h Loss is: 92.88713836669922, L1 loss: 2.660507917404175, Total Loss is: 103.47358703613281\n",
            "MSE Loss is: 10.69530963897705, h Loss is: 92.8311538696289, L1 loss: 2.662195920944214, Total Loss is: 103.5264663696289\n",
            "MSE Loss is: 10.450069427490234, h Loss is: 92.7785415649414, L1 loss: 2.6633260250091553, Total Loss is: 103.22860717773438\n",
            "MSE Loss is: 10.649717330932617, h Loss is: 92.74980163574219, L1 loss: 2.66321063041687, Total Loss is: 103.39952087402344\n",
            "MSE Loss is: 10.291528701782227, h Loss is: 92.72872161865234, L1 loss: 2.6628592014312744, Total Loss is: 103.02024841308594\n",
            "MSE Loss is: 10.513078689575195, h Loss is: 92.7061767578125, L1 loss: 2.6622402667999268, Total Loss is: 103.21925354003906\n",
            "MSE Loss is: 10.803781509399414, h Loss is: 92.67427825927734, L1 loss: 2.6615498065948486, Total Loss is: 103.47805786132812\n",
            "MSE Loss is: 10.575653076171875, h Loss is: 92.65333557128906, L1 loss: 2.660001277923584, Total Loss is: 103.22898864746094\n",
            "MSE Loss is: 11.082199096679688, h Loss is: 92.63729858398438, L1 loss: 2.659468412399292, Total Loss is: 103.71949768066406\n",
            "MSE Loss is: 10.43115234375, h Loss is: 92.60015869140625, L1 loss: 2.6591365337371826, Total Loss is: 103.03131103515625\n",
            "MSE Loss is: 10.449686050415039, h Loss is: 92.53471374511719, L1 loss: 2.659855365753174, Total Loss is: 102.9843978881836\n",
            "MSE Loss is: 10.636636734008789, h Loss is: 92.47332000732422, L1 loss: 2.6608505249023438, Total Loss is: 103.10995483398438\n",
            "MSE Loss is: 10.864300727844238, h Loss is: 92.39411926269531, L1 loss: 2.661604642868042, Total Loss is: 103.2584228515625\n",
            "MSE Loss is: 10.233631134033203, h Loss is: 92.3369140625, L1 loss: 2.6622722148895264, Total Loss is: 102.57054138183594\n",
            "MSE Loss is: 10.204174041748047, h Loss is: 92.2868881225586, L1 loss: 2.6628165245056152, Total Loss is: 102.49105834960938\n",
            "MSE Loss is: 10.59115219116211, h Loss is: 92.26139068603516, L1 loss: 2.663926362991333, Total Loss is: 102.8525390625\n",
            "MSE Loss is: 10.393714904785156, h Loss is: 92.25208282470703, L1 loss: 2.6662349700927734, Total Loss is: 102.64579772949219\n",
            "MSE Loss is: 10.387550354003906, h Loss is: 92.23567199707031, L1 loss: 2.6684529781341553, Total Loss is: 102.62322235107422\n",
            "MSE Loss is: 10.419290542602539, h Loss is: 92.2228775024414, L1 loss: 2.6696889400482178, Total Loss is: 102.64216613769531\n",
            "MSE Loss is: 10.559029579162598, h Loss is: 92.19097900390625, L1 loss: 2.6709954738616943, Total Loss is: 102.75000762939453\n",
            "New h_val is : tf.Tensor(2.7230992, shape=(), dtype=float32)\n",
            "Epoch: {} 10\n",
            "MSE Loss is: 10.54549789428711, h Loss is: 110.42018127441406, L1 loss: 2.67218017578125, Total Loss is: 120.96568298339844\n",
            "MSE Loss is: 10.762956619262695, h Loss is: 110.38121032714844, L1 loss: 2.673685073852539, Total Loss is: 121.1441650390625\n",
            "MSE Loss is: 10.670604705810547, h Loss is: 110.35633850097656, L1 loss: 2.6747567653656006, Total Loss is: 121.02694702148438\n",
            "MSE Loss is: 10.175342559814453, h Loss is: 110.35581970214844, L1 loss: 2.674893617630005, Total Loss is: 120.53115844726562\n",
            "MSE Loss is: 10.331649780273438, h Loss is: 110.32998657226562, L1 loss: 2.6749210357666016, Total Loss is: 120.66163635253906\n",
            "MSE Loss is: 10.29180908203125, h Loss is: 110.25936889648438, L1 loss: 2.6750638484954834, Total Loss is: 120.55117797851562\n",
            "MSE Loss is: 10.48342227935791, h Loss is: 110.15917205810547, L1 loss: 2.6757664680480957, Total Loss is: 120.64259338378906\n",
            "MSE Loss is: 10.33492660522461, h Loss is: 110.05686950683594, L1 loss: 2.6772313117980957, Total Loss is: 120.39179992675781\n",
            "MSE Loss is: 10.342098236083984, h Loss is: 109.97737121582031, L1 loss: 2.6783759593963623, Total Loss is: 120.31947326660156\n",
            "MSE Loss is: 10.380621910095215, h Loss is: 109.912109375, L1 loss: 2.678891897201538, Total Loss is: 120.29273223876953\n",
            "MSE Loss is: 10.479453086853027, h Loss is: 109.88030242919922, L1 loss: 2.679229497909546, Total Loss is: 120.35975646972656\n",
            "MSE Loss is: 10.14422607421875, h Loss is: 109.83602905273438, L1 loss: 2.679469585418701, Total Loss is: 119.98025512695312\n",
            "MSE Loss is: 10.98906135559082, h Loss is: 109.78230285644531, L1 loss: 2.6796364784240723, Total Loss is: 120.7713623046875\n",
            "MSE Loss is: 10.42962646484375, h Loss is: 109.7523193359375, L1 loss: 2.679640531539917, Total Loss is: 120.18194580078125\n",
            "MSE Loss is: 10.305355072021484, h Loss is: 109.69192504882812, L1 loss: 2.6801466941833496, Total Loss is: 119.99728393554688\n",
            "MSE Loss is: 10.37324333190918, h Loss is: 109.60293579101562, L1 loss: 2.6811439990997314, Total Loss is: 119.97618103027344\n",
            "MSE Loss is: 10.423160552978516, h Loss is: 109.49919891357422, L1 loss: 2.682368278503418, Total Loss is: 119.92236328125\n",
            "MSE Loss is: 10.896747589111328, h Loss is: 109.40118408203125, L1 loss: 2.68363356590271, Total Loss is: 120.29792785644531\n",
            "MSE Loss is: 10.326187133789062, h Loss is: 109.32627868652344, L1 loss: 2.684913396835327, Total Loss is: 119.6524658203125\n",
            "MSE Loss is: 10.954854965209961, h Loss is: 109.2680435180664, L1 loss: 2.686190366744995, Total Loss is: 120.222900390625\n",
            "MSE Loss is: 10.475570678710938, h Loss is: 109.24687194824219, L1 loss: 2.6871285438537598, Total Loss is: 119.72244262695312\n",
            "MSE Loss is: 10.843253135681152, h Loss is: 109.25422668457031, L1 loss: 2.6872336864471436, Total Loss is: 120.09748077392578\n",
            "MSE Loss is: 10.4108247756958, h Loss is: 109.26145935058594, L1 loss: 2.68597149848938, Total Loss is: 119.67228698730469\n",
            "MSE Loss is: 10.625428199768066, h Loss is: 109.2800521850586, L1 loss: 2.6854984760284424, Total Loss is: 119.90547943115234\n",
            "MSE Loss is: 10.622261047363281, h Loss is: 109.2579574584961, L1 loss: 2.684943914413452, Total Loss is: 119.88021850585938\n",
            "MSE Loss is: 10.822315216064453, h Loss is: 109.2176742553711, L1 loss: 2.684755802154541, Total Loss is: 120.03999328613281\n",
            "MSE Loss is: 10.918410301208496, h Loss is: 109.17131042480469, L1 loss: 2.6846673488616943, Total Loss is: 120.0897216796875\n",
            "MSE Loss is: 10.29078197479248, h Loss is: 109.13338470458984, L1 loss: 2.6850345134735107, Total Loss is: 119.42416381835938\n",
            "MSE Loss is: 10.43624496459961, h Loss is: 109.10124969482422, L1 loss: 2.6858718395233154, Total Loss is: 119.53749084472656\n",
            "MSE Loss is: 10.581578254699707, h Loss is: 109.06831359863281, L1 loss: 2.687303304672241, Total Loss is: 119.64989471435547\n",
            "MSE Loss is: 10.692741394042969, h Loss is: 109.03328704833984, L1 loss: 2.6889331340789795, Total Loss is: 119.72602844238281\n",
            "MSE Loss is: 10.44406509399414, h Loss is: 109.00114440917969, L1 loss: 2.6899468898773193, Total Loss is: 119.44520568847656\n",
            "MSE Loss is: 10.648269653320312, h Loss is: 108.99478912353516, L1 loss: 2.6897268295288086, Total Loss is: 119.64305877685547\n",
            "MSE Loss is: 10.28885269165039, h Loss is: 108.9930191040039, L1 loss: 2.689086675643921, Total Loss is: 119.28187561035156\n",
            "MSE Loss is: 10.514551162719727, h Loss is: 108.98495483398438, L1 loss: 2.6882336139678955, Total Loss is: 119.49950408935547\n",
            "MSE Loss is: 10.797211647033691, h Loss is: 108.96189880371094, L1 loss: 2.687474012374878, Total Loss is: 119.75910949707031\n",
            "MSE Loss is: 10.571044921875, h Loss is: 108.95128631591797, L1 loss: 2.685880661010742, Total Loss is: 119.52233123779297\n",
            "MSE Loss is: 11.07274055480957, h Loss is: 108.94672393798828, L1 loss: 2.685046911239624, Total Loss is: 120.01946258544922\n",
            "MSE Loss is: 10.43071460723877, h Loss is: 108.9184799194336, L1 loss: 2.6844077110290527, Total Loss is: 119.34919738769531\n",
            "MSE Loss is: 10.445302963256836, h Loss is: 108.86046600341797, L1 loss: 2.6850955486297607, Total Loss is: 119.30577087402344\n",
            "MSE Loss is: 10.632396697998047, h Loss is: 108.81043243408203, L1 loss: 2.686094045639038, Total Loss is: 119.44282531738281\n",
            "MSE Loss is: 10.857637405395508, h Loss is: 108.74103546142578, L1 loss: 2.6866021156311035, Total Loss is: 119.59867095947266\n",
            "MSE Loss is: 10.22900390625, h Loss is: 108.701416015625, L1 loss: 2.6871156692504883, Total Loss is: 118.930419921875\n",
            "MSE Loss is: 10.195209503173828, h Loss is: 108.66885375976562, L1 loss: 2.6876792907714844, Total Loss is: 118.86405944824219\n",
            "MSE Loss is: 10.584992408752441, h Loss is: 108.66437530517578, L1 loss: 2.688962459564209, Total Loss is: 119.2493667602539\n",
            "MSE Loss is: 10.394847869873047, h Loss is: 108.67676544189453, L1 loss: 2.691162586212158, Total Loss is: 119.07160949707031\n",
            "MSE Loss is: 10.382683753967285, h Loss is: 108.6764907836914, L1 loss: 2.6932408809661865, Total Loss is: 119.05917358398438\n",
            "MSE Loss is: 10.411283493041992, h Loss is: 108.67803955078125, L1 loss: 2.6943068504333496, Total Loss is: 119.08932495117188\n",
            "MSE Loss is: 10.551390647888184, h Loss is: 108.65565490722656, L1 loss: 2.695452928543091, Total Loss is: 119.20704650878906\n",
            "New h_val is : tf.Tensor(2.6818576, shape=(), dtype=float32)\n",
            "Epoch: {} 11\n",
            "MSE Loss is: 10.53739070892334, h Loss is: 128.1156463623047, L1 loss: 2.696472644805908, Total Loss is: 138.6530303955078\n",
            "MSE Loss is: 10.7598876953125, h Loss is: 128.09495544433594, L1 loss: 2.6978533267974854, Total Loss is: 138.85484313964844\n",
            "MSE Loss is: 10.666799545288086, h Loss is: 128.09466552734375, L1 loss: 2.6987595558166504, Total Loss is: 138.76145935058594\n",
            "MSE Loss is: 10.17006778717041, h Loss is: 128.12667846679688, L1 loss: 2.698636770248413, Total Loss is: 138.2967529296875\n",
            "MSE Loss is: 10.324128150939941, h Loss is: 128.1275634765625, L1 loss: 2.6984152793884277, Total Loss is: 138.45169067382812\n",
            "MSE Loss is: 10.287260055541992, h Loss is: 128.0716552734375, L1 loss: 2.6987149715423584, Total Loss is: 138.35891723632812\n",
            "MSE Loss is: 10.481791496276855, h Loss is: 127.97813415527344, L1 loss: 2.699483871459961, Total Loss is: 138.45993041992188\n",
            "MSE Loss is: 10.331172943115234, h Loss is: 127.8816146850586, L1 loss: 2.700732946395874, Total Loss is: 138.21278381347656\n",
            "MSE Loss is: 10.338580131530762, h Loss is: 127.81402587890625, L1 loss: 2.7016093730926514, Total Loss is: 138.15260314941406\n",
            "MSE Loss is: 10.380449295043945, h Loss is: 127.76393127441406, L1 loss: 2.702092409133911, Total Loss is: 138.14437866210938\n",
            "MSE Loss is: 10.476582527160645, h Loss is: 127.75391387939453, L1 loss: 2.7024343013763428, Total Loss is: 138.23049926757812\n",
            "MSE Loss is: 10.141074180603027, h Loss is: 127.72537994384766, L1 loss: 2.702626943588257, Total Loss is: 137.866455078125\n",
            "MSE Loss is: 10.985101699829102, h Loss is: 127.6810073852539, L1 loss: 2.702712059020996, Total Loss is: 138.66610717773438\n",
            "MSE Loss is: 10.42607307434082, h Loss is: 127.66258239746094, L1 loss: 2.702740430831909, Total Loss is: 138.08865356445312\n",
            "MSE Loss is: 10.302400588989258, h Loss is: 127.60553741455078, L1 loss: 2.703312635421753, Total Loss is: 137.90794372558594\n",
            "MSE Loss is: 10.365942001342773, h Loss is: 127.51332092285156, L1 loss: 2.7044975757598877, Total Loss is: 137.87925720214844\n",
            "MSE Loss is: 10.422646522521973, h Loss is: 127.40599060058594, L1 loss: 2.706014633178711, Total Loss is: 137.82864379882812\n",
            "MSE Loss is: 10.890804290771484, h Loss is: 127.30917358398438, L1 loss: 2.7074830532073975, Total Loss is: 138.19998168945312\n",
            "MSE Loss is: 10.324981689453125, h Loss is: 127.24335479736328, L1 loss: 2.7083866596221924, Total Loss is: 137.56832885742188\n",
            "MSE Loss is: 10.956350326538086, h Loss is: 127.19986724853516, L1 loss: 2.7096526622772217, Total Loss is: 138.15621948242188\n",
            "MSE Loss is: 10.475088119506836, h Loss is: 127.20296478271484, L1 loss: 2.7104499340057373, Total Loss is: 137.6780548095703\n",
            "MSE Loss is: 10.840702056884766, h Loss is: 127.23844909667969, L1 loss: 2.7103633880615234, Total Loss is: 138.0791473388672\n",
            "MSE Loss is: 10.409358024597168, h Loss is: 127.27014923095703, L1 loss: 2.708928346633911, Total Loss is: 137.67950439453125\n",
            "MSE Loss is: 10.623614311218262, h Loss is: 127.31112670898438, L1 loss: 2.7083349227905273, Total Loss is: 137.9347381591797\n",
            "MSE Loss is: 10.621818542480469, h Loss is: 127.29911041259766, L1 loss: 2.707728624343872, Total Loss is: 137.92092895507812\n",
            "MSE Loss is: 10.821319580078125, h Loss is: 127.26394653320312, L1 loss: 2.707606554031372, Total Loss is: 138.08526611328125\n",
            "MSE Loss is: 10.915044784545898, h Loss is: 127.22502136230469, L1 loss: 2.7075626850128174, Total Loss is: 138.1400604248047\n",
            "MSE Loss is: 10.289312362670898, h Loss is: 127.20257568359375, L1 loss: 2.707890510559082, Total Loss is: 137.49188232421875\n",
            "MSE Loss is: 10.432992935180664, h Loss is: 127.19142150878906, L1 loss: 2.7087597846984863, Total Loss is: 137.62442016601562\n",
            "MSE Loss is: 10.578926086425781, h Loss is: 127.18135833740234, L1 loss: 2.7101972103118896, Total Loss is: 137.76028442382812\n",
            "MSE Loss is: 10.69192123413086, h Loss is: 127.1678695678711, L1 loss: 2.711634874343872, Total Loss is: 137.8597869873047\n",
            "MSE Loss is: 10.441059112548828, h Loss is: 127.1544189453125, L1 loss: 2.7126224040985107, Total Loss is: 137.59547424316406\n",
            "MSE Loss is: 10.647895812988281, h Loss is: 127.16650390625, L1 loss: 2.7125473022460938, Total Loss is: 137.81439208984375\n",
            "MSE Loss is: 10.28796672821045, h Loss is: 127.17900848388672, L1 loss: 2.711749315261841, Total Loss is: 137.46697998046875\n",
            "MSE Loss is: 10.5162353515625, h Loss is: 127.17945098876953, L1 loss: 2.7107014656066895, Total Loss is: 137.6956787109375\n",
            "MSE Loss is: 10.793145179748535, h Loss is: 127.16064453125, L1 loss: 2.709853172302246, Total Loss is: 137.95379638671875\n",
            "MSE Loss is: 10.567977905273438, h Loss is: 127.15913391113281, L1 loss: 2.7080860137939453, Total Loss is: 137.72711181640625\n",
            "MSE Loss is: 11.067012786865234, h Loss is: 127.1676025390625, L1 loss: 2.7070555686950684, Total Loss is: 138.234619140625\n",
            "MSE Loss is: 10.431001663208008, h Loss is: 127.15166473388672, L1 loss: 2.7062315940856934, Total Loss is: 137.58267211914062\n",
            "MSE Loss is: 10.442888259887695, h Loss is: 127.10460662841797, L1 loss: 2.7068183422088623, Total Loss is: 137.54750061035156\n",
            "MSE Loss is: 10.63018798828125, h Loss is: 127.06893157958984, L1 loss: 2.7077980041503906, Total Loss is: 137.69912719726562\n",
            "MSE Loss is: 10.853191375732422, h Loss is: 127.00965118408203, L1 loss: 2.7084639072418213, Total Loss is: 137.8628387451172\n",
            "MSE Loss is: 10.226343154907227, h Loss is: 126.9867935180664, L1 loss: 2.7088863849639893, Total Loss is: 137.213134765625\n",
            "MSE Loss is: 10.189270973205566, h Loss is: 126.96843719482422, L1 loss: 2.70965838432312, Total Loss is: 137.15771484375\n",
            "MSE Loss is: 10.581706047058105, h Loss is: 126.98152160644531, L1 loss: 2.710850477218628, Total Loss is: 137.563232421875\n",
            "MSE Loss is: 10.396465301513672, h Loss is: 127.01223754882812, L1 loss: 2.7126665115356445, Total Loss is: 137.40870666503906\n",
            "MSE Loss is: 10.37946891784668, h Loss is: 127.02567291259766, L1 loss: 2.714698553085327, Total Loss is: 137.40513610839844\n",
            "MSE Loss is: 10.405879974365234, h Loss is: 127.04129791259766, L1 loss: 2.7157418727874756, Total Loss is: 137.44717407226562\n",
            "MSE Loss is: 10.546689987182617, h Loss is: 127.03089141845703, L1 loss: 2.7168726921081543, Total Loss is: 137.57757568359375\n",
            "New h_val is : tf.Tensor(2.66016, shape=(), dtype=float32)\n",
            "Epoch: {} 12\n",
            "MSE Loss is: 10.532051086425781, h Loss is: 148.1067352294922, L1 loss: 2.717884063720703, Total Loss is: 158.6387939453125\n",
            "MSE Loss is: 10.757862091064453, h Loss is: 148.11102294921875, L1 loss: 2.7192318439483643, Total Loss is: 158.86888122558594\n",
            "MSE Loss is: 10.664473533630371, h Loss is: 148.14170837402344, L1 loss: 2.7200825214385986, Total Loss is: 158.80618286132812\n",
            "MSE Loss is: 10.166759490966797, h Loss is: 148.21136474609375, L1 loss: 2.719851016998291, Total Loss is: 158.3781280517578\n",
            "MSE Loss is: 10.319658279418945, h Loss is: 148.23951721191406, L1 loss: 2.7195537090301514, Total Loss is: 158.55917358398438\n",
            "MSE Loss is: 10.284849166870117, h Loss is: 148.1934814453125, L1 loss: 2.7201130390167236, Total Loss is: 158.47833251953125\n",
            "MSE Loss is: 10.481475830078125, h Loss is: 148.09872436523438, L1 loss: 2.7207894325256348, Total Loss is: 158.5802001953125\n",
            "MSE Loss is: 10.328852653503418, h Loss is: 148.00128173828125, L1 loss: 2.7219314575195312, Total Loss is: 158.33013916015625\n",
            "MSE Loss is: 10.33682632446289, h Loss is: 147.94285583496094, L1 loss: 2.7226758003234863, Total Loss is: 158.27967834472656\n",
            "MSE Loss is: 10.380733489990234, h Loss is: 147.90872192382812, L1 loss: 2.7228786945343018, Total Loss is: 158.28945922851562\n",
            "MSE Loss is: 10.474862098693848, h Loss is: 147.92471313476562, L1 loss: 2.723055362701416, Total Loss is: 158.3995819091797\n",
            "MSE Loss is: 10.139354705810547, h Loss is: 147.91580200195312, L1 loss: 2.7231876850128174, Total Loss is: 158.05516052246094\n",
            "MSE Loss is: 10.982587814331055, h Loss is: 147.8821258544922, L1 loss: 2.723487615585327, Total Loss is: 158.86471557617188\n",
            "MSE Loss is: 10.423891067504883, h Loss is: 147.8755645751953, L1 loss: 2.7235639095306396, Total Loss is: 158.29945373535156\n",
            "MSE Loss is: 10.300710678100586, h Loss is: 147.8183135986328, L1 loss: 2.7241592407226562, Total Loss is: 158.1190185546875\n",
            "MSE Loss is: 10.3609619140625, h Loss is: 147.71685791015625, L1 loss: 2.7252860069274902, Total Loss is: 158.07781982421875\n",
            "MSE Loss is: 10.422521591186523, h Loss is: 147.59930419921875, L1 loss: 2.726740837097168, Total Loss is: 158.02182006835938\n",
            "MSE Loss is: 10.886739730834961, h Loss is: 147.4992218017578, L1 loss: 2.7281463146209717, Total Loss is: 158.38595581054688\n",
            "MSE Loss is: 10.324230194091797, h Loss is: 147.4412841796875, L1 loss: 2.728959798812866, Total Loss is: 157.76551818847656\n",
            "MSE Loss is: 10.957529067993164, h Loss is: 147.413330078125, L1 loss: 2.7299182415008545, Total Loss is: 158.37086486816406\n",
            "MSE Loss is: 10.475273132324219, h Loss is: 147.4438934326172, L1 loss: 2.7306058406829834, Total Loss is: 157.91915893554688\n",
            "MSE Loss is: 10.839715957641602, h Loss is: 147.5109100341797, L1 loss: 2.730372428894043, Total Loss is: 158.3506317138672\n",
            "MSE Loss is: 10.408693313598633, h Loss is: 147.56834411621094, L1 loss: 2.7287676334381104, Total Loss is: 157.97703552246094\n",
            "MSE Loss is: 10.622766494750977, h Loss is: 147.63108825683594, L1 loss: 2.72806715965271, Total Loss is: 158.2538604736328\n",
            "MSE Loss is: 10.621925354003906, h Loss is: 147.6250762939453, L1 loss: 2.7273614406585693, Total Loss is: 158.24700927734375\n",
            "MSE Loss is: 10.820996284484863, h Loss is: 147.590576171875, L1 loss: 2.727191925048828, Total Loss is: 158.4115753173828\n",
            "MSE Loss is: 10.912818908691406, h Loss is: 147.55694580078125, L1 loss: 2.727100372314453, Total Loss is: 158.46975708007812\n",
            "MSE Loss is: 10.288406372070312, h Loss is: 147.55116271972656, L1 loss: 2.7273921966552734, Total Loss is: 157.83956909179688\n",
            "MSE Loss is: 10.430843353271484, h Loss is: 147.5644073486328, L1 loss: 2.728239059448242, Total Loss is: 157.99525451660156\n",
            "MSE Loss is: 10.577502250671387, h Loss is: 147.58082580566406, L1 loss: 2.729659080505371, Total Loss is: 158.1583251953125\n",
            "MSE Loss is: 10.69195556640625, h Loss is: 147.5909423828125, L1 loss: 2.7310562133789062, Total Loss is: 158.28289794921875\n",
            "MSE Loss is: 10.439735412597656, h Loss is: 147.59527587890625, L1 loss: 2.731959104537964, Total Loss is: 158.03500366210938\n",
            "MSE Loss is: 10.647963523864746, h Loss is: 147.6227569580078, L1 loss: 2.7317559719085693, Total Loss is: 158.27072143554688\n",
            "MSE Loss is: 10.287968635559082, h Loss is: 147.64404296875, L1 loss: 2.7308175563812256, Total Loss is: 157.9320068359375\n",
            "MSE Loss is: 10.517724990844727, h Loss is: 147.6478271484375, L1 loss: 2.729677200317383, Total Loss is: 158.16555786132812\n",
            "MSE Loss is: 10.790576934814453, h Loss is: 147.62913513183594, L1 loss: 2.7286808490753174, Total Loss is: 158.41970825195312\n",
            "MSE Loss is: 10.565755844116211, h Loss is: 147.63633728027344, L1 loss: 2.7268192768096924, Total Loss is: 158.20208740234375\n",
            "MSE Loss is: 11.06360149383545, h Loss is: 147.6610870361328, L1 loss: 2.7256765365600586, Total Loss is: 158.7246856689453\n",
            "MSE Loss is: 10.431524276733398, h Loss is: 147.6619110107422, L1 loss: 2.724760055541992, Total Loss is: 158.0934295654297\n",
            "MSE Loss is: 10.441629409790039, h Loss is: 147.62998962402344, L1 loss: 2.7253665924072266, Total Loss is: 158.07162475585938\n",
            "MSE Loss is: 10.62904167175293, h Loss is: 147.6112060546875, L1 loss: 2.726623296737671, Total Loss is: 158.24024963378906\n",
            "MSE Loss is: 10.850081443786621, h Loss is: 147.56103515625, L1 loss: 2.727235794067383, Total Loss is: 158.41111755371094\n",
            "MSE Loss is: 10.224832534790039, h Loss is: 147.55296325683594, L1 loss: 2.7275753021240234, Total Loss is: 157.77780151367188\n",
            "MSE Loss is: 10.18525218963623, h Loss is: 147.54457092285156, L1 loss: 2.728290557861328, Total Loss is: 157.72982788085938\n",
            "MSE Loss is: 10.580160140991211, h Loss is: 147.5712127685547, L1 loss: 2.729440689086914, Total Loss is: 158.1513671875\n",
            "MSE Loss is: 10.398008346557617, h Loss is: 147.6187744140625, L1 loss: 2.7311172485351562, Total Loss is: 158.01678466796875\n",
            "MSE Loss is: 10.377242088317871, h Loss is: 147.64614868164062, L1 loss: 2.73278546333313, Total Loss is: 158.0233917236328\n",
            "MSE Loss is: 10.402124404907227, h Loss is: 147.67916870117188, L1 loss: 2.7338051795959473, Total Loss is: 158.081298828125\n",
            "MSE Loss is: 10.543776512145996, h Loss is: 147.68612670898438, L1 loss: 2.734711170196533, Total Loss is: 158.2299041748047\n",
            "New h_val is : tf.Tensor(2.6529408, shape=(), dtype=float32)\n",
            "Epoch: {} 13\n",
            "MSE Loss is: 10.528477668762207, h Loss is: 170.7642364501953, L1 loss: 2.7356700897216797, Total Loss is: 181.29270935058594\n",
            "MSE Loss is: 10.756348609924316, h Loss is: 170.80130004882812, L1 loss: 2.736923933029175, Total Loss is: 181.55764770507812\n",
            "MSE Loss is: 10.66296672821045, h Loss is: 170.8682403564453, L1 loss: 2.7377090454101562, Total Loss is: 181.5312042236328\n",
            "MSE Loss is: 10.16464614868164, h Loss is: 170.97775268554688, L1 loss: 2.7373554706573486, Total Loss is: 181.14239501953125\n",
            "MSE Loss is: 10.317039489746094, h Loss is: 171.03024291992188, L1 loss: 2.7371299266815186, Total Loss is: 181.3472900390625\n",
            "MSE Loss is: 10.283685684204102, h Loss is: 170.98643493652344, L1 loss: 2.737600564956665, Total Loss is: 181.27012634277344\n",
            "MSE Loss is: 10.481803894042969, h Loss is: 170.88209533691406, L1 loss: 2.738219976425171, Total Loss is: 181.3638916015625\n",
            "MSE Loss is: 10.327375411987305, h Loss is: 170.77952575683594, L1 loss: 2.739290237426758, Total Loss is: 181.10690307617188\n",
            "MSE Loss is: 10.336057662963867, h Loss is: 170.73207092285156, L1 loss: 2.739999532699585, Total Loss is: 181.06813049316406\n",
            "MSE Loss is: 10.381133079528809, h Loss is: 170.7196502685547, L1 loss: 2.740431547164917, Total Loss is: 181.1007843017578\n",
            "MSE Loss is: 10.47378158569336, h Loss is: 170.77041625976562, L1 loss: 2.7405083179473877, Total Loss is: 181.24420166015625\n",
            "MSE Loss is: 10.138360023498535, h Loss is: 170.78627014160156, L1 loss: 2.7403995990753174, Total Loss is: 180.9246368408203\n",
            "MSE Loss is: 10.980888366699219, h Loss is: 170.76318359375, L1 loss: 2.7406492233276367, Total Loss is: 181.74407958984375\n",
            "MSE Loss is: 10.422521591186523, h Loss is: 170.76416015625, L1 loss: 2.740673303604126, Total Loss is: 181.18667602539062\n",
            "MSE Loss is: 10.299707412719727, h Loss is: 170.69821166992188, L1 loss: 2.741253614425659, Total Loss is: 180.9979248046875\n",
            "MSE Loss is: 10.357461929321289, h Loss is: 170.57736206054688, L1 loss: 2.7423369884490967, Total Loss is: 180.93482971191406\n",
            "MSE Loss is: 10.422479629516602, h Loss is: 170.44203186035156, L1 loss: 2.7437493801116943, Total Loss is: 180.86451721191406\n",
            "MSE Loss is: 10.883878707885742, h Loss is: 170.33639526367188, L1 loss: 2.745119333267212, Total Loss is: 181.22027587890625\n",
            "MSE Loss is: 10.323637008666992, h Loss is: 170.28933715820312, L1 loss: 2.745861053466797, Total Loss is: 180.61297607421875\n",
            "MSE Loss is: 10.95832347869873, h Loss is: 170.28280639648438, L1 loss: 2.7466962337493896, Total Loss is: 181.2411346435547\n",
            "MSE Loss is: 10.475748062133789, h Loss is: 170.3483428955078, L1 loss: 2.7472195625305176, Total Loss is: 180.8240966796875\n",
            "MSE Loss is: 10.839588165283203, h Loss is: 170.451416015625, L1 loss: 2.746885061264038, Total Loss is: 181.29100036621094\n",
            "MSE Loss is: 10.408400535583496, h Loss is: 170.5343780517578, L1 loss: 2.7453577518463135, Total Loss is: 180.94277954101562\n",
            "MSE Loss is: 10.622428894042969, h Loss is: 170.61500549316406, L1 loss: 2.744455099105835, Total Loss is: 181.2374267578125\n",
            "MSE Loss is: 10.622243881225586, h Loss is: 170.60723876953125, L1 loss: 2.7437076568603516, Total Loss is: 181.22947692871094\n",
            "MSE Loss is: 10.820950508117676, h Loss is: 170.5662078857422, L1 loss: 2.743628740310669, Total Loss is: 181.3871612548828\n",
            "MSE Loss is: 10.911264419555664, h Loss is: 170.5355224609375, L1 loss: 2.743518590927124, Total Loss is: 181.44679260253906\n",
            "MSE Loss is: 10.287779808044434, h Loss is: 170.55020141601562, L1 loss: 2.7437961101531982, Total Loss is: 180.83798217773438\n",
            "MSE Loss is: 10.429328918457031, h Loss is: 170.59530639648438, L1 loss: 2.7447500228881836, Total Loss is: 181.02462768554688\n",
            "MSE Loss is: 10.576778411865234, h Loss is: 170.64585876464844, L1 loss: 2.7461178302764893, Total Loss is: 181.22264099121094\n",
            "MSE Loss is: 10.692388534545898, h Loss is: 170.6832733154297, L1 loss: 2.747349262237549, Total Loss is: 181.3756561279297\n",
            "MSE Loss is: 10.439352035522461, h Loss is: 170.70370483398438, L1 loss: 2.7481353282928467, Total Loss is: 181.14305114746094\n",
            "MSE Loss is: 10.648210525512695, h Loss is: 170.74050903320312, L1 loss: 2.748162031173706, Total Loss is: 181.3887176513672\n",
            "MSE Loss is: 10.288393020629883, h Loss is: 170.7625274658203, L1 loss: 2.7470364570617676, Total Loss is: 181.05091857910156\n",
            "MSE Loss is: 10.51894474029541, h Loss is: 170.7615509033203, L1 loss: 2.7456488609313965, Total Loss is: 181.28050231933594\n",
            "MSE Loss is: 10.788923263549805, h Loss is: 170.7386474609375, L1 loss: 2.744767427444458, Total Loss is: 181.52757263183594\n",
            "MSE Loss is: 10.564014434814453, h Loss is: 170.75685119628906, L1 loss: 2.7428138256073, Total Loss is: 181.32086181640625\n",
            "MSE Loss is: 11.061630249023438, h Loss is: 170.80462646484375, L1 loss: 2.7414910793304443, Total Loss is: 181.8662567138672\n",
            "MSE Loss is: 10.43208122253418, h Loss is: 170.83009338378906, L1 loss: 2.7405354976654053, Total Loss is: 181.26217651367188\n",
            "MSE Loss is: 10.441061019897461, h Loss is: 170.81964111328125, L1 loss: 2.7412149906158447, Total Loss is: 181.2606964111328\n",
            "MSE Loss is: 10.62844467163086, h Loss is: 170.82022094726562, L1 loss: 2.742438554763794, Total Loss is: 181.44866943359375\n",
            "MSE Loss is: 10.84780502319336, h Loss is: 170.77626037597656, L1 loss: 2.7430026531219482, Total Loss is: 181.6240692138672\n",
            "MSE Loss is: 10.223998069763184, h Loss is: 170.77783203125, L1 loss: 2.7432801723480225, Total Loss is: 181.0018310546875\n",
            "MSE Loss is: 10.182464599609375, h Loss is: 170.7725372314453, L1 loss: 2.7439606189727783, Total Loss is: 180.9550018310547\n",
            "MSE Loss is: 10.579639434814453, h Loss is: 170.80868530273438, L1 loss: 2.745094060897827, Total Loss is: 181.38832092285156\n",
            "MSE Loss is: 10.399272918701172, h Loss is: 170.87322998046875, L1 loss: 2.7467398643493652, Total Loss is: 181.2725067138672\n",
            "MSE Loss is: 10.375659942626953, h Loss is: 170.91851806640625, L1 loss: 2.74835467338562, Total Loss is: 181.29417419433594\n",
            "MSE Loss is: 10.39944076538086, h Loss is: 170.9766845703125, L1 loss: 2.7494680881500244, Total Loss is: 181.37612915039062\n",
            "MSE Loss is: 10.541958808898926, h Loss is: 171.01043701171875, L1 loss: 2.7499380111694336, Total Loss is: 181.55239868164062\n",
            "New h_val is : tf.Tensor(2.656783, shape=(), dtype=float32)\n",
            "Epoch: {} 14\n",
            "MSE Loss is: 10.526031494140625, h Loss is: 196.49685668945312, L1 loss: 2.750683307647705, Total Loss is: 207.02288818359375\n",
            "MSE Loss is: 10.755107879638672, h Loss is: 196.57342529296875, L1 loss: 2.751845598220825, Total Loss is: 207.3285369873047\n",
            "MSE Loss is: 10.661907196044922, h Loss is: 196.6786346435547, L1 loss: 2.7525949478149414, Total Loss is: 207.34054565429688\n",
            "MSE Loss is: 10.16326904296875, h Loss is: 196.826171875, L1 loss: 2.752162218093872, Total Loss is: 206.98944091796875\n",
            "MSE Loss is: 10.315540313720703, h Loss is: 196.8963165283203, L1 loss: 2.7519795894622803, Total Loss is: 207.21185302734375\n",
            "MSE Loss is: 10.283246994018555, h Loss is: 196.84603881835938, L1 loss: 2.7523915767669678, Total Loss is: 207.12928771972656\n",
            "MSE Loss is: 10.482422828674316, h Loss is: 196.72535705566406, L1 loss: 2.753019332885742, Total Loss is: 207.20777893066406\n",
            "MSE Loss is: 10.326412200927734, h Loss is: 196.6181640625, L1 loss: 2.754096746444702, Total Loss is: 206.944580078125\n",
            "MSE Loss is: 10.335821151733398, h Loss is: 196.58969116210938, L1 loss: 2.7550759315490723, Total Loss is: 206.92550659179688\n",
            "MSE Loss is: 10.381514549255371, h Loss is: 196.61077880859375, L1 loss: 2.755540609359741, Total Loss is: 206.99229431152344\n",
            "MSE Loss is: 10.473077774047852, h Loss is: 196.70687866210938, L1 loss: 2.755585193634033, Total Loss is: 207.17996215820312\n",
            "MSE Loss is: 10.137734413146973, h Loss is: 196.7505340576172, L1 loss: 2.7552947998046875, Total Loss is: 206.88827514648438\n",
            "MSE Loss is: 10.979665756225586, h Loss is: 196.73228454589844, L1 loss: 2.7553844451904297, Total Loss is: 207.71194458007812\n",
            "MSE Loss is: 10.421646118164062, h Loss is: 196.73025512695312, L1 loss: 2.7551910877227783, Total Loss is: 207.1519012451172\n",
            "MSE Loss is: 10.299060821533203, h Loss is: 196.64190673828125, L1 loss: 2.755767345428467, Total Loss is: 206.9409637451172\n",
            "MSE Loss is: 10.354918479919434, h Loss is: 196.4906768798828, L1 loss: 2.756819486618042, Total Loss is: 206.84559631347656\n",
            "MSE Loss is: 10.422407150268555, h Loss is: 196.3319549560547, L1 loss: 2.7582032680511475, Total Loss is: 206.75436401367188\n",
            "MSE Loss is: 10.88179874420166, h Loss is: 196.224609375, L1 loss: 2.7595574855804443, Total Loss is: 207.10641479492188\n",
            "MSE Loss is: 10.323086738586426, h Loss is: 196.1983184814453, L1 loss: 2.7603213787078857, Total Loss is: 206.5214080810547\n",
            "MSE Loss is: 10.958769798278809, h Loss is: 196.22341918945312, L1 loss: 2.761479377746582, Total Loss is: 207.18218994140625\n",
            "MSE Loss is: 10.476336479187012, h Loss is: 196.33181762695312, L1 loss: 2.761728286743164, Total Loss is: 206.8081512451172\n",
            "MSE Loss is: 10.839914321899414, h Loss is: 196.47251892089844, L1 loss: 2.7613906860351562, Total Loss is: 207.31243896484375\n",
            "MSE Loss is: 10.408287048339844, h Loss is: 196.5751953125, L1 loss: 2.759789228439331, Total Loss is: 206.98348999023438\n",
            "MSE Loss is: 10.62232494354248, h Loss is: 196.66380310058594, L1 loss: 2.7588508129119873, Total Loss is: 207.2861328125\n",
            "MSE Loss is: 10.622611999511719, h Loss is: 196.64305114746094, L1 loss: 2.758074998855591, Total Loss is: 207.26565551757812\n",
            "MSE Loss is: 10.821002960205078, h Loss is: 196.5888671875, L1 loss: 2.7579612731933594, Total Loss is: 207.4098663330078\n",
            "MSE Loss is: 10.910135269165039, h Loss is: 196.5631561279297, L1 loss: 2.757848024368286, Total Loss is: 207.47329711914062\n",
            "MSE Loss is: 10.28727912902832, h Loss is: 196.60887145996094, L1 loss: 2.758129596710205, Total Loss is: 206.89614868164062\n",
            "MSE Loss is: 10.428182601928711, h Loss is: 196.69903564453125, L1 loss: 2.7591004371643066, Total Loss is: 207.12721252441406\n",
            "MSE Loss is: 10.576471328735352, h Loss is: 196.79345703125, L1 loss: 2.760469675064087, Total Loss is: 207.36993408203125\n",
            "MSE Loss is: 10.69299602508545, h Loss is: 196.85968017578125, L1 loss: 2.761676073074341, Total Loss is: 207.55267333984375\n",
            "MSE Loss is: 10.43947696685791, h Loss is: 196.88943481445312, L1 loss: 2.7624080181121826, Total Loss is: 207.32891845703125\n",
            "MSE Loss is: 10.648519515991211, h Loss is: 196.9237060546875, L1 loss: 2.7623467445373535, Total Loss is: 207.5722198486328\n",
            "MSE Loss is: 10.288985252380371, h Loss is: 196.93382263183594, L1 loss: 2.761190414428711, Total Loss is: 207.22280883789062\n",
            "MSE Loss is: 10.519936561584473, h Loss is: 196.91928100585938, L1 loss: 2.7597110271453857, Total Loss is: 207.43922424316406\n",
            "MSE Loss is: 10.787840843200684, h Loss is: 196.8911895751953, L1 loss: 2.7587881088256836, Total Loss is: 207.6790313720703\n",
            "MSE Loss is: 10.562561988830566, h Loss is: 196.9281005859375, L1 loss: 2.7567803859710693, Total Loss is: 207.49066162109375\n",
            "MSE Loss is: 11.060554504394531, h Loss is: 197.011962890625, L1 loss: 2.7553553581237793, Total Loss is: 208.072509765625\n",
            "MSE Loss is: 10.432610511779785, h Loss is: 197.07408142089844, L1 loss: 2.7543399333953857, Total Loss is: 207.50669860839844\n",
            "MSE Loss is: 10.440925598144531, h Loss is: 197.09124755859375, L1 loss: 2.7549707889556885, Total Loss is: 207.53216552734375\n",
            "MSE Loss is: 10.628128051757812, h Loss is: 197.11050415039062, L1 loss: 2.7561728954315186, Total Loss is: 207.73863220214844\n",
            "MSE Loss is: 10.846078872680664, h Loss is: 197.06381225585938, L1 loss: 2.756692409515381, Total Loss is: 207.90989685058594\n",
            "MSE Loss is: 10.223555564880371, h Loss is: 197.06491088867188, L1 loss: 2.7569243907928467, Total Loss is: 207.28846740722656\n",
            "MSE Loss is: 10.180486679077148, h Loss is: 197.0538787841797, L1 loss: 2.7575881481170654, Total Loss is: 207.23435974121094\n",
            "MSE Loss is: 10.579716682434082, h Loss is: 197.09693908691406, L1 loss: 2.758725166320801, Total Loss is: 207.67665100097656\n",
            "MSE Loss is: 10.400221824645996, h Loss is: 197.18411254882812, L1 loss: 2.760359287261963, Total Loss is: 207.58433532714844\n",
            "MSE Loss is: 10.374530792236328, h Loss is: 197.25816345214844, L1 loss: 2.7620608806610107, Total Loss is: 207.6326904296875\n",
            "MSE Loss is: 10.39744758605957, h Loss is: 197.355224609375, L1 loss: 2.763171434402466, Total Loss is: 207.75267028808594\n",
            "MSE Loss is: 10.540818214416504, h Loss is: 197.4275665283203, L1 loss: 2.7635483741760254, Total Loss is: 207.9683837890625\n",
            "New h_val is : tf.Tensor(2.6692696, shape=(), dtype=float32)\n",
            "Epoch: {} 15\n",
            "MSE Loss is: 10.524311065673828, h Loss is: 225.75830078125, L1 loss: 2.7640187740325928, Total Loss is: 236.28260803222656\n",
            "MSE Loss is: 10.754040718078613, h Loss is: 225.87643432617188, L1 loss: 2.7650036811828613, Total Loss is: 236.63047790527344\n",
            "MSE Loss is: 10.661089897155762, h Loss is: 226.01565551757812, L1 loss: 2.765906572341919, Total Loss is: 236.67674255371094\n",
            "MSE Loss is: 10.162335395812988, h Loss is: 226.19422912597656, L1 loss: 2.765280246734619, Total Loss is: 236.3565673828125\n",
            "MSE Loss is: 10.314704895019531, h Loss is: 226.273681640625, L1 loss: 2.764960527420044, Total Loss is: 236.58837890625\n",
            "MSE Loss is: 10.283231735229492, h Loss is: 226.21026611328125, L1 loss: 2.7655675411224365, Total Loss is: 236.49349975585938\n",
            "MSE Loss is: 10.483152389526367, h Loss is: 226.07228088378906, L1 loss: 2.766357898712158, Total Loss is: 236.55543518066406\n",
            "MSE Loss is: 10.325761795043945, h Loss is: 225.96844482421875, L1 loss: 2.7674074172973633, Total Loss is: 236.29420471191406\n",
            "MSE Loss is: 10.33583927154541, h Loss is: 225.97384643554688, L1 loss: 2.7683589458465576, Total Loss is: 236.3096923828125\n",
            "MSE Loss is: 10.381824493408203, h Loss is: 226.04212951660156, L1 loss: 2.7688188552856445, Total Loss is: 236.4239501953125\n",
            "MSE Loss is: 10.472626686096191, h Loss is: 226.1922149658203, L1 loss: 2.7688398361206055, Total Loss is: 236.6648406982422\n",
            "MSE Loss is: 10.137303352355957, h Loss is: 226.26011657714844, L1 loss: 2.7685155868530273, Total Loss is: 236.3974151611328\n",
            "MSE Loss is: 10.978720664978027, h Loss is: 226.2327117919922, L1 loss: 2.768568992614746, Total Loss is: 237.21142578125\n",
            "MSE Loss is: 10.42106819152832, h Loss is: 226.21133422851562, L1 loss: 2.7682368755340576, Total Loss is: 236.6324005126953\n",
            "MSE Loss is: 10.298595428466797, h Loss is: 226.08535766601562, L1 loss: 2.7686214447021484, Total Loss is: 236.3839569091797\n",
            "MSE Loss is: 10.353021621704102, h Loss is: 225.8953399658203, L1 loss: 2.769758701324463, Total Loss is: 236.2483673095703\n",
            "MSE Loss is: 10.422271728515625, h Loss is: 225.71551513671875, L1 loss: 2.771177291870117, Total Loss is: 236.13778686523438\n",
            "MSE Loss is: 10.880221366882324, h Loss is: 225.61802673339844, L1 loss: 2.772444725036621, Total Loss is: 236.4982452392578\n",
            "MSE Loss is: 10.322535514831543, h Loss is: 225.62680053710938, L1 loss: 2.7734134197235107, Total Loss is: 235.9493408203125\n",
            "MSE Loss is: 10.958938598632812, h Loss is: 225.69430541992188, L1 loss: 2.774535894393921, Total Loss is: 236.6532440185547\n",
            "MSE Loss is: 10.47696304321289, h Loss is: 225.8482208251953, L1 loss: 2.7746455669403076, Total Loss is: 236.32518005371094\n",
            "MSE Loss is: 10.840452194213867, h Loss is: 226.0199737548828, L1 loss: 2.774261474609375, Total Loss is: 236.8604278564453\n",
            "MSE Loss is: 10.408262252807617, h Loss is: 226.12896728515625, L1 loss: 2.772589921951294, Total Loss is: 236.5372314453125\n",
            "MSE Loss is: 10.622282028198242, h Loss is: 226.2132110595703, L1 loss: 2.7716312408447266, Total Loss is: 236.8354949951172\n",
            "MSE Loss is: 10.62296199798584, h Loss is: 226.169677734375, L1 loss: 2.7708442211151123, Total Loss is: 236.79263305664062\n",
            "MSE Loss is: 10.821088790893555, h Loss is: 226.10208129882812, L1 loss: 2.770702838897705, Total Loss is: 236.9231719970703\n",
            "MSE Loss is: 10.909276962280273, h Loss is: 226.0914764404297, L1 loss: 2.7705955505371094, Total Loss is: 237.00074768066406\n",
            "MSE Loss is: 10.286815643310547, h Loss is: 226.1855926513672, L1 loss: 2.770886182785034, Total Loss is: 236.472412109375\n",
            "MSE Loss is: 10.427251815795898, h Loss is: 226.33612060546875, L1 loss: 2.771878957748413, Total Loss is: 236.76336669921875\n",
            "MSE Loss is: 10.576430320739746, h Loss is: 226.48123168945312, L1 loss: 2.7732582092285156, Total Loss is: 237.0576629638672\n",
            "MSE Loss is: 10.6936674118042, h Loss is: 226.57040405273438, L1 loss: 2.774444818496704, Total Loss is: 237.26406860351562\n",
            "MSE Loss is: 10.439847946166992, h Loss is: 226.5941925048828, L1 loss: 2.775130033493042, Total Loss is: 237.03404235839844\n",
            "MSE Loss is: 10.648828506469727, h Loss is: 226.6083526611328, L1 loss: 2.775042772293091, Total Loss is: 237.25718688964844\n",
            "MSE Loss is: 10.289624214172363, h Loss is: 226.59359741210938, L1 loss: 2.7738358974456787, Total Loss is: 236.8832244873047\n",
            "MSE Loss is: 10.520768165588379, h Loss is: 226.56150817871094, L1 loss: 2.7722790241241455, Total Loss is: 237.082275390625\n",
            "MSE Loss is: 10.78712272644043, h Loss is: 226.53541564941406, L1 loss: 2.7713229656219482, Total Loss is: 237.32254028320312\n",
            "MSE Loss is: 10.56129264831543, h Loss is: 226.60745239257812, L1 loss: 2.769282579421997, Total Loss is: 237.1687469482422\n",
            "MSE Loss is: 11.060014724731445, h Loss is: 226.7455596923828, L1 loss: 2.7677628993988037, Total Loss is: 237.80557250976562\n",
            "MSE Loss is: 10.433111190795898, h Loss is: 226.8555450439453, L1 loss: 2.7666938304901123, Total Loss is: 237.2886505126953\n",
            "MSE Loss is: 10.441078186035156, h Loss is: 226.9007110595703, L1 loss: 2.767284393310547, Total Loss is: 237.341796875\n",
            "MSE Loss is: 10.627949714660645, h Loss is: 226.92889404296875, L1 loss: 2.7684760093688965, Total Loss is: 237.5568389892578\n",
            "MSE Loss is: 10.844745635986328, h Loss is: 226.8632049560547, L1 loss: 2.7689545154571533, Total Loss is: 237.70794677734375\n",
            "MSE Loss is: 10.223325729370117, h Loss is: 226.85134887695312, L1 loss: 2.7691543102264404, Total Loss is: 237.07467651367188\n",
            "MSE Loss is: 10.179059028625488, h Loss is: 226.82786560058594, L1 loss: 2.7698140144348145, Total Loss is: 237.00692749023438\n",
            "MSE Loss is: 10.5801420211792, h Loss is: 226.8833465576172, L1 loss: 2.770970106124878, Total Loss is: 237.46348571777344\n",
            "MSE Loss is: 10.400884628295898, h Loss is: 227.00816345214844, L1 loss: 2.7726070880889893, Total Loss is: 237.40904235839844\n",
            "MSE Loss is: 10.373747825622559, h Loss is: 227.12893676757812, L1 loss: 2.774420976638794, Total Loss is: 237.502685546875\n",
            "MSE Loss is: 10.39590072631836, h Loss is: 227.28146362304688, L1 loss: 2.7754876613616943, Total Loss is: 237.6773681640625\n",
            "MSE Loss is: 10.540103912353516, h Loss is: 227.40170288085938, L1 loss: 2.7758336067199707, Total Loss is: 237.94180297851562\n",
            "New h_val is : tf.Tensor(2.6885462, shape=(), dtype=float32)\n",
            "Epoch: {} 16\n",
            "MSE Loss is: 10.5230712890625, h Loss is: 259.0464172363281, L1 loss: 2.7761952877044678, Total Loss is: 269.5694885253906\n",
            "MSE Loss is: 10.753111839294434, h Loss is: 259.2005920410156, L1 loss: 2.7771148681640625, Total Loss is: 269.9537048339844\n",
            "MSE Loss is: 10.660392761230469, h Loss is: 259.36322021484375, L1 loss: 2.7780239582061768, Total Loss is: 270.02362060546875\n",
            "MSE Loss is: 10.161657333374023, h Loss is: 259.56536865234375, L1 loss: 2.7774579524993896, Total Loss is: 269.7270202636719\n",
            "MSE Loss is: 10.314263343811035, h Loss is: 259.6502990722656, L1 loss: 2.7769901752471924, Total Loss is: 269.9645690917969\n",
            "MSE Loss is: 10.283478736877441, h Loss is: 259.5749816894531, L1 loss: 2.7776153087615967, Total Loss is: 269.85845947265625\n",
            "MSE Loss is: 10.483880996704102, h Loss is: 259.4270324707031, L1 loss: 2.778416633605957, Total Loss is: 269.9109191894531\n",
            "MSE Loss is: 10.325284957885742, h Loss is: 259.3410949707031, L1 loss: 2.7794575691223145, Total Loss is: 269.6663818359375\n",
            "MSE Loss is: 10.335943222045898, h Loss is: 259.3966979980469, L1 loss: 2.7803854942321777, Total Loss is: 269.7326354980469\n",
            "MSE Loss is: 10.382075309753418, h Loss is: 259.5225524902344, L1 loss: 2.780836820602417, Total Loss is: 269.9046325683594\n",
            "MSE Loss is: 10.472362518310547, h Loss is: 259.7276306152344, L1 loss: 2.7808423042297363, Total Loss is: 270.1999816894531\n",
            "MSE Loss is: 10.136967658996582, h Loss is: 259.80706787109375, L1 loss: 2.7804932594299316, Total Loss is: 269.94403076171875\n",
            "MSE Loss is: 10.977935791015625, h Loss is: 259.7497253417969, L1 loss: 2.780515432357788, Total Loss is: 270.7276611328125\n",
            "MSE Loss is: 10.420666694641113, h Loss is: 259.6919860839844, L1 loss: 2.780165433883667, Total Loss is: 270.1126403808594\n",
            "MSE Loss is: 10.298232078552246, h Loss is: 259.51837158203125, L1 loss: 2.780573606491089, Total Loss is: 269.81658935546875\n",
            "MSE Loss is: 10.351591110229492, h Loss is: 259.2905578613281, L1 loss: 2.7816836833953857, Total Loss is: 269.64215087890625\n",
            "MSE Loss is: 10.422046661376953, h Loss is: 259.09979248046875, L1 loss: 2.783071517944336, Total Loss is: 269.5218505859375\n",
            "MSE Loss is: 10.878957748413086, h Loss is: 259.02783203125, L1 loss: 2.7843563556671143, Total Loss is: 269.90679931640625\n",
            "MSE Loss is: 10.321979522705078, h Loss is: 259.08502197265625, L1 loss: 2.785304069519043, Total Loss is: 269.4070129394531\n",
            "MSE Loss is: 10.958930969238281, h Loss is: 259.1974792480469, L1 loss: 2.7864034175872803, Total Loss is: 270.1564025878906\n",
            "MSE Loss is: 10.477598190307617, h Loss is: 259.3903503417969, L1 loss: 2.7864131927490234, Total Loss is: 269.8679504394531\n",
            "MSE Loss is: 10.841035842895508, h Loss is: 259.5793762207031, L1 loss: 2.7860007286071777, Total Loss is: 270.42041015625\n",
            "MSE Loss is: 10.408280372619629, h Loss is: 259.678955078125, L1 loss: 2.7842795848846436, Total Loss is: 270.0872497558594\n",
            "MSE Loss is: 10.62221622467041, h Loss is: 259.75030517578125, L1 loss: 2.783306837081909, Total Loss is: 270.3725280761719\n",
            "MSE Loss is: 10.623294830322266, h Loss is: 259.68292236328125, L1 loss: 2.7825191020965576, Total Loss is: 270.30621337890625\n",
            "MSE Loss is: 10.821179389953613, h Loss is: 259.6106872558594, L1 loss: 2.7823543548583984, Total Loss is: 270.4318542480469\n",
            "MSE Loss is: 10.908564567565918, h Loss is: 259.63238525390625, L1 loss: 2.782258987426758, Total Loss is: 270.54095458984375\n",
            "MSE Loss is: 10.286330223083496, h Loss is: 259.7925720214844, L1 loss: 2.7825608253479004, Total Loss is: 270.0788879394531\n",
            "MSE Loss is: 10.426467895507812, h Loss is: 260.0136413574219, L1 loss: 2.7835772037506104, Total Loss is: 270.44012451171875\n",
            "MSE Loss is: 10.576574325561523, h Loss is: 260.2061462402344, L1 loss: 2.7849762439727783, Total Loss is: 270.78271484375\n",
            "MSE Loss is: 10.694330215454102, h Loss is: 260.30316162109375, L1 loss: 2.786146640777588, Total Loss is: 270.99749755859375\n",
            "MSE Loss is: 10.440292358398438, h Loss is: 260.3001403808594, L1 loss: 2.786790132522583, Total Loss is: 270.74041748046875\n",
            "MSE Loss is: 10.6491060256958, h Loss is: 260.27734375, L1 loss: 2.7867181301116943, Total Loss is: 270.92645263671875\n",
            "MSE Loss is: 10.290267944335938, h Loss is: 260.2316589355469, L1 loss: 2.785451650619507, Total Loss is: 270.52191162109375\n",
            "MSE Loss is: 10.521498680114746, h Loss is: 260.1879577636719, L1 loss: 2.783825635910034, Total Loss is: 270.7094421386719\n",
            "MSE Loss is: 10.786611557006836, h Loss is: 260.1795959472656, L1 loss: 2.782837390899658, Total Loss is: 270.9662170410156\n",
            "MSE Loss is: 10.560125350952148, h Loss is: 260.3078308105469, L1 loss: 2.7807717323303223, Total Loss is: 270.8679504394531\n",
            "MSE Loss is: 11.059789657592773, h Loss is: 260.5158996582031, L1 loss: 2.7791786193847656, Total Loss is: 271.57568359375\n",
            "MSE Loss is: 10.43362808227539, h Loss is: 260.67706298828125, L1 loss: 2.7780587673187256, Total Loss is: 271.1106872558594\n",
            "MSE Loss is: 10.44141960144043, h Loss is: 260.7401428222656, L1 loss: 2.778613805770874, Total Loss is: 271.1815490722656\n",
            "MSE Loss is: 10.6278076171875, h Loss is: 260.7597961425781, L1 loss: 2.779803991317749, Total Loss is: 271.3876037597656\n",
            "MSE Loss is: 10.843722343444824, h Loss is: 260.65631103515625, L1 loss: 2.780243158340454, Total Loss is: 271.5000305175781\n",
            "MSE Loss is: 10.223222732543945, h Loss is: 260.62384033203125, L1 loss: 2.780416965484619, Total Loss is: 270.8470764160156\n",
            "MSE Loss is: 10.178035736083984, h Loss is: 260.5909423828125, L1 loss: 2.781080961227417, Total Loss is: 270.76898193359375\n",
            "MSE Loss is: 10.580768585205078, h Loss is: 260.6746520996094, L1 loss: 2.7822697162628174, Total Loss is: 271.25543212890625\n",
            "MSE Loss is: 10.401304244995117, h Loss is: 260.859619140625, L1 loss: 2.783923387527466, Total Loss is: 271.26092529296875\n",
            "MSE Loss is: 10.373260498046875, h Loss is: 261.0476379394531, L1 loss: 2.785827875137329, Total Loss is: 271.4208984375\n",
            "MSE Loss is: 10.39466381072998, h Loss is: 261.268310546875, L1 loss: 2.7868611812591553, Total Loss is: 271.6629638671875\n",
            "MSE Loss is: 10.539690017700195, h Loss is: 261.4375, L1 loss: 2.787186861038208, Total Loss is: 271.9772033691406\n",
            "New h_val is : tf.Tensor(2.7130919, shape=(), dtype=float32)\n",
            "Epoch: {} 17\n",
            "MSE Loss is: 10.522161483764648, h Loss is: 296.9029846191406, L1 loss: 2.787468433380127, Total Loss is: 307.4251403808594\n",
            "MSE Loss is: 10.752309799194336, h Loss is: 297.0820007324219, L1 loss: 2.788330554962158, Total Loss is: 307.8343200683594\n",
            "MSE Loss is: 10.659738540649414, h Loss is: 297.25970458984375, L1 loss: 2.7892487049102783, Total Loss is: 307.91943359375\n",
            "MSE Loss is: 10.16114616394043, h Loss is: 297.4856872558594, L1 loss: 2.7887463569641113, Total Loss is: 307.6468200683594\n",
            "MSE Loss is: 10.314071655273438, h Loss is: 297.5833435058594, L1 loss: 2.7882091999053955, Total Loss is: 307.89739990234375\n",
            "MSE Loss is: 10.283916473388672, h Loss is: 297.5067443847656, L1 loss: 2.7887799739837646, Total Loss is: 307.7906494140625\n",
            "MSE Loss is: 10.48452377319336, h Loss is: 297.3617248535156, L1 loss: 2.7895989418029785, Total Loss is: 307.84625244140625\n",
            "MSE Loss is: 10.3248929977417, h Loss is: 297.306640625, L1 loss: 2.790651798248291, Total Loss is: 307.63153076171875\n",
            "MSE Loss is: 10.336057662963867, h Loss is: 297.4224548339844, L1 loss: 2.791559934616089, Total Loss is: 307.7585144042969\n",
            "MSE Loss is: 10.382316589355469, h Loss is: 297.60784912109375, L1 loss: 2.791996717453003, Total Loss is: 307.99017333984375\n",
            "MSE Loss is: 10.472238540649414, h Loss is: 297.8608093261719, L1 loss: 2.7919914722442627, Total Loss is: 308.3330383300781\n",
            "MSE Loss is: 10.136667251586914, h Loss is: 297.9346923828125, L1 loss: 2.791625499725342, Total Loss is: 308.07135009765625\n",
            "MSE Loss is: 10.977243423461914, h Loss is: 297.8279113769531, L1 loss: 2.7916200160980225, Total Loss is: 308.8051452636719\n",
            "MSE Loss is: 10.420394897460938, h Loss is: 297.7239685058594, L1 loss: 2.7912590503692627, Total Loss is: 308.14434814453125\n",
            "MSE Loss is: 10.29794692993164, h Loss is: 297.5021667480469, L1 loss: 2.7916927337646484, Total Loss is: 307.80010986328125\n",
            "MSE Loss is: 10.35052490234375, h Loss is: 297.2444763183594, L1 loss: 2.79278826713562, Total Loss is: 307.5950012207031\n",
            "MSE Loss is: 10.421690940856934, h Loss is: 297.0555725097656, L1 loss: 2.794140577316284, Total Loss is: 307.4772644042969\n",
            "MSE Loss is: 10.877882957458496, h Loss is: 297.0213623046875, L1 loss: 2.7954468727111816, Total Loss is: 307.89923095703125\n",
            "MSE Loss is: 10.32144546508789, h Loss is: 297.1307067871094, L1 loss: 2.796384811401367, Total Loss is: 307.4521484375\n",
            "MSE Loss is: 10.958855628967285, h Loss is: 297.28143310546875, L1 loss: 2.7974698543548584, Total Loss is: 308.24029541015625\n",
            "MSE Loss is: 10.478214263916016, h Loss is: 297.5004577636719, L1 loss: 2.7974042892456055, Total Loss is: 307.9786682128906\n",
            "MSE Loss is: 10.841547012329102, h Loss is: 297.6920471191406, L1 loss: 2.796973705291748, Total Loss is: 308.5335998535156\n",
            "MSE Loss is: 10.408329010009766, h Loss is: 297.7714538574219, L1 loss: 2.795219659805298, Total Loss is: 308.1797790527344\n",
            "MSE Loss is: 10.622111320495605, h Loss is: 297.83111572265625, L1 loss: 2.794234275817871, Total Loss is: 308.4532165527344\n",
            "MSE Loss is: 10.623634338378906, h Loss is: 297.74835205078125, L1 loss: 2.793450117111206, Total Loss is: 308.3719787597656\n",
            "MSE Loss is: 10.821245193481445, h Loss is: 297.6853332519531, L1 loss: 2.793264150619507, Total Loss is: 308.506591796875\n",
            "MSE Loss is: 10.907896041870117, h Loss is: 297.75506591796875, L1 loss: 2.793184757232666, Total Loss is: 308.6629638671875\n",
            "MSE Loss is: 10.285812377929688, h Loss is: 297.9934387207031, L1 loss: 2.7934982776641846, Total Loss is: 308.27923583984375\n",
            "MSE Loss is: 10.425813674926758, h Loss is: 298.28485107421875, L1 loss: 2.7945382595062256, Total Loss is: 308.7106628417969\n",
            "MSE Loss is: 10.576834678649902, h Loss is: 298.51336669921875, L1 loss: 2.7959651947021484, Total Loss is: 309.0902099609375\n",
            "MSE Loss is: 10.694906234741211, h Loss is: 298.5990905761719, L1 loss: 2.797123908996582, Total Loss is: 309.29400634765625\n",
            "MSE Loss is: 10.440711975097656, h Loss is: 298.551025390625, L1 loss: 2.7977871894836426, Total Loss is: 308.9917297363281\n",
            "MSE Loss is: 10.649341583251953, h Loss is: 298.4820251464844, L1 loss: 2.797680616378784, Total Loss is: 309.1313781738281\n",
            "MSE Loss is: 10.290922164916992, h Loss is: 298.4092102050781, L1 loss: 2.796368360519409, Total Loss is: 308.70013427734375\n",
            "MSE Loss is: 10.52213191986084, h Loss is: 298.3672180175781, L1 loss: 2.7946815490722656, Total Loss is: 308.88934326171875\n",
            "MSE Loss is: 10.786188125610352, h Loss is: 298.39556884765625, L1 loss: 2.7936596870422363, Total Loss is: 309.1817626953125\n",
            "MSE Loss is: 10.559009552001953, h Loss is: 298.5969543457031, L1 loss: 2.791579484939575, Total Loss is: 309.1559753417969\n",
            "MSE Loss is: 11.059755325317383, h Loss is: 298.8831481933594, L1 loss: 2.7899329662323, Total Loss is: 309.9429016113281\n",
            "MSE Loss is: 10.434188842773438, h Loss is: 299.0893859863281, L1 loss: 2.78875994682312, Total Loss is: 309.5235595703125\n",
            "MSE Loss is: 10.441864967346191, h Loss is: 299.1545715332031, L1 loss: 2.7892746925354004, Total Loss is: 309.596435546875\n",
            "MSE Loss is: 10.627617835998535, h Loss is: 299.1470947265625, L1 loss: 2.7904698848724365, Total Loss is: 309.77471923828125\n",
            "MSE Loss is: 10.842977523803711, h Loss is: 298.9914245605469, L1 loss: 2.79087233543396, Total Loss is: 309.83441162109375\n",
            "MSE Loss is: 10.223221778869629, h Loss is: 298.93994140625, L1 loss: 2.7910234928131104, Total Loss is: 309.1631774902344\n",
            "MSE Loss is: 10.17732048034668, h Loss is: 298.909912109375, L1 loss: 2.7916958332061768, Total Loss is: 309.08721923828125\n",
            "MSE Loss is: 10.58144760131836, h Loss is: 299.0440673828125, L1 loss: 2.7929317951202393, Total Loss is: 309.6255187988281\n",
            "MSE Loss is: 10.401519775390625, h Loss is: 299.312744140625, L1 loss: 2.7946617603302, Total Loss is: 309.7142639160156\n",
            "MSE Loss is: 10.373065948486328, h Loss is: 299.5837097167969, L1 loss: 2.796595335006714, Total Loss is: 309.956787109375\n",
            "MSE Loss is: 10.393688201904297, h Loss is: 299.87811279296875, L1 loss: 2.7975990772247314, Total Loss is: 310.27178955078125\n",
            "MSE Loss is: 10.539509773254395, h Loss is: 300.0907287597656, L1 loss: 2.7979111671447754, Total Loss is: 310.6302490234375\n",
            "New h_val is : tf.Tensor(2.7417603, shape=(), dtype=float32)\n",
            "Epoch: {} 18\n",
            "MSE Loss is: 10.521474838256836, h Loss is: 339.934814453125, L1 loss: 2.7981326580047607, Total Loss is: 350.456298828125\n",
            "MSE Loss is: 10.7516450881958, h Loss is: 340.1308288574219, L1 loss: 2.7989375591278076, Total Loss is: 350.8824768066406\n",
            "MSE Loss is: 10.659093856811523, h Loss is: 340.3231201171875, L1 loss: 2.799860715866089, Total Loss is: 350.9822082519531\n",
            "MSE Loss is: 10.160787582397461, h Loss is: 340.5851745605469, L1 loss: 2.799435615539551, Total Loss is: 350.7459716796875\n",
            "MSE Loss is: 10.314041137695312, h Loss is: 340.7113037109375, L1 loss: 2.798837900161743, Total Loss is: 351.02532958984375\n",
            "MSE Loss is: 10.284505844116211, h Loss is: 340.6478576660156, L1 loss: 2.799346446990967, Total Loss is: 350.932373046875\n",
            "MSE Loss is: 10.48500919342041, h Loss is: 340.5154113769531, L1 loss: 2.8001861572265625, Total Loss is: 351.00042724609375\n",
            "MSE Loss is: 10.324565887451172, h Loss is: 340.4969787597656, L1 loss: 2.8012747764587402, Total Loss is: 350.821533203125\n",
            "MSE Loss is: 10.33616828918457, h Loss is: 340.67596435546875, L1 loss: 2.8021671772003174, Total Loss is: 351.01214599609375\n",
            "MSE Loss is: 10.38258171081543, h Loss is: 340.91546630859375, L1 loss: 2.8025786876678467, Total Loss is: 351.29803466796875\n",
            "MSE Loss is: 10.472200393676758, h Loss is: 341.20770263671875, L1 loss: 2.802564859390259, Total Loss is: 351.6799011230469\n",
            "MSE Loss is: 10.136391639709473, h Loss is: 341.2615051269531, L1 loss: 2.8021903038024902, Total Loss is: 351.39788818359375\n",
            "MSE Loss is: 10.976622581481934, h Loss is: 341.09149169921875, L1 loss: 2.8021607398986816, Total Loss is: 352.068115234375\n",
            "MSE Loss is: 10.420242309570312, h Loss is: 340.9385070800781, L1 loss: 2.801790475845337, Total Loss is: 351.3587646484375\n",
            "MSE Loss is: 10.297752380371094, h Loss is: 340.6729736328125, L1 loss: 2.8022537231445312, Total Loss is: 350.9707336425781\n",
            "MSE Loss is: 10.34976863861084, h Loss is: 340.3945617675781, L1 loss: 2.8033571243286133, Total Loss is: 350.74432373046875\n",
            "MSE Loss is: 10.42116928100586, h Loss is: 340.21759033203125, L1 loss: 2.8046669960021973, Total Loss is: 350.6387634277344\n",
            "MSE Loss is: 10.876946449279785, h Loss is: 340.2261047363281, L1 loss: 2.805985689163208, Total Loss is: 351.1030578613281\n",
            "MSE Loss is: 10.320958137512207, h Loss is: 340.38409423828125, L1 loss: 2.806924819946289, Total Loss is: 350.7050476074219\n",
            "MSE Loss is: 10.958795547485352, h Loss is: 340.56103515625, L1 loss: 2.8080050945281982, Total Loss is: 351.51983642578125\n",
            "MSE Loss is: 10.47877311706543, h Loss is: 340.7921142578125, L1 loss: 2.807875394821167, Total Loss is: 351.2708740234375\n",
            "MSE Loss is: 10.841922760009766, h Loss is: 340.97686767578125, L1 loss: 2.807429552078247, Total Loss is: 351.81878662109375\n",
            "MSE Loss is: 10.408414840698242, h Loss is: 341.0328369140625, L1 loss: 2.805659532546997, Total Loss is: 351.4412536621094\n",
            "MSE Loss is: 10.621994018554688, h Loss is: 341.09027099609375, L1 loss: 2.804666042327881, Total Loss is: 351.7122802734375\n",
            "MSE Loss is: 10.623978614807129, h Loss is: 341.00396728515625, L1 loss: 2.8038833141326904, Total Loss is: 351.6279602050781\n",
            "MSE Loss is: 10.821247100830078, h Loss is: 340.96337890625, L1 loss: 2.8036766052246094, Total Loss is: 351.7846374511719\n",
            "MSE Loss is: 10.907210350036621, h Loss is: 341.0919189453125, L1 loss: 2.8036155700683594, Total Loss is: 351.9991149902344\n",
            "MSE Loss is: 10.285282135009766, h Loss is: 341.41265869140625, L1 loss: 2.803938627243042, Total Loss is: 351.69793701171875\n",
            "MSE Loss is: 10.42528247833252, h Loss is: 341.7694091796875, L1 loss: 2.8049981594085693, Total Loss is: 352.1947021484375\n",
            "MSE Loss is: 10.577116012573242, h Loss is: 342.01947021484375, L1 loss: 2.8064587116241455, Total Loss is: 352.5965881347656\n",
            "MSE Loss is: 10.69531536102295, h Loss is: 342.0762634277344, L1 loss: 2.807615280151367, Total Loss is: 352.7715759277344\n",
            "MSE Loss is: 10.441061019897461, h Loss is: 341.97021484375, L1 loss: 2.808319568634033, Total Loss is: 352.4112854003906\n",
            "MSE Loss is: 10.649531364440918, h Loss is: 341.85284423828125, L1 loss: 2.808159112930298, Total Loss is: 352.50238037109375\n",
            "MSE Loss is: 10.291590690612793, h Loss is: 341.76165771484375, L1 loss: 2.8068203926086426, Total Loss is: 352.0532531738281\n",
            "MSE Loss is: 10.522607803344727, h Loss is: 341.7373962402344, L1 loss: 2.805086851119995, Total Loss is: 352.260009765625\n",
            "MSE Loss is: 10.785776138305664, h Loss is: 341.81793212890625, L1 loss: 2.804027557373047, Total Loss is: 352.60369873046875\n",
            "MSE Loss is: 10.557924270629883, h Loss is: 342.1046142578125, L1 loss: 2.8020007610321045, Total Loss is: 352.66253662109375\n",
            "MSE Loss is: 11.059869766235352, h Loss is: 342.47088623046875, L1 loss: 2.8002560138702393, Total Loss is: 353.53076171875\n",
            "MSE Loss is: 10.434761047363281, h Loss is: 342.7120056152344, L1 loss: 2.7990362644195557, Total Loss is: 353.1467590332031\n",
            "MSE Loss is: 10.442320823669434, h Loss is: 342.76251220703125, L1 loss: 2.799487352371216, Total Loss is: 353.204833984375\n",
            "MSE Loss is: 10.627312660217285, h Loss is: 342.71295166015625, L1 loss: 2.8006885051727295, Total Loss is: 353.34027099609375\n",
            "MSE Loss is: 10.84251594543457, h Loss is: 342.49713134765625, L1 loss: 2.8010618686676025, Total Loss is: 353.33966064453125\n",
            "MSE Loss is: 10.223302841186523, h Loss is: 342.4356384277344, L1 loss: 2.8011932373046875, Total Loss is: 352.658935546875\n",
            "MSE Loss is: 10.176844596862793, h Loss is: 342.4263916015625, L1 loss: 2.801872968673706, Total Loss is: 352.6032409667969\n",
            "MSE Loss is: 10.58200740814209, h Loss is: 342.63409423828125, L1 loss: 2.803168535232544, Total Loss is: 353.2160949707031\n",
            "MSE Loss is: 10.401582717895508, h Loss is: 343.00799560546875, L1 loss: 2.804990768432617, Total Loss is: 353.4095764160156\n",
            "MSE Loss is: 10.373140335083008, h Loss is: 343.37261962890625, L1 loss: 2.806939125061035, Total Loss is: 353.7457580566406\n",
            "MSE Loss is: 10.39295768737793, h Loss is: 343.74163818359375, L1 loss: 2.8079116344451904, Total Loss is: 354.13458251953125\n",
            "MSE Loss is: 10.539482116699219, h Loss is: 343.98944091796875, L1 loss: 2.808210849761963, Total Loss is: 354.5289306640625\n",
            "New h_val is : tf.Tensor(2.7737637, shape=(), dtype=float32)\n",
            "Epoch: {} 19\n",
            "MSE Loss is: 10.52092456817627, h Loss is: 388.83831787109375, L1 loss: 2.8083901405334473, Total Loss is: 399.3592529296875\n",
            "MSE Loss is: 10.751123428344727, h Loss is: 389.0498046875, L1 loss: 2.8091390132904053, Total Loss is: 399.8009338378906\n",
            "MSE Loss is: 10.658468246459961, h Loss is: 389.2650146484375, L1 loss: 2.810053586959839, Total Loss is: 399.9234924316406\n",
            "MSE Loss is: 10.160582542419434, h Loss is: 389.5824890136719, L1 loss: 2.8097267150878906, Total Loss is: 399.7430725097656\n",
            "MSE Loss is: 10.3140869140625, h Loss is: 389.7574768066406, L1 loss: 2.809112071990967, Total Loss is: 400.0715637207031\n",
            "MSE Loss is: 10.285219192504883, h Loss is: 389.72027587890625, L1 loss: 2.8095226287841797, Total Loss is: 400.0054931640625\n",
            "MSE Loss is: 10.48531723022461, h Loss is: 389.6054992675781, L1 loss: 2.810375928878784, Total Loss is: 400.0908203125\n",
            "MSE Loss is: 10.324331283569336, h Loss is: 389.6240539550781, L1 loss: 2.8115248680114746, Total Loss is: 399.9483947753906\n",
            "MSE Loss is: 10.336282730102539, h Loss is: 389.8634033203125, L1 loss: 2.812408208847046, Total Loss is: 400.1996765136719\n",
            "MSE Loss is: 10.382847785949707, h Loss is: 390.1517333984375, L1 loss: 2.812777042388916, Total Loss is: 400.5345764160156\n",
            "MSE Loss is: 10.472200393676758, h Loss is: 390.47637939453125, L1 loss: 2.8127527236938477, Total Loss is: 400.9485778808594\n",
            "MSE Loss is: 10.136173248291016, h Loss is: 390.4990539550781, L1 loss: 2.8123791217803955, Total Loss is: 400.6352233886719\n",
            "MSE Loss is: 10.976076126098633, h Loss is: 390.255859375, L1 loss: 2.8123345375061035, Total Loss is: 401.23193359375\n",
            "MSE Loss is: 10.420215606689453, h Loss is: 390.0552062988281, L1 loss: 2.8119609355926514, Total Loss is: 400.4754333496094\n",
            "MSE Loss is: 10.297662734985352, h Loss is: 389.75140380859375, L1 loss: 2.812448024749756, Total Loss is: 400.049072265625\n",
            "MSE Loss is: 10.349308013916016, h Loss is: 389.45965576171875, L1 loss: 2.8135900497436523, Total Loss is: 399.8089599609375\n",
            "MSE Loss is: 10.42049789428711, h Loss is: 389.29974365234375, L1 loss: 2.8148531913757324, Total Loss is: 399.7202453613281\n",
            "MSE Loss is: 10.876169204711914, h Loss is: 389.3511047363281, L1 loss: 2.8161537647247314, Total Loss is: 400.2272644042969\n",
            "MSE Loss is: 10.320521354675293, h Loss is: 389.54962158203125, L1 loss: 2.8171045780181885, Total Loss is: 399.8701477050781\n",
            "MSE Loss is: 10.95881462097168, h Loss is: 389.73785400390625, L1 loss: 2.8181939125061035, Total Loss is: 400.6966552734375\n",
            "MSE Loss is: 10.479227066040039, h Loss is: 389.9702453613281, L1 loss: 2.818035125732422, Total Loss is: 400.449462890625\n",
            "MSE Loss is: 10.842182159423828, h Loss is: 390.14227294921875, L1 loss: 2.817538022994995, Total Loss is: 400.9844665527344\n",
            "MSE Loss is: 10.408531188964844, h Loss is: 390.17767333984375, L1 loss: 2.8157708644866943, Total Loss is: 400.5862121582031\n",
            "MSE Loss is: 10.621906280517578, h Loss is: 390.2464904785156, L1 loss: 2.8147830963134766, Total Loss is: 400.868408203125\n",
            "MSE Loss is: 10.624296188354492, h Loss is: 390.1704406738281, L1 loss: 2.813993453979492, Total Loss is: 400.79473876953125\n",
            "MSE Loss is: 10.82115650177002, h Loss is: 390.1628723144531, L1 loss: 2.8137569427490234, Total Loss is: 400.9840393066406\n",
            "MSE Loss is: 10.906515121459961, h Loss is: 390.355224609375, L1 loss: 2.813716173171997, Total Loss is: 401.2617492675781\n",
            "MSE Loss is: 10.284778594970703, h Loss is: 390.75775146484375, L1 loss: 2.8140482902526855, Total Loss is: 401.04254150390625\n",
            "MSE Loss is: 10.4248685836792, h Loss is: 391.16973876953125, L1 loss: 2.815122365951538, Total Loss is: 401.5946044921875\n",
            "MSE Loss is: 10.577311515808105, h Loss is: 391.4276428222656, L1 loss: 2.816614866256714, Total Loss is: 402.00494384765625\n",
            "MSE Loss is: 10.695542335510254, h Loss is: 391.4402160644531, L1 loss: 2.8177878856658936, Total Loss is: 402.1357727050781\n",
            "MSE Loss is: 10.44131088256836, h Loss is: 391.26812744140625, L1 loss: 2.8185384273529053, Total Loss is: 401.7094421386719\n",
            "MSE Loss is: 10.649688720703125, h Loss is: 391.10565185546875, L1 loss: 2.8183176517486572, Total Loss is: 401.7553405761719\n",
            "MSE Loss is: 10.29224967956543, h Loss is: 391.0082702636719, L1 loss: 2.8169679641723633, Total Loss is: 401.3005065917969\n",
            "MSE Loss is: 10.522867202758789, h Loss is: 391.0164489746094, L1 loss: 2.8152103424072266, Total Loss is: 401.539306640625\n",
            "MSE Loss is: 10.785348892211914, h Loss is: 391.1619873046875, L1 loss: 2.814116954803467, Total Loss is: 401.94732666015625\n",
            "MSE Loss is: 10.55688762664795, h Loss is: 391.5411376953125, L1 loss: 2.812143564224243, Total Loss is: 402.0980224609375\n",
            "MSE Loss is: 11.060115814208984, h Loss is: 391.9851379394531, L1 loss: 2.8103508949279785, Total Loss is: 403.0452575683594\n",
            "MSE Loss is: 10.435262680053711, h Loss is: 392.24945068359375, L1 loss: 2.8090884685516357, Total Loss is: 402.6847229003906\n",
            "MSE Loss is: 10.442708969116211, h Loss is: 392.26983642578125, L1 loss: 2.8094089031219482, Total Loss is: 402.7125549316406\n",
            "MSE Loss is: 10.626860618591309, h Loss is: 392.1685485839844, L1 loss: 2.81060528755188, Total Loss is: 402.79541015625\n",
            "MSE Loss is: 10.842329978942871, h Loss is: 391.8903503417969, L1 loss: 2.8109986782073975, Total Loss is: 402.732666015625\n",
            "MSE Loss is: 10.223398208618164, h Loss is: 391.83380126953125, L1 loss: 2.811086416244507, Total Loss is: 402.05718994140625\n",
            "MSE Loss is: 10.17654037475586, h Loss is: 391.8653564453125, L1 loss: 2.8117663860321045, Total Loss is: 402.0419006347656\n",
            "MSE Loss is: 10.582277297973633, h Loss is: 392.1705322265625, L1 loss: 2.813122034072876, Total Loss is: 402.7528076171875\n",
            "MSE Loss is: 10.401577949523926, h Loss is: 392.6682434082031, L1 loss: 2.815049409866333, Total Loss is: 403.06982421875\n",
            "MSE Loss is: 10.373366355895996, h Loss is: 393.133544921875, L1 loss: 2.8170063495635986, Total Loss is: 403.50689697265625\n",
            "MSE Loss is: 10.392471313476562, h Loss is: 393.5754089355469, L1 loss: 2.817941665649414, Total Loss is: 403.9678955078125\n",
            "MSE Loss is: 10.539488792419434, h Loss is: 393.850341796875, L1 loss: 2.8182239532470703, Total Loss is: 404.38983154296875\n",
            "New h_val is : tf.Tensor(2.8085623, shape=(), dtype=float32)\n",
            "saving model to: /content//CausalNN_model_final_1713472393.h5\n",
            "53/53 [==============================] - 0s 2ms/step\n",
            "The conv layer 1 weights before training : [[ 0.01729116 -0.03241783  0.20739552  0.21393666  0.05369946  0.34967968\n",
            "  -0.14644605 -0.3412853  -0.11558445 -0.22822307  0.21469983  0.0880048\n",
            "  -0.1930318   0.18624505 -0.17902574 -0.3153036  -0.22727013 -0.03919902\n",
            "   0.06509387 -0.07369781 -0.10506889 -0.03960952  0.17076865  0.15640828]]\n",
            "The conv layer 1 weights after training : [[ 0.11671788 -0.0145506   0.01118697 -0.1006496   0.3801754   0.00004819\n",
            "  -0.02015377 -0.01712992  0.2449584  -0.00014866 -0.01037108  0.03003637\n",
            "   0.51087815 -0.00926478 -0.15354508  0.07170993  0.57371676  0.00047973\n",
            "  -0.04189474  0.07138208  0.0101239   0.00596141 -0.07376736  0.05771814]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9s9XP9x6rupn",
        "outputId": "fa6781e0-3703-42ff-9eb8-4c9dc20bca6b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.28146005, 0.49190438, 0.1881849 , 0.17641288],\n",
              "        [0.2992469 , 0.48059452, 0.22816728, 0.19841833],\n",
              "        [0.33408394, 0.42284498, 0.33214664, 0.19857386],\n",
              "        [0.29867512, 0.48222014, 0.29584503, 0.12229615],\n",
              "        [0.33640367, 0.61653215, 0.2618387 , 0.19631158]], dtype=float32),\n",
              " array([[0.589811  , 0.55491996, 0.75778573, 0.5528252 ],\n",
              "        [0.61878201, 0.52792297, 0.82147084, 0.53097383],\n",
              "        [0.56813402, 0.52503256, 0.86755772, 0.5733878 ],\n",
              "        [0.61048959, 0.57615099, 0.77059246, 0.55889173],\n",
              "        [0.5042508 , 0.56395476, 0.84200102, 0.59009443]]))"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "y_pred[0:5], data_y_syn[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mat = np.genfromtxt('/content/proposed-full-adj-mat-V3.csv', delimiter=',')"
      ],
      "metadata": {
        "id": "KVaZnV-O1wRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckpp6cuerupo"
      },
      "outputs": [],
      "source": [
        "mat_df_2d_s = pd.DataFrame(mat)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mat_f_df = pd.DataFrame(mat).T"
      ],
      "metadata": {
        "id": "UAa0WohUJ_t_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        },
        "id": "9CtUItJErupo",
        "outputId": "320bb153-8c64-4494-a839-606bee6ce839"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0         1         2         3\n",
              "0   0.060424 -0.086885  0.158718 -0.023992\n",
              "1  -0.071343  0.219383 -0.102552 -0.049103\n",
              "2   0.197360  0.230671  0.013068 -0.011465\n",
              "3  -0.518587  0.195263  0.023268 -0.004621\n",
              "4   0.416361  0.019698  0.006211  0.048413\n",
              "5  -0.010432  0.209990 -0.004963 -0.034856\n",
              "6  -0.019452  0.071430  0.007873 -0.011133\n",
              "7   0.013019 -0.285973 -0.074198 -0.009662\n",
              "8   0.215037 -0.135168  0.307494 -0.130542\n",
              "9  -0.031971  0.131829  0.021005 -0.069707\n",
              "10  0.066766  0.036119  0.077588 -0.034936\n",
              "11  0.059482 -0.076978 -0.145271  0.079954\n",
              "12  0.516758  0.105059 -0.557891  0.437681\n",
              "13 -0.023034  0.223007 -0.045904 -0.031027\n",
              "14 -0.080299  0.048158  0.119814 -0.015027\n",
              "15  0.010720 -0.129315 -0.027521 -0.114521\n",
              "16  0.567747  1.062261 -0.187627  0.349131\n",
              "17  0.026067  0.249058  0.072498 -0.149108\n",
              "18  0.132662  0.036266  0.123289 -0.078585\n",
              "19  0.016741  0.075880 -0.806988  0.555153\n",
              "20  0.000000 -0.173380  0.080703 -0.061715\n",
              "21 -0.095194  0.000000  0.037806 -0.136843\n",
              "22  0.069021  0.172888  0.000000  0.835213\n",
              "23 -0.108998 -0.878886  2.633786  0.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b12001e8-080c-4116-835a-01c9d75117e7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.060424</td>\n",
              "      <td>-0.086885</td>\n",
              "      <td>0.158718</td>\n",
              "      <td>-0.023992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.071343</td>\n",
              "      <td>0.219383</td>\n",
              "      <td>-0.102552</td>\n",
              "      <td>-0.049103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.197360</td>\n",
              "      <td>0.230671</td>\n",
              "      <td>0.013068</td>\n",
              "      <td>-0.011465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.518587</td>\n",
              "      <td>0.195263</td>\n",
              "      <td>0.023268</td>\n",
              "      <td>-0.004621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.416361</td>\n",
              "      <td>0.019698</td>\n",
              "      <td>0.006211</td>\n",
              "      <td>0.048413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>-0.010432</td>\n",
              "      <td>0.209990</td>\n",
              "      <td>-0.004963</td>\n",
              "      <td>-0.034856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-0.019452</td>\n",
              "      <td>0.071430</td>\n",
              "      <td>0.007873</td>\n",
              "      <td>-0.011133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.013019</td>\n",
              "      <td>-0.285973</td>\n",
              "      <td>-0.074198</td>\n",
              "      <td>-0.009662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.215037</td>\n",
              "      <td>-0.135168</td>\n",
              "      <td>0.307494</td>\n",
              "      <td>-0.130542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>-0.031971</td>\n",
              "      <td>0.131829</td>\n",
              "      <td>0.021005</td>\n",
              "      <td>-0.069707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.066766</td>\n",
              "      <td>0.036119</td>\n",
              "      <td>0.077588</td>\n",
              "      <td>-0.034936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.059482</td>\n",
              "      <td>-0.076978</td>\n",
              "      <td>-0.145271</td>\n",
              "      <td>0.079954</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.516758</td>\n",
              "      <td>0.105059</td>\n",
              "      <td>-0.557891</td>\n",
              "      <td>0.437681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>-0.023034</td>\n",
              "      <td>0.223007</td>\n",
              "      <td>-0.045904</td>\n",
              "      <td>-0.031027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>-0.080299</td>\n",
              "      <td>0.048158</td>\n",
              "      <td>0.119814</td>\n",
              "      <td>-0.015027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.010720</td>\n",
              "      <td>-0.129315</td>\n",
              "      <td>-0.027521</td>\n",
              "      <td>-0.114521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.567747</td>\n",
              "      <td>1.062261</td>\n",
              "      <td>-0.187627</td>\n",
              "      <td>0.349131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.026067</td>\n",
              "      <td>0.249058</td>\n",
              "      <td>0.072498</td>\n",
              "      <td>-0.149108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.132662</td>\n",
              "      <td>0.036266</td>\n",
              "      <td>0.123289</td>\n",
              "      <td>-0.078585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.016741</td>\n",
              "      <td>0.075880</td>\n",
              "      <td>-0.806988</td>\n",
              "      <td>0.555153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.173380</td>\n",
              "      <td>0.080703</td>\n",
              "      <td>-0.061715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>-0.095194</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.037806</td>\n",
              "      <td>-0.136843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.069021</td>\n",
              "      <td>0.172888</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.835213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>-0.108998</td>\n",
              "      <td>-0.878886</td>\n",
              "      <td>2.633786</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b12001e8-080c-4116-835a-01c9d75117e7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b12001e8-080c-4116-835a-01c9d75117e7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b12001e8-080c-4116-835a-01c9d75117e7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e5cd19c1-8ccd-4fce-a368-e3ee8d8d7756\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e5cd19c1-8ccd-4fce-a368-e3ee8d8d7756')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e5cd19c1-8ccd-4fce-a368-e3ee8d8d7756 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"mat_df_2d_s\",\n  \"rows\": 24,\n  \"fields\": [\n    {\n      \"column\": 0,\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 24,\n        \"samples\": [\n          0.21503712236881256,\n          0.5677467584609985,\n          0.060424309223890305\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 1,\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 24,\n        \"samples\": [\n          -0.1351684182882309,\n          1.0622609853744507,\n          -0.08688480406999588\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 2,\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 24,\n        \"samples\": [\n          0.30749425292015076,\n          -0.18762744963169098,\n          0.15871800482273102\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 3,\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 24,\n        \"samples\": [\n          -0.13054174184799194,\n          0.3491307199001312,\n          -0.02399151213467121\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "mat_df_2d_s.T"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mat_f_df.iloc[20:,:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "qrIvGlpIKGpn",
        "outputId": "a3563c2e-31d4-4f14-8962-0be1209e8284"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0         1         2         3\n",
              "20  0.000000 -0.173380  0.080703 -0.061715\n",
              "21 -0.095194  0.000000  0.037806 -0.136843\n",
              "22  0.069021  0.172888  0.000000  0.835213\n",
              "23 -0.108998 -0.878886  2.633786  0.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a655d8e7-ddff-448d-b084-b3e9d991851b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.173380</td>\n",
              "      <td>0.080703</td>\n",
              "      <td>-0.061715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>-0.095194</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.037806</td>\n",
              "      <td>-0.136843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.069021</td>\n",
              "      <td>0.172888</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.835213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>-0.108998</td>\n",
              "      <td>-0.878886</td>\n",
              "      <td>2.633786</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a655d8e7-ddff-448d-b084-b3e9d991851b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a655d8e7-ddff-448d-b084-b3e9d991851b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a655d8e7-ddff-448d-b084-b3e9d991851b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f145f777-b284-4d4f-8917-f358fea3a390\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f145f777-b284-4d4f-8917-f358fea3a390')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f145f777-b284-4d4f-8917-f358fea3a390 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"mat_f_df\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": 0,\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          -0.09519411623477936,\n          -0.1089979037642479,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 1,\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.0,\n          -0.8788855075836182,\n          -0.17337976396083832\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 2,\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.03780581057071686,\n          2.633786201477051,\n          0.08070340752601624\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 3,\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          -0.13684332370758057,\n          0.0,\n          -0.06171455979347229\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Full Causal Graph"
      ],
      "metadata": {
        "id": "rK-co1IcGV7q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCrZpZYQrupo"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "G_2d_s = nx.DiGraph()\n",
        "\n",
        "nodes_2d_s = [\"S1(t-5)\", \"S2(t-5)\", \"S3(t-5)\",  \"S4(t-5)\",\n",
        "         \"S1(t-4)\", \"S2(t-4)\", \"S3(t-4)\",  \"S4(t-4)\",\n",
        "         \"S1(t-3)\", \"S2(t-3)\", \"S3(t-3)\",  \"S4(t-3)\",\n",
        "         \"S1(t-2)\", \"S2(t-2)\", \"S3(t-2)\",  \"S4(t-2)\",\n",
        "         \"S1(t-1)\", \"S2(t-1)\", \"S3(t-1)\",  \"S4(t-1)\",\n",
        "         \"S1(t)\", \"S2(t)\", \"S3(t)\",  \"S4(t)\"]\n",
        "nodes_r_2d_s=[\"S1(t)\", \"S2(t)\", \"S3(t)\",  \"S4(t)\"]\n",
        "pred_graph_f = np.zeros((4,24))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nzqj5wyrupo"
      },
      "outputs": [],
      "source": [
        "for i in range (0, 24):\n",
        "  G_2d_s.add_node(nodes_2d_s[i],pos=(int(i/4)+1,(i%4)+1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, 24):\n",
        "  for j in range (0, 4):\n",
        "    if abs(matrix_2d_2d_s[j,i]) > 0.3:\n",
        "      print(i,j)\n",
        "      G_2d_s.add_edge(nodes_2d_s[i], nodes_r_2d_s[j], weight=i)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wh3wYhpEVnwv",
        "outputId": "d5333b98-19f3-4f19-89f0-a651e40a0dc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0\n",
            "3 2\n",
            "4 0\n",
            "8 0\n",
            "12 0\n",
            "13 1\n",
            "16 0\n",
            "16 1\n",
            "17 1\n",
            "18 3\n",
            "20 1\n",
            "23 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtzcncqBrupp"
      },
      "outputs": [],
      "source": [
        "pos_2d_s=nx.get_node_attributes(G_2d_s,'pos')\n",
        "#pos_2d_s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QoGvVgS6SbW"
      },
      "outputs": [],
      "source": [
        "weights_2d_s = nx.get_edge_attributes(G_2d_s,'weight').values()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt #data1\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "nx.draw(G_2d_s, pos_2d_s, cmap = plt.get_cmap('jet'), edge_cmap= plt.cm.tab20, edge_color=weights_2d_s, with_labels = True, connectionstyle='arc3, rad = 0.3')\n",
        "#nx.draw_networkx(G, with_labels = True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B5XFjL7sBviT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cdt.metrics.SHD(true_full_graph, pred_graph_f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWBPKyEhfReZ",
        "outputId": "cc95398e-9a42-4eb7-d9fc-a330b041f98d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8.0"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "true_values = true_full_graph\n",
        "predictions = pred_graph_f\n",
        "\n",
        "N = true_values.shape[1]*true_values.shape[0]\n",
        "accuracy = (true_values == predictions).sum() / N\n",
        "TP = ((predictions == 1) & (true_values == 1)).sum()\n",
        "FP = ((predictions == 1) & (true_values == 0)).sum()\n",
        "TN = ((predictions == 0) & (true_values == 0)).sum()\n",
        "FN = ((predictions == 0) & (true_values == 1)).sum()\n",
        "precision = TP / (TP+FP)\n",
        "recall = TP / (TP + FN)\n",
        "FDR = FP / (FP + TP)\n",
        "F1 = 2 * (precision * recall) / (precision + recall)\n",
        "print('Accuracy: {}, Precision: {}, Recall: {}, FDR: {}, F1 Score: {}'.format(accuracy, precision, recall, FDR,F1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeXtHJDEfJzy",
        "outputId": "2e1cf26d-02a2-46e6-c6f9-3617ea174d2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9166666666666666, Precision: 0.5454545454545454, Recall: 0.6666666666666666, FDR: 0.45454545454545453, F1 Score: 0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ijo5Wyjm6qRR"
      },
      "source": [
        "#Summary Causal Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfX8KQwJ6qRR"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "G_2d_s1 = nx.DiGraph()\n",
        "\n",
        "nodes_2d_s1 = [\"S1\", \"S2\", \"S3\",  \"S4\"]\n",
        "nodes_r_2d_s1= [\"S1\", \"S2\", \"S3\",  \"S4\"]\n",
        "edges_2d_s1 = []\n",
        "pred_graph_s1 = np.zeros((4,4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79j7uDTN6qRR"
      },
      "outputs": [],
      "source": [
        "for i in range (0, 4):\n",
        "  G_2d_s1.add_node(nodes_2d_s1[i],pos=(int(i/2)+1,(i%2)+1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyYUYXSb6qRS"
      },
      "outputs": [],
      "source": [
        "for i in range(0, 24):\n",
        "  for j in range (0, 4):\n",
        "    if matrix_2d_2d_s[j,i] > 0.3:\n",
        "      print(i,j)\n",
        "      col = np.round(matrix_2d_2d_s[j,i], 2)\n",
        "      G_2d_s1.add_edge(nodes_2d_s1[i%4], nodes_r_2d_s1[j], weight=1)\n",
        "      pred_graph_s1[i%4, j]=1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQRq97oL6qRS",
        "outputId": "8302304d-48ed-4498-869c-61e3452643ba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'S1': (1, 1), 'S2': (1, 2), 'S3': (2, 1), 'S4': (2, 2)}"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ],
      "source": [
        "pos_2d_s1=nx.get_node_attributes(G_2d_s1,'pos')\n",
        "pos_2d_s1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "938WuKdz6qRS"
      },
      "outputs": [],
      "source": [
        "weights_2d_s1 = nx.get_edge_attributes(G_2d_s1,'weight').values()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "nx.draw(G_2d_s1, pos_2d_s1,  edge_cmap= plt.cm.tab20,  #cmap = plt.get_cmap('jet'),\n",
        "        font_size=12, node_size=1200, node_color='#c0c0c0', #[30,30,30,30,30],\n",
        "        edge_color=weights_2d_s1, with_labels = True, connectionstyle='arc3, rad = 0.3')\n",
        "#nx.draw_networkx(G, with_labels = True)\n",
        "plt.colorbar(mpl.cm.ScalarMappable(norm=mpl.colors.Normalize(0.3, 1), cmap=plt.cm.tab20),\n",
        "              orientation='vertical', label='Edge Strength')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "id": "Mw47bO8L_aC4",
        "outputId": "45bcff3c-564f-4fa1-dc02-c4bfcb71c48a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unable to determine Axes to steal space for Colorbar. Using gca(), but will raise in the future. Either provide the *cax* argument to use as the Axes for the Colorbar, provide the *ax* argument to steal space from it, or add *mappable* to an Axes.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_graph_s1"
      ],
      "metadata": {
        "id": "j7XG_bPVY2l4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true_graph"
      ],
      "metadata": {
        "id": "8tiiQoIcRobc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cdt.metrics.SHD(true_graph, pred_graph_s1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_1qPVtYFV73",
        "outputId": "314a0e2d-4a0a-4109-db73-2d63a6b1d361"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.0"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "true_values = true_graph\n",
        "predictions = pred_graph_s1\n",
        "\n",
        "N = true_values.shape[1]*true_values.shape[0]\n",
        "accuracy = (true_values == predictions).sum() / N\n",
        "TP = ((predictions == 1) & (true_values == 1)).sum()\n",
        "FP = ((predictions == 1) & (true_values == 0)).sum()\n",
        "TN = ((predictions == 0) & (true_values == 0)).sum()\n",
        "FN = ((predictions == 0) & (true_values == 1)).sum()\n",
        "precision = TP / (TP+FP)\n",
        "recall = TP / (TP + FN)\n",
        "FDR = FP / (FP + TP)\n",
        "F1 = 2 * (precision * recall) / (precision + recall)\n",
        "print('Accuracy: {}, Precision: {}, Recall: {}, FDR: {}, F1 Score: {}'.format(accuracy, precision, recall, FDR,F1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTy-ZiS66Sbr",
        "outputId": "d2901fa1-196b-484e-f027-c4d0b95646e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8125, Precision: 0.7142857142857143, Recall: 0.8333333333333334, FDR: 0.2857142857142857, F1 Score: 0.7692307692307692\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Non-stationarity test"
      ],
      "metadata": {
        "id": "b0BJvUeIBd52"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####ADF Test for Stationarity\n",
        "\n",
        "Null hypothesis: The time series has a unit root and is not stationary.\n",
        "\n",
        "If failed to be rejected, it suggests the time series is not stationarity.\n",
        "\n",
        "If p-values is higher than 0.05 alpha level, we cannot reject the null hypothesis. So the time series is not stationary."
      ],
      "metadata": {
        "id": "Clvsqf54Dx1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ADF Test for non-stationarity\n",
        "def adf_test(data_df):\n",
        "    test_stat, p_val = [], []\n",
        "    cv_1pct, cv_5pct, cv_10pct = [], [], []\n",
        "    stationary = []\n",
        "    for c in data_df.columns:\n",
        "        adf_res = adfuller(data_df[c].dropna(), autolag='AIC')\n",
        "        test_stat.append(adf_res[0])\n",
        "        p_val.append(adf_res[1])\n",
        "        cv_1pct.append(adf_res[4]['1%'])\n",
        "        cv_5pct.append(adf_res[4]['5%'])\n",
        "        cv_10pct.append(adf_res[4]['10%'])\n",
        "        if adf_res[1] > 0.05:\n",
        "          stationary.append('No')\n",
        "        else:\n",
        "          stationary.append('Yes')\n",
        "    adf_res_df = pd.DataFrame({'Test statistic': test_stat,\n",
        "                               'p-value': p_val,\n",
        "                               'Critical value - 1%': cv_1pct,\n",
        "                               'Critical value - 5%': cv_5pct,\n",
        "                               'Critical value - 10%': cv_10pct,\n",
        "                               'Stationary': stationary},\n",
        "                             index=data_df.columns).T\n",
        "    adf_res_df = adf_res_df.round(4)\n",
        "    return adf_res_df"
      ],
      "metadata": {
        "id": "7PzZKqG4DYLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adf_test(data)"
      ],
      "metadata": {
        "id": "hei5gH_lIKJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### KPSS Test for Stationary\n",
        "\n",
        "Null hypothesis: The time series is stationary.\n",
        "\n",
        "If the p-value is less than 0.05 alpha level, we can reject the null hypothesis and derive that the time series is not stationary."
      ],
      "metadata": {
        "id": "ndnMzuoVDr6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def kpss_test(data_df):\n",
        "    test_stat, p_val = [], []\n",
        "    cv_1pct, cv_2p5pct, cv_5pct, cv_10pct = [], [], [], []\n",
        "    stationary = []\n",
        "    for c in data_df.columns:\n",
        "        kpss_res = kpss(data_df[c].dropna())#, regression='ct')\n",
        "        test_stat.append(kpss_res[0])\n",
        "        p_val.append(kpss_res[1])\n",
        "        cv_1pct.append(kpss_res[3]['1%'])\n",
        "        cv_2p5pct.append(kpss_res[3]['2.5%'])\n",
        "        cv_5pct.append(kpss_res[3]['5%'])\n",
        "        cv_10pct.append(kpss_res[3]['10%'])\n",
        "        if kpss_res[1] > 0.05:\n",
        "          stationary.append('Yes')\n",
        "        else:\n",
        "          stationary.append('No')\n",
        "    kpss_res_df = pd.DataFrame({'Test statistic': test_stat,\n",
        "                               'p-value': p_val,\n",
        "                               'Critical value - 1%': cv_1pct,\n",
        "                               'Critical value - 2.5%': cv_2p5pct,\n",
        "                               'Critical value - 5%': cv_5pct,\n",
        "                               'Critical value - 10%': cv_10pct,\n",
        "                               'Stationary': stationary},\n",
        "                             index=data_df.columns).T\n",
        "    kpss_res_df = kpss_res_df.round(4)\n",
        "    return kpss_res_df"
      ],
      "metadata": {
        "id": "adtH2OsIDs6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kpss_test(data)"
      ],
      "metadata": {
        "id": "PtMXlZJZmgXC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
